The Graph Neural Network Model

Abstract

	new neural network model (GNN)
	that extednds existing neural network metrhods for preocssing the data represented in graph domains.

	GNN can process most of the practically usefdul types of graphs
		- acyclic
		- cyclic
		- directed, 
		- undirected

	GNN implements a func  t(T,n) in Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space
		WHY one of its nodes?

	supervised learning algorithm derived to estimate the parameters of the proposed GNN model (compuitational cost considered)

	Experimental results shown to validate the prosposed learning algorithm and demonstrate it's generalization capabilities



Intro
	
	graphs
		single node sequencest	
		trees, 
		acyclic graphs
		cyclic graphs

	data relationshiops exploitation
		-> community of inductive logic programming

	in machine learning, structured data is often associated with the goal of (supervised or unsuppervised) learning from examples  a function tau t that maps G and one of its nodes n to a vector of reals: t(G,n) in Rm

	appplications toa graphical domain 2 classes
		1- graph -focused 
		2) node-focused

	graph-focused applications
		t independent of the node n
		t implements a classificer or a regressor on a graph structure data set

		ex: chemical bond
			nodes =atoms, edges = chemical bonds
			t(G) may be used to estimate the prob. that the chemical compound causes a certain disease.

		ex: image segmentation
			classification of image according to its contetns


	node-focused applications
		t depends on the node n
		classfifcaition and regression depends on the properties of each node.
		? how?
		obejct detection example
			finding wheter an image contains a given object
			and then localize its position

		ex2 web page calssification


	traditional machine learning approaches
		preprocessing graph structure which maps the graph structured info to a simpler representation 
			->e.g. vectors of reals 
			-> squashes the graph struct into a vector of reals and then deals with the preprocessed data using a list-based data processing techinique.

			-> some info is lost in the process, like topological dependency
			-> result may depend on preprocessing

	approaches that preserve some graph structure
		- recursigve neural networks
		- markov chains

		both applied to node and graph-focused problems.

	this paper method , graph neural network, deals directly with graph structured information



	Recursive neural networks
		-> input is a directed acyclic graphs
		-> methods estimates the parameteres w of a function phiw, which maps a graph to a vector of reals.
		-> cyclic graphs can also be preprocessed for this
		-> applied to logical term classification, chemical compund classifciation, logo recognition, web page scoring, face localization

		- also related to SVM
			apply special kernels to operate on graph structured data
			ex: diffusion kernel = based on heat diggusion equation
			ex: kernels that exploit vector produced by a graph random walker

		- RNN also encode the input graph into an internal repreesentation. however in RNN the internal encoding is learned, while in svm it is designed by the user.

	Markov chain models
		emulate processe where the causal connections among events are represented by graphs

		random walk theory (which addresses a particular class of Markov chain models) has been applied with some success to the realization of web page ranking algorithm.
		->attemps to extend these models with learning capabilities s.t. a parametric model representing the behavior of the system can be estimated from a set of training examples extracted from a collection. -> able to generalizae th results to socre all the web pages in a collection

		->other statistical methods propsed , asume data sets constsist of patterns and rlationshiops betwen patterns
			random fields
			bayesian networks
			statistical relation learning
			transfuctive learning
			semisupervised approaches for graph processing

	this paper
		supervised neural network model, suitable for both graph and node-focused applications.
		unifies the two models into a common framework -> GNN graph neural network.

		GNN = extension of both RNN and random walk models, and it retains their characteristics
			extends RNN :
				can process a more general class cyclic, directed and undirected graphs

				can deal with node-focused apps without any preprocessing steps.

			extends RandomWalks theory by :
				introudciton of a llearning algorithm and by enlarging the class of processe that can be modeled

**
			based on  information diffusion mechanism
				graph processed by a set of units, 
				each unit corresponds to a  node of the graph, linked according to the graph connectivity.

				units update their states and exchange info until they reach a stable equilibrium.

				output of a GNN is then computed locally at each node on the base of the unit state

				diffusion mechanism ensures that a unique stable equilibrium always exists

		learning algorithm to estimate parameters of the GNN model on given training examples.
			-computation cost considered
			-under condition GNN =universal approximation property
			- under mild conditions = approximate most of the practically useful functions phi on graphs

	paper structure
		section II: notation, GNN model, learning algorithm 
		section III: computational cost of the learning algorithm
		section IV: experimental results
		section V: conclusion





section II. The graph neural network model

Notation
	G = (N,E)
		N set of nodes
		E set of edges
		ne[n]: neighbors of n (nodes connected to n by an arc)
		co[n]: arcs having n as vertex

		labels represented by real vectors in nodes and edges
			ln in R_ln       labels of nodes
			l(n1,n2) in R_lE  labels of edges

		l vector that stacks together all the labels of the graph

		y vector of data from a graph
		S a subset of the nodes
		yS: is Y on the components related to the node (the edges) in S

		lne[n] vector of labels of all neighbors of n
		labels usually include features of nodes and their relationshiops 

		arcs: directed and undIrected both permitTed

			if coexists in same graph -> distintuigsh them by labels

		Nonpositional graphs
			DESCRIBED SO FAR

		Positional graphs
			a unique integer identifier is assigned to each neigbors of a node n
			to inidciate its logical position

			for each node n in a positional graph
				there exist an injective function 
				vn: ne[n]->{1..|N|}
				 which assigns to each neighbor u of n a position vn(u)

				neighbor position can be implicitly used for storing useful information


	Domain of the papeer
		pairs of a graph and a node
		D= GxN
			G is a set of graphs 
			N is a subset of their nodes

	supervised learning framework
		L = {	(Gi, nij, tij)| 
				Gi=(Ni,Ei) in G;
			 	nij in Ni, 
			 	tij in Rm, 
			 	1<=i<=p, 
			 	1<=j<=qi
			 	}

			 ni.j in Ni is the jth node in the set Ni in N 
			 tij is the desired target asociated to nij
			 p<=|G| and qi<=|Ni|

		all the graphs of the learning set can be combined into a unique disconnected graph
		so:
			L=(G,Tau)
			where G=(N,E)
			and Tau is a set of pairs {(ni,ti)|ni in N, ti in Rm, 1=i<=q}

	this is useful for simplicity

	also it captures direclty the very nature of the probleme where the domain consists of only one graph (like the web)


	A the model

		nodes = concepts or objects
		edges = relationships

		each concept defined by
			- its features
			- the related concepts

		attach state to each node: 
			- state xn in Rs to each node n
			- based on the info in the nieghborhood of n
			- xn contains a representation of the concept of n, and can be used to produce an output "o_n" e.g. a decision about the concept.

		- fw a parametric function called the local transition function that expressed the dependence of a node n on tis neighborhood and 
		- let gw be the local output function that describes how the output is produced.

			xn = fw(ln, lco[n], xne[n], lne[n])
			on = gw(xn, ln)

		Remark1)neighbordhood concept
			flexibility
			e.g. xne could contain info of lne[n]
			e.g. ne[n] could contains 2 steps away neighbors

		Remark2)for directed graphs
			fw also a representation of the direction of the arcs as input
			ex: dl =1 if l directed to n
				dl = 0 if l comes from n

			all results of this paper hold for directed graphs and mixed directed and undirected links.

		Remark3) fw and gw and parameters may depend on each node
			-> different mechanisms to represent diff kinds of objects., fkn and gkn and wk parameters set.

			in this paper model where all fw and gw are shared for all nodes

		x, o, l , lN 
			vectors with all states, outputs, labels and node labels of the graph

			x = Fw(x,l)
			o = Gw(x,lN)

			Fw global transition func
			Gw  global output function
				stacked versions of |N| instances of fw and gw respectively

			-> 1) interested in x,o uniquely defined
			-> 2) interested in Fw and Gw define a map: 
					phiw: D -> Rm

					takes a graph as input and return o_n for each node, so o

					Banach Theorem 
						exitence of unique solution provided that Fw is a contraction map with respect to the state
						
					later GNN shows that Fw is a contraction map enforced by an appropiate implementation

			-> to accomodate for positional graphs
				xne[n], lco[n] and lne[n] can be sorted accorded to position
				padding for nonexisting neighbors


			-> non positional graphs
				usufeul to replace fw by
				xn = SUM{u in ne[n]}hw(ln, l(n,u), xu , lu)

				hw parametric function
				transition function used in rnn

		model needs
		1- solution to xn=fw() and on=gw()
		2- learning algo to adapt fw and gw to the training set
		3- implementation of fw and gw

	B computation of the state

		Banach's fixed point theorem
		- uniqueness of the solution of (1)
		- iterative computing of the state
			x(t+1)=Fw(x(t),l)

			x(t):t-th iteration of x
			converges exp. fast to the sol for any initial value x(0)
			Jacobi iterative method for solving nonlinear equations

			so ouptuts and states can be computed by iterating

			eq(5)

			xn(t+1)=fw(ln,lco[n],xne[n](t),lne[n])
			on(t) =gw(xn(t),ln) n in N

			->this is an encoding network
				units which compute fw and gw

				each node has a unit fw and gw
				xn(t) is stored in each unit
				when a unit is activated, fw computes xn(t+1)
				using the lables and info in the neighborhood
				simultaneous and repeated activations of the units produce the behaviour describe in the iterative formjula

				output of node n is produced by gw unit


		fw ,gw as feed forward neural networks
			-> encoding scheme turns out to be a recurrent neural network
			-> connetion between neurons = dividied into internal and external connections
			-> interanl conenctions determined by neural network architecture of the unit
			-> external connectivity depends on the edges of the processed graphs

	C Learning algorithm

		Learning = estimating the parameter w such taht phiw approximates the data in the learning data set.

		L={(Gi, nij, tij)| Gi=(Ni, Ei) in g;
		                   nij in Ni;
		                   tij in Rm,
		                   1 <= i <= p,
		                   1 <= j <= qi}

		qi is the number of supervised nodes in Gi
			- for Graph focused tasks, qi=1, one special node used for the target
			- for node-focused tasks, supervision can be performed on every node


		Learning task as a minimization of a quadratic cost function:

		ew = SUM{i=1,p}SUM{j=1,qi}(tij - phiw(Gi,nij))^2


		remark4) ew may include another term to control other properties of the model
			- smoothing factor to penalize any abrupt changes of the outputs
			- to improve generalization


		Learning algo based on gradient decent. Steps

			a) xn(t) are iteratively updated by eq(5) until at time T they approach the fixed point solution of (2): x(T) ~= x

			b) the gradient Dew(T)/Dw is computed
			c) the weigths w are updated according to the gradient computed in step b)

			observations:
			- for a) if Fw is a contraction map, then convergence to the fixed point is ensured.
			- b) can be carried out very efficient by exploiting the diffusion process in GNN
				-> related to the diffusion process that takes place in recurrent (backpropagation through time algorithm)

				Fig 3
					from graph with li l(i,j) etc

					to encoding network with fw and gw for each node


					to unfolded encoding network

						RNN when fw and gw are implemented as feed forward nn.

				each layer of the unfolding encoding network corresponds toa time instant and contains a copy of all the units fw

				last layer, time T, contains the gw units

				Backpropagation through time
					compte the gradient of the cost func at time T with r.t. all the instances of fw, and gw.
					Dew(T)/Dw is obatined by summing the gradients of all instances.

					->requires to store the states of every instance of the units 
					-> if graph is large and T- t0 is large THIS CAN BE A PROBLEM.


				MOre Efficient approach 
					Almeida-Pineda algorithm
					5) stable point before gradient computation
					x(t) = x
					backpropagration throught time can be carried out by storing only x

					Theorem 1 (Differentiability)

					Theorem 2 (Backpropagation)

		Efficient learning algorithm
			Table 1 p 7

			FORWARD
				takes input the params w and iterates to find convergent point. Stops when || x(t) - x(t-1)|| < ef (threshold)

			BACKWARD
				eq(7) is iterated until ||z(t-1) - z(t)|| < eb threshold
				eq(8)
				then the gradient is calculated

			main procedure updates the weights
				until stoppped criteriont (accuracy or other)

			learning rate
				can use momentum, adaptive learning or other strategies..


		Other implementations not based on gradient descent are not straighforward.

	D transition and output function implementations


	gw: no particular constraint
		in GNNs , gw is a multilayered feedforward neural network.


	fw: important constraints
		implementation determines the number and the existence of the solutions

		fw such that global transition function Fw is a contraction map w.t.t. the state x.

		2 examples NN for fw, in non-positional form (3), but for positioinal form they exist as-well.

		1) linear (nonpositional) GNN. eq(3) implemented by
			hw(ln, l(n,u), xu, lu)=AnuXu + bn

			vector bn in Rs
			martix Anu in Rsxs 
				are defined by the output of 2 feedforward neural networks (FNNs) whose parameters correspond to the parameters of the GNN.

			transition network ->FNN that generates Anu
			forcing network ->FNN that genereates bn

			PHIw:  func implemented by transition network
			rhow:  func implemented by forcing network

			Anu = mu/s[ne[u]]·RESIZE
			bn = rhow(ln)

			with hyperbolic tangent activation func:
			Fw(x,l) = Ax+b
				b : stacking all bn
				A: block matrix with Anu block = Any if u is a neighbor of n and Anu block = 0 otherwise.

				A and b only depend on node and edge labels (not on state x)

				then contraction map proof


		2) nonlinear (nonpositional) GNN.
			hw is realized by a multilayered FNN

			3L NN are universal approximators, hw can approximate any desired function.

			but not all w parameters can be used -> must be ensured that the corresponding Fw is a contraction map
			-> adding a penalty term

			ew =SUM{i=1,p}SUM{j=1,qi}(tij - phiw(Gi,nij))^2 + betalL(|| DFw/Dx)

				L(y) is (y - mu)^2 if y > mu
				        0   otherwise

			penalty term can be any expression differentiable w.r.t. w that is monotoen increasing w.r.t. the norm of the Jacobian.
			ex:
				pw=SUM{i=1,s}L(||Ai||1)
				
					Ai = ith column of DFw/Dx
				which is an approximation of
					L(||DFw/Dw||1) = L(maxi||Ai||1)


	E Comparison with Random Walks and Recursive Neural Networks

	RNN are a special case of GNN
		1) input graph is a directed acyclic graph;
		2) input of fw are limited to ln and xch[n] where ch[n] is the set of children of n
		3) there is a supersource node sn from which all the other nodes can be reached, tipically the output node (graph focused tasks)

		architecture for realizing fw and gw include
			multilayered FNN
			cascade correlations
			self-organizing maps

		-> exlucde any sort of cyclic dependence
		->RNN the encoding networks are FNNs, simplifies the computation of the states, can be computed following a predefined ordering

	Random Walks on graphs
		->fw as a linear function

		random walks and Markov chain models 
			-ranknig algorithm for internet search engines

		random walks state:
			xn = SUM{i in pa[n]} a_ni·x_i

			pa[n] set of parents of n

			ani are nomramlized so SUM{i in pa[n]}a_in = 1

			ani: prob that walker visit i from n
			xn : prob that a walker is in xn in the steady state


		stacking all states
			x = Ax with A = {a_ni}

			instead of learnined A, A is a constant stochastic matrix  for random walks



III Computational complexity Issues
-----------------------------------

Study
- positional GNS by FNN for fw,gw 
- linear nonpositional GNNs
- nonlinear nonpositional GNNS


considerations &  notation

A. COmplexity of instructions




IV Experimental results
-------------------------

Succesful examples
	- image classificaition 77
	- object localization in images 78
	- web page ranking 79
	- relational learning 80
	- XML classification 81


Experiments shown
	- subgraph matching
	- mutagenesis
	- web page ranking

Facts that hold unless explicitly said they don't
	- experiments with linear and nonlinear GNNS
		linear GNNS
			gw, phiw, rhow
		nonlinear GNNs
			gw, hw
	- only nonpositional GNNs have been implemented and tested
		nonpositioanl transition func. slightly outperforms the positional one (accoridn to existing results on RNN) -> 


	-  all funcs (gw, phiw, rhow, gw and hw) 3L FNN with sigmoidal activation func.
	- results avg over 5 diff. runs
	- dataset of each run: random fraphs constructed by a procedure
		similar to ER?
	- training, validation and test set
		when only one G graph
		->validatin and test set include different supervised nodes of G

		validation set to see the network with lowerst cost -> best model -> test set

	- model performance
		accuracy in classificaiton probs
			tij = -1 or 1
			if phiw(Gi, nij)>0 and tij=1 -> correct
			elif phiw(Gi, nij)<0 and tij=-1 -> correct
			incorrect otherwise


		releateive error in regression probs
			tij in R
			rel.error = |(t - phiw)/t|

A . The Subgraph matching problem

	learn tau s.t. 
		tau(Gi, nij)=1
			if nij belongs to a subgraph of Gi, which is isomoprphic to S
		tau(Gi, nij)=-1
			otherwise

	applications
		- object localization
		- detection of active parts in chemical compounds
		- basic test to asses a method for graph processing.

	GNN is slower that other methods for subgraph matching
	but
	GNN is a general approach, can be used without modification to a variety of extensioins of the problem.
		-several graphs detected at the same time
		-graphs corrupted by noise on struct or labels
		-target unknown and provided only by examples

	600 connected random graphs -> training, test , validation
	S : randomly generated in each trial, inserted into every graph of the data set. Thus every Gi contained a copy of S at least.
		all nodes had integer lables [1,10]
		supervision: S was located by a bruteforce algorithm
		Gaussian noise (0,0.25) added to all the labels -> all copies of S were different due to the noise

	state dimension: s=5
	GNNS: 5 hidden neurons

	also a FNN applied to the test,: one output , 20 iden and one input units

	performance varies with |S|/|G|
	also with |S| 

	RESULT:
		- GNN outperform FNN
		- GNN can exploit label contents and graph topology at the same time
		- GNN nonlinear implement a more general model that can approximate a larger class of functions
		- GNN use a diffusion mechanism to decide whether a node belongs to the subgraph. S larger ->more info to be diffused-> more complex func. to be learned.
		- details on computation cost


B The Mutagenesis Problem

	mutagenesis data [13]
		- small dataset
		- benchmark in relational learning
		- benchmark in inductive logic programming

		- 230 nitroaromatic ompounds  common intermediate subrpoducts of many chemical reactions.
		- goal: learn to recognize the mutagenci compounds -> binary classification

		- 188 moelcules out of 230 are linear regression friendly, the 42 remaining are termed regression unfriendly

		- dataset, for each molecule:
			- atom-bond structure (AB)
			- chemical measurements (2 measures C and water/octanol)
			- structural (PS) attributes (2 other measures)
			- higher level featuers from AB -> functional groups (FG)

	results
		best using: AB; C and PS without functional groups

		data preprocessing:
			- graph for each molecule with atoms and edges (AB), nodel labesl contain aotm tyep, energu..,C, PS and AB + one supervised node the first atom in the AB description.
			- output is 1 if the molecule is mutagenic and -1 otherwise

		evaluation
			- non linear GNN vs other methods
			- 10-fold cv + results avg on 5 runs of cv
		
			- GNNS best accuracy on regresion unfriendly part and on the whole dataset. and close to the state of the art techniques on the regression-friendly part.

			- GNN had better overall result in regress. unfriendly part than in global, the opposit for the state of the art methods -> GNN can capture characteristics of the paterns that are suefulto solve the probbelm but are not homogeneously distributed in the two parts.

C Web Page Ranking

GOal: learn the rank of a web page (Google's PageRank)
	pn = dSUM{u in pa[n]}pu/on+(1-d)
	on: outdegree of n
	d in [0,1] is the dampoing factor

	GNN will learn a modified version of PageRank which adapts the authority measure according to the page content.

dataset:
	- random web graph G with 5000 nodes, generated with delta = 0.2
	- training, val, test sets with different nodes of this graph
	- 50 supervised nodes, 50 val, remaining in the test set

	- node has bilabel [an,bn] booleans say if topic an is present or if bn is present

	tau(G,n)= 2pn/||p||1 if (an XOR bn)=1
	          pn/||p||1  otherwise

	          pn: is the pageRank value of the node

	linear GNN model
Results
	- good performance on the test set
	- NO overfitting suggested by error func on the validation set by num epochs...


V Conclusion

- novel neural network model
- graphs: cyclic, directed, undirected, or a mixture of these.
- based on information diffusion and relaxation mechanisms
- extends into connectionist techniques for processing structured data, and random walk based methods.
- learning algorithmm provided
- experiments showed good performance on Mutagenesis