\documentclass[xcolor=table]{beamer}
\usepackage{adjustbox}
\usepackage{comment}
\usetheme{Madrid}
\usepackage{wrapfig}


\title{Graph Neural Network Applications}

% A subtitle is optional and this may be deleted
\subtitle{Master in Innovation and Research in Informatics - Master's Thesis}

\author{Pau~Rodriguez}

\institute[]
{
  Universitat Politecnica de Catalunya\\
  MIRI - Data Science
  }

\date{October 2019}

\subject{}

\pgfdeclareimage[height=0.5cm]{./img/Logo_UPC}{./img/Logo_UPC}
\logo{\pgfuseimage{./img/Logo_UPC}}

\begin{comment}
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}
\end{comment}
\begin{comment}


\end{comment}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
\end{frame}



%------------------------------------------------------------------------
\section{Introduction}
\begin{frame}{Introduction}
    \begin{block}{What are Graph Neural Networks}
    {
     %Present a summarized view of Graph neural networks   
     %Present trends       
    }
    \end{block}

\end{frame}


%------------------------------------------------------------------------
\begin{frame}{Introduction}
    \begin{block}{What are Graph Neural Networks}
    {
    % Main papers and their applications in topics/areas list     
    }
    \end{block}

\end{frame}


%--------------------------------------------------------------------
\begin{frame}{Introduction}
    \begin{block}{Goals}
    {
     %The goal of this masterâ€™s thesis is to apply Graph Neural Network models to different problems to create a novel solution. The idea is to get to know how Graph Neural Networks are used in each situation. Two problems are explored: Girvan-Newmann algorithm approximation and compiled code function classification. They correspond to the two main tasks that Graph Neural Network perform with success, semi-supervised learning of nodes on a graph and supervised graph classification.        
    }
    \end{block}
    \begin{block}{Motivation}
    {
    % Graph Neural Networks seem to be a promising way of solving graph-related problems, with applications in many domains. The time seems right to jump into learning about the most recent models since they have attained state-of-the-art performance on some of the tasks they have solved.
    }
    \end{block}
\end{frame}

%------------------------------------------------------------------------REMOVE? -> TOC IS THE SAME
\begin{frame}{Introduction}
    \begin{block}{Organization}
    {
%      %The thesis is organized in the following way: the next section will explain the state-of-the-art in Graph Neural
% Network models by presenting the may models, their internals and the problems they have solved. Then, in
% section three, the methodology followed in the experiments is presented. After that, section four will go through
% an overview of the implementation of the experiments, whereas in section five the results of the experiments are
% summarized. Finally, in section six the conclusion of the thesis is presented.       
    }
    \end{block}
    
\end{frame}



%------------------------------------------------------------------------
\section{State of the art}
\begin{frame}{State of the art}{Nomenclature}
% summarize Notation subsection from SOTA on report G,V,E,...

\end{frame}



%------------------------------------------------------------------------
\begin{frame}{State of the art}{Euclidean space}
% Euclidean space definition
% why Graph are considered non Euclidean

%avoid??

\end{frame}




%------------------------------------------------------------------------
\begin{frame}{State of the art}{Graph Neural Networks}

% Graph Neural Network definitioin: Aggre, Combine, readout

% alternatives quick mention: hand-engineerred features, kernel funcs, markov-random walk embeddings

\end{frame}





%------------------------------------------------------------------------
\begin{frame}{State of the art}{Graph Neural Networks}


% Usual architecture: embedding + downstream ML algorithm


% tasks: node classif/regr, (sub)graph classif/regre 
%       -> repr. learning
% setupt: semi-supervised node repr. learning, and supervised graph repr. learning



\end{frame}




%------------------------------------------------------------------------
\begin{frame}{State of the art}{Graph Convolutional Networks}

% p6 sota GCN definition

% clarification: Kipf GCN versus broad type of convolutional networks
% convolution: sharing parameters


\end{frame}



%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spectral methods}

% spectral methods characteristics pros and cons


\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods}
% spatial methods pros and cons
\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods - GCN}

%highlights 
%formula

\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods - MPNN}

%highlights 
%formula

\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods - GraphSage}

%highlights 
%formula

\end{frame}



%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods - GGNN}

%highlights 
%formula

\end{frame}





%------------------------------------------------------------------------
\begin{frame}{State of the art}{Spatial methods - GIN}

%highlights 
%formula

\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Other GNNs}

% G attention networks
% G AUto-encoders
% G Generative Networks
% G spatial-temporal networks
\end{frame}


%------------------------------------------------------------------------
\begin{frame}{State of the art}{Successful applications of GNN}

%CV
%Recsys
% BIochemistry
% Communication Networks modelling
% Program verification
\end{frame}



\section{Experiments}
%------------------------------------------------------------------------REMOVE(go directly to first experiment)
\begin{frame}{Experiments}
    \begin{block}{General goal}{
    % Apply GNN to 2 experiments: one covering semi-supervised node regression(classification)
    % the other covering supervised graph classification in the domain :

    }\end{block}
    \begin{block}{General motivation}{
        % Make use of those great GNN algorithms in novel ways by applying them to new areas

    }\end{block}
    \begin{block}{Methodology}{
        % adaptation of CRIPS-DM without business understanding and deployment
        % for model selection & evaluation: hyper-parameter search with cross-validation for each type of model
        % types of models: from simple baselines to more complex ones
    }\end{block}



\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{}

% Girvan-newman algorithm
\begin{block}{Girvan-Newman algorithm approximation}{
       Using Graph Neural Networks to approximate the Girvan-Newman algorithm for community detection in graphs.
    }\end{block}


\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Context}

% Girvan-newman algorithm
\begin{block}{Girvan-Newman algorithm}{
        Clustering based approach algorithm for community detection that iteratively isolates groups of nodes of a graph by removing the edge with highest edge betweenness.
    }\end{block}


\begin{itemize}
    \item Edge betweenness: number of shortest paths between pairs of nodes that run along an edge
    \item Girvan-Newman time complexity: $O(m^{2}n)$ with $m$ edges and $n$ vertices
    %computing all the shortest paths that run along each edge is very costly, and must be recomputed at each iteration
\end{itemize}


\end{frame}



%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Goals and motivation}

\begin{itemize}
    % Goal: find a novel way to approximate the GN algorithm
    \item \textbf{Goal:} approximate the edge betweenness computation used in the Girvan-Newman algorithm by using Graph Neural Networks. 
    % show that it is possible
    % see how good the approximation is
    % see if the time cost is reduced
    \item \textbf{Motivation:} community detection is a popular task in social network analysis. Reducing the time cost of this algorithm would be very useful.
    % motivation: community detection popular task
\end{itemize}


\end{frame}


%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Description}


A Graph Neural Network has been trained to approximate the edge betweenness computation with a \textbf{semi-supervised node classification} task.\\

Discrete ranges of values of the edge betweenness are generated by percentiles.\\

% better performance if doing classification

\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Description}


The implementation of the Girvan-Newman approximation would be:
\begin{itemize}
    \item Compute the edge betweenness of some edges of the graph
    \item Approximate the edge betweenness of the rest of edges of the graph (model prediction).
    \item Selection of the most central edge, and iterate.
\end{itemize}

\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Results}

\input{./tables/exp1_results.tex}


\end{frame}





%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Results}


\begin{figure}[H]
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/scatter_plot_20epochs.png}
\endminipage
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/histogram_20epochs.png}
\endminipage
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/boxplot_20epochs.png}
\endminipage
\caption{Predictions compared to target values in an approximation of the edge betweenness using 28 discrete ranges and 20 epochs of training (best accuracy model) }\label{fig:edgeb_exp1_10bins}
\end{figure}

\end{frame}



%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Results}


\begin{figure}[H]
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/scatter_plot_20epochs.png}
\endminipage
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/scatter_plot_250epochs.png}
\endminipage
\minipage{0.32\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/GN_exp1/28bins/scatter_plot_600epochs.png}
\endminipage
\caption{Predictions compared to target values in an approximation of the edge betweenness using 28 ranges with 20, 250 and 650 epochs of training}\label{fig:edgeb_exp1_20bins}
\end{figure}

\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Conclusion}

% how good/bad is it?
\begin{itemize}
    \item The best trained model obtains a 34.3\% accuracy in a 28 discrete ranges setup.
    \item Limitation: the maximum edge betweenness is assigned to a group of edges and there is a limit on the number of discrete ranges that can be used.
    \item Model performance is highly dependent on the dataset split.
\end{itemize}
% how it could be improved even more?

\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Experiment 1 - Girvan-Newman algorithm approximation }{ Next steps}

% ways to apply this to Girvan-Newman approximation
\begin{itemize}
    \item Perform a longer hyper-parameter search.
    %\item Study if there is a better data partitioning approach 
    \item Test the model on different heterogeneous graphs to see how stable the model is.
    \item Create a smaller range for high edge betweenness values (and compensate for class imbalance).
    \item Use a different performance metric that benefits the detection of high edge betweenness values.
    \item Test an inductive setting.
\end{itemize}
\end{frame}






%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{}


\begin{block}
    {
    Compiled code function classification
     }
    {
        Assembler code snippets renaming using Graph Neural Networks for graph classification

    }
\end{block}


\end{frame}





%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Context}

%Malicious software analysis overview


\begin{figure}
    \includegraphics[scale=0.32]{./img/malware_static_analysis.png}
\end{figure}


Malware = malicious software, computer virus


% explain: reverse engineering malware (static analysis) = 
%             search for clues that help antivirus programs detect malicious code
%             like network communications, destination ip addresses,
%              destination domain names
%              files that are modified on the file system
%              mutexes (to avoid reinfection or multiple execution of the malicious code)
%             THE PROBLEM: It takes a lot of time
%             ONE


% Static because it involves reading code and not executing it and registering the events (which is another type of analysis)

% the task in the experiment 2 will try to help professionals working in stesp 3 and 4, which involv reading assembler code

\end{frame}





%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Context}

MALWARE STATIC ANALYSIS: understanding functionalities and searching for artifacts by analyzing assembler code.

% image of disassembled code:
%            ida pro free
%            a call to an unknown function 
%            code with comments on that call for example
%            function names and

\begin{figure}
    \includegraphics[scale=0.35]{./img/slides_exp2-context01c.png}
\end{figure}



% explain: compiled binary, 
%                - code that is transformed to an architecture dependent machine code
%                - so the processor can load it into memory and execute instruction by instruction
% explain: assembler language, 
%                % - allows to translated machine code directly into assembler language instructions
% explain: disassembler program,
%                - a program that reads a compiled code, interpres file structure and machine code into assembler language


\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Context}


% explanation of the idea: rename function to help the analyst look faster for clues
Is it possible to rename the functions to help the analyst in searching specific functionalities within the code?



\nocite{code2vec} \nocite{139} 

%\nocite{code2vec,129}
\bibliographystyle{unsrt}
\bibliography{references}


%code2vec uses abstract syntax trees to generate embeddings of functions that allow for later classification and then renaming of functions. It is performed on more complex syntax programming languages, with examples from free available code on github.

%Lergning to represent programs with graphs, they use GGNN to generate embeddings from snippets of code (again code availabe from github) and then they apply it for tasks like variable renaming.

% that was my source of inspiration: task inspiration form code2vec, gnn application comes from Allamanis paper.


\end{frame}




%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Goals and Motivation}

\begin{block}{Goal}{
       
        % Goal classify snipets of assembler code (subroutines or functions)
        Classify snippets of assembler code (subroutines or functions) into well known general purpose functionalities (Network, Disk, Encryption,etc)

    }\end{block}
    \begin{block}{M otivation}{
        Speed up the process of reverse engineering malicious code by adding some indications on the main functionality of assembler functions.
    }\end{block}
\end{frame}


%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Description}

% data gathering

% data transformation

\begin{figure}
    \includegraphics[scale=0.24]{./img/Features_and_models_diagram.png}
\end{figure} 


% There 3 main steps in this experiment, which are 
%  chosing a set software binaries and extracting their assembler language code
% finding a suitable labeling strategy

% then transforming this assembler language code into features for the algorithms: code features(counts of instructions, registers,), the graph of the code (a graph derrived from the execution flow of instructions and the operands of each instruction), and summary statistics of each graph (average node degree, or centrality measures).

% finally the models. The bagOfWOrds models are seentialy like the baseline but with a layer that will select the best parameters of the bag of words embedding
% hyper-parameter search using cross- validation (training and test splits and then k-fodl cross-validation)
% from more simple to more complex models
% goal is to see if gnn models can outperform more basic models




\end{frame}


%-----------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Description}

Generating and labeling the dataset



\begin{figure}[H]
\minipage{0.5\textwidth}

Code source:
\begin{itemize}
    \item Apache http server,
    \item Nginx http server,
    \item Filezilla fpt client,
    \item OpenSSL libraries,
    \item GlibC
\end{itemize}
\endminipage
\hfill
\minipage{0.5\textwidth}%
Compiled with debugging information (function names).\\

Original function names where grouped into class labels by a semi-automated rule-based keyword appearance procedure.
\\

Versions: v1 (10 classes), v2(120 classes), v3(24 classes)

\endminipage
\end{figure}



% assembler code file + debuggin info(function name)
% rule based label from 

% compilation with debugging information
%selected software: well-known Apache http, fielzill ftp client, nginx, httppser server, openssl and glibc


%number of classes in each approach

\end{frame}





%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Description}

% data gathering

% data transformation
Feature extraction


\begin{figure}[H]
\minipage{0.5\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/code_graph02.png}
\endminipage
\minipage{0.5\textwidth}%
  \centering
    \includegraphics[width=0.9\linewidth]{img/code_graph01.png}
\endminipage
%\caption{Graph created from a fragment of assembler language code}
\end{figure}

\begin{itemize}
\item Extraction of features from code listing\\
\item Graph generation from the code listing\\
\item Extraction of topological features from the graph
\item Bag-of-words embedding (tokenization plus filtering by TF-IDF)
\end{itemize}
% graph of instructions created from the execution flow. 
% each instruction is linked to its previous and following instruction
% when jumps or calls are found, there is a link to a third instruction 
% operands of each instruction are also linked to it


\end{frame}



%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Description}

Machine learning models
%  Models used: describe quickly
\begin{itemize}
    % Baseline models
    \item \textbf{Baseline models}: Logistic Regression, Decision Trees, Random Forest, XGBoost, Neural Network (MLP). Code and graph topological features as input.

    % Bag of word with tf-idf
    \item \textbf{Bag-of-words models}: baseline models with Bag-of-word embedding filtered by TF-IDF as input

    % GNN: GCN, GraphSage, GIN, GGNN
    \item \textbf{Graph Neural Networks}: GCN, GraphSage, Gated Graph Neural Network, Graph Isomorphism Network. Graph as input.
\end{itemize}

\end{frame}


%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Results}

\input{./tables/v1_results_resized.tex}



\end{frame}


%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Results}

\input{./tables/v3_results_resized.tex}
\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Conclusion}


% conclusion on why topological features don't give as much signal as code features
% features vs classes boxplots?
\begin{itemize}
    \item Graph Neural Networks can be used for classification of code snippets
    \item Bag-of-word embedding carries more signal than the graph
    \item GNN training is highly influenced by how data is partitioned
    \item GNN training is costly
\end{itemize}

\end{frame}

%------------------------------------------------------------------------
\begin{frame}{Experiment 2 - Compiled code function classification}{Next steps}

% improvements
\begin{itemize}
    \item Combine graphs with Bag-of-words embedding (node or graph level?)
    \begin{figure}[H]

      \centering
        \includegraphics[width=0.9\linewidth]{img/exp2_improvements.png}

    \end{figure}

    \item Define a more constrained scenario, a more granular approach
\end{itemize}

\end{frame}


\section{Conclusion}
%------------------------------------------------------------------------
\begin{frame}{Conclusion}{Results summary}

\end{frame}



%------------------------------------------------------------------------
\begin{frame}{Conclusion}{Next steps summary}

\end{frame}



% %\nocite{*}
% \bibliographystyle{unsrt}
% \bibliography{references}












% %-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

% \section{Multivariate Analysis}

% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{Dataset}
% Our initial dataset: House sales in king county
% \input{./tables/dataset.tex}
% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{Preprocessing}
% \begin{itemize}
%     \item \textbf{Missing data and Errors - } The dataset is complete (no blanks\textbackslash Na's) 
%     \item \textbf{Outliers - } Using Robustified Mahalanobis distance, we remove 8 individuals 
% \end{itemize}

% \begin{figure}
% \includegraphics[scale=0.45]{./img/mahalanobis.png}
% \end{figure}

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: plots}

% \begin{figure}
% \includegraphics[scale=0.27]{./img/pairs-first.png}
% \end{figure}

% %We plot different variables of the dataset compared to the price, to start to graps what are the most important variables, what variables are possible candidates to be transformed.

% \end{frame}

% \begin{frame}{Multivariate Analysis}{EDA: histograms}

% \begin{figure}
% \includegraphics[scale=0.4]{./img/histograms.png}
% \end{figure}


% % we also plot histograms of each variables to see if they are drawn from a single distribution or not. etcetera
% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: transformations}
% Logarithms and ratios
% \begin{figure}
% \includegraphics[scale=0.37]{./img/transformations.png}
% \end{figure}

% % here we have...
% %ratios capture same information in a lower dimension?
% %logs change the structure, correct skewness


% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: transformations}
% Reducing skewness with logarithms.
% \begin{figure}
% \includegraphics[scale=0.32]{./img/log_skewness.png}
% \end{figure}

% %Logarithmic transformation can be useful to reduce skewness(for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics). 
% % Not always worked so well.
% %>We also tried the Box-Cox transform, but didn't get better results than with this
% % we decided to create transformations for all continuous vars and then perform feature selection by experiments

% \end{frame}



% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: correlation}
% For each highly correlated pair we remove one of the variables.
% \vspace*{-5pt}
% \begin{figure}
% \includegraphics[scale=0.27]{./img/corr01.png}
% \end{figure}
% %\vspace*{-35pt}
% %we transform the variables using logarithms and ratios. sqft_living log transformation, bedroom/floor, floor/bathrooms, bathrooms/bedrooms = which can give an idea of the amount of money spent on the house, logarithm transform som echange of aspect.

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: correlation}
% For each highly correlated pair we remove one of the variables.
% \vspace*{-5pt}
% \begin{figure}
% \includegraphics[scale=0.27]{./img/corr02.png}
% \end{figure}
% %\vspace*{-35pt}
% % sqft_lot15/sqft_lot15, sqft_living15/sqft_living, sqft_above/sqft_living
% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{EDA: correlation}
% For each highly correlated pair we remove one of the variables.
% \vspace*{-5pt}
% \begin{figure}
% \includegraphics[scale=0.25]{./img/corr03.png}
% \end{figure}
% %\vspace*{-35pt}

% \end{frame}



% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{Unsupervised analysis: PCA}
% Correlations between variables and target(Price)
% \vspace*{-8pt}
% \begin{figure}
% \includegraphics[scale=0.34]{./img/PCA.png}
% \end{figure}
% %sqft_lot not correlated (almost orthogonal)
% %the rest of continuous variables (sqft_living, bedrroms, sqft_above, grade, bathrroms) highly correlated between them and with the price
% % correlation between predictor variables is generally bad for training the models

% \end{frame}



% %------------------------------------------------------------------------------
% \begin{frame}{Multivariate Analysis}{Unsupervised analysis: Hierarchical Clustering}

% Assumption: only one cluster
% \begin{figure}
% \includegraphics[scale=0.4]{./img/Hclustering.png}
% \end{figure}
% % worried about if there are different clusters for the data
% % after performing a hierarchycal clustering we conclude we can assume only one cluster
% % other wise, we would be forced to train different models for each cluster, and separate train and testing data according to the proximity to the centroid of each of the clusters.

% \end{frame}

% %Feature extraction
% %Feature extraction - logs, ratios: which ones
% %Feature extraction - correlation : the subsets we select 
% %Feature extraction - PCA  -> how we build the features + target


% \section{Feature Extraction}
% %------------------------------------------------------------------------------
% \begin{frame}{Feature Extraction}
% For each transformation: 
% \begin{itemize}
%     \item logarithms
%     \item ratios
%     \item removing highly correlated variables
%     \item significant principal components from PCA
% \end{itemize}
% we create new datasets by binding the new variables with the original target variable.

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Feature Extraction}
% Derived datasets:
% \input{./tables/datasets.tex}

% \end{frame}


% %models
% %Models - methodology
% %Models - selected models (maybe in one single slide all of them!)
% %Models - selected models - linear
% %Models - selected models - Ridge regression
% %Models - selected models - Lasso regression
% %Models - selected models - Decision trees for regression
% %Models - selected models - Random Forest for regression
% \section{Models}
% %------------------------------------------------------------------------------
% \begin{frame}{Models}{Methodology}

% Common training model methodology:
% \begin{itemize}
%     \item \textbf{Split dataset:} randomly split the training and test dataset 
%     \item \textbf{Model selection:}model selection(hyperparameters) by cross validation over the training set
%     \item \textbf{Refit:} refit of the model over all the training set with selected hyper parameters
%     \item \textbf{Generalization:} prediction of the model over test set to compute the testing error
% \end{itemize}
% We use the NRMSE.

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Models}{Methodology}

% Selected types of models:
% \begin{itemize}
%     \item Linear regression
%     \item Ridge regression
%     \item Lasso regression
%     \item Decision trees
%     \item Random forest
% \end{itemize}

% \end{frame}

% %Experiments
% %Experiments - overview
% %Experiments - initial exploration
% %Experiments - Feature selection
% %Experiments - Refinement
% %Experiments - Results
% \section{Experiments}
% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Methodology overview}
% Our approach consists on exploration with final refinement:
% \begin{enumerate}
%     \item \textbf{Space exploration - }train all models, each with all the generated datasets and select the best 3.
%     \item \textbf{Feature selection - }for each model and dataset,
%     \item \textbf{Refit - }refit the best 3 models after feature selection.
% \end{enumerate}

% \end{frame}

% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Space exploration}
% \vspace*{-5pt}
% Fragment of the experiments during exploration
% \vspace*{2pt}
% \input{./tables/exp_models_vs_featuresets_presentation.tex}
% \end{frame}

% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Space exploration}
% Sorted plot of the validation, training and testing error (NRMSE)
% \vspace*{-7pt}
% \begin{figure}
% \includegraphics[scale=0.5]{./img/exp01.png}
% \end{figure}
% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Space exploration}
% Selected models and datasets 
% \vspace*{3pt}
% \input{./tables/top3.tex}
% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Feature selection}

% Evolution of the validation error in the Backward selection algorithm execution for the Lasso model. We had a minor improvement but in the end the dataset was smaller.
% \vspace*{-7pt}
% \begin{figure}
% \includegraphics[scale=0.3]{./img/lasso_SBS.png}
% \end{figure}

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Feature selection}

% Evolution of the validation error in the Backward selection algorithm execution for the Random Forest model.
% We can see that removing latitude or longitude has negative impact on the validation error whereas removing sqrt\_lot will reduce the validation error in 1\%.
% \vspace*{-7pt}
% \begin{figure}
% \includegraphics[scale=0.3]{./img/SBS.png}
% \end{figure}

% \end{frame}


% %------------------------------------------------------------------------------
% \begin{frame}{Experiments \& Results}{Refit \& Result}

% Final result
% \vspace*{3pt}
% \input{./tables/final.tex}
% \end{frame}



% %Conclusion
% %Conclusion - Presenting results
% %Conclusion - comparison of models/algorithms
% %Conclusion - Final impressions
% \section{Conclusion}
% %------------------------------------------------------------------------------
% %\begin{frame}{Conclusion}
% %    \begin{block}{\center Million dollar question}
% %    \center What is the best combination of feature set and model? 
% %    \end{block}
% %    \begin{block}{}
% %    Sequential forward selection performance:
% %    \begin{itemize}
% %        \item Choose sub-optimal feature set for random forest $\rightarrow$ that led to higher validation error.
% %        \item Decision trees and Lasso regression are less sensitive and SFS choose good feature set that preforms as good as found by exploration.
% %    \end{itemize}
% %    \end{block}
% %    \begin{block}{}
% %    During exploration RF overcome all other models. However, with SFS the generated forests under-preformed even compared to the decisions tree. 
% %    \end{block}
% %\end{frame}


% \begin{frame}{Conclusion}
%     \begin{block}{\center Million dollar question}
%     \center What is the best combination of feature set and model? 
%     \end{block}
%     \begin{block}{}
%     Sequential Backward selection performance:
%     \begin{itemize}
%         \item Random forest preformed better with smaller subset.
%         \item Decision trees and Lasso regression are less sensitive and SBS choose good feature set that preforms as good as found by exploration.
%     \end{itemize}
%     \end{block}
%     \begin{block}{}
%     During exploration RF overcome all other models and got even better performance using SBS.
%     Trying to use SFS, the generated RF under-preformed even compared to the decisions tree, it happens because the SFS lead to local minimum. 
%     \end{block}
% \end{frame}

% %------------------------------------------------------------------------------
% %\begin{frame}{Conclusion}
% %    \begin{block}{\center why?}
% %    What is the best combination of feature set and model? 
% %    \begin{itemize}
% %        \item  Random forest expected to be the best model $\rightarrow$ but highly dependent on the dataset.
% %        \item Decision trees and Lasso regression are less sensitive to the dataset.
% %    \end{itemize}
% %    \end{block}

% %\end{frame}

% %------------------------------------------------------------------------------
% \begin{frame}{Conclusion}{Future work}
% \begin{block}{Proposed improvements}
% \begin{itemize}
%     \item Test more advanced models: Splines, RVM, Bayesian approach to regression
%     \item Perform a deeper exploration of the possible features and combinations of features
%     \item Explore better feature selection methods: Bi-directional search, Plus-L Minus-R selection etc.
% \end{itemize}
% \end{block}
% \end{frame}

\end{document}


