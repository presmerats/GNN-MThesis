{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import product\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "#from datasets import get_dataset\n",
    "#from train_eval import cross_validation_with_val_set\n",
    "\n",
    "#from gcn import GCN, GCNWithJK\n",
    "#from graph_sage import GraphSAGE, GraphSAGEWithJK\n",
    "#from gin import GIN0, GIN0WithJK, GIN, GINWithJK\n",
    "#from graclus import Graclus\n",
    "#from top_k import TopK\n",
    "#from diff_pool import DiffPool\n",
    "#from sag_pool import SAGPool\n",
    "#from global_attention import GlobalAttentionNet\n",
    "#from set2set import Set2SetNet\n",
    "#from sort_pool import SortPool\n",
    "\n",
    "import time, os\n",
    "import argparse\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import inspect\n",
    "import pkgutil\n",
    "from importlib import import_module\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, JumpingKnowledge\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir+'/graph_classification') \n",
    "\n",
    "from TFM_graph_classification import *\n",
    "from TFM_graph_classification_models import *\n",
    "from TFM_function_renaming_dataset_creation import *\n",
    "\n",
    "from TFM_function_renaming_dataset_creation import *\n",
    "from TFM_function_renaming_dataset_creation import FunctionsDataset\n",
    "from TFM_function_renaming_baseline_models import *\n",
    "from TFM_function_renaming_preprocess_dataset_splits import *\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('exp2')\n",
    "#logger.basicConfig(filename='exp2-gnn.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "f_handler = logging.FileHandler('exp2-gnn.log')\n",
    "f_handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(f_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GCNWithJK(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, mode='cat'):\n",
    "        super(GCNWithJK, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        self.jump = JumpingKnowledge(mode)\n",
    "        if mode == 'cat':\n",
    "            self.lin1 = Linear(num_layers * hidden, hidden)\n",
    "        else:\n",
    "            self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.jump.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            xs += [x]\n",
    "        x = self.jump(xs)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import GINConv, global_mean_pool, JumpingKnowledge\n",
    "from torch_geometric.nn import SAGEConv, GlobalAttention\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, JumpingKnowledge\n",
    "from torch_geometric.nn import SAGEConv, Set2Set\n",
    "from torch_geometric.nn import SAGEConv, global_sort_pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SortPool(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes):\n",
    "        super(SortPool, self).__init__()\n",
    "        self.k = 10\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.lin1 = Linear(self.k * hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class Set2SetNet(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes):\n",
    "        super(Set2SetNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.set2set = Set2Set(hidden, processing_steps=4)\n",
    "        self.lin1 = Linear(2 * hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.set2set.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.set2set(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GraphSAGEWithJK(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, mode='cat'):\n",
    "        super(GraphSAGEWithJK, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.jump = JumpingKnowledge(mode)\n",
    "        if mode == 'cat':\n",
    "            self.lin1 = Linear(num_layers * hidden, hidden)\n",
    "        else:\n",
    "            self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.jump.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            xs += [x]\n",
    "        x = self.jump(xs)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    " \n",
    "class GlobalAttentionNet(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes):\n",
    "        super(GlobalAttentionNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden, hidden))\n",
    "        self.att = GlobalAttention(Linear(hidden, 1))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.att.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        x = self.att(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cross_validation_with_val_set(dataset,model, folds, epochs, batch_size,\n",
    "                                  lr, lr_decay_factor, lr_decay_step_size,\n",
    "                                  weight_decay, logger=None):\n",
    "\n",
    "    \n",
    "    val_losses, accs, durations, f1macros = [], [], [], []\n",
    "    val_f1macros  =[]\n",
    "    for fold, (train_idx, test_idx,\n",
    "               val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "\n",
    "        train_dataset = dataset[train_idx]\n",
    "        test_dataset = dataset[test_idx]\n",
    "        val_dataset = dataset[val_idx]\n",
    "\n",
    "        if 'adj' in train_dataset[0]:\n",
    "            train_loader = DenseLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_loader = DenseLoader(val_dataset, batch_size, shuffle=False)\n",
    "            test_loader = DenseLoader(test_dataset, batch_size, shuffle=False)\n",
    "        else:\n",
    "            train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "        model.to(device).reset_parameters()\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = train(model, optimizer, train_loader)\n",
    "                val_losses.append(eval_loss(model, val_loader))\n",
    "                val_f1macros.append(eval_f1macro(model, val_loader))\n",
    "                accs.append(eval_acc(model, test_loader))\n",
    "                f1macros.append(eval_f1macro(model, test_loader))\n",
    "                eval_info = {\n",
    "                    'fold': fold,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_losses[-1],\n",
    "                    'val_f1macro': val_f1macros[-1],\n",
    "                    'test_acc': accs[-1],\n",
    "                    'test_f1-macro': f1macros[-1],\n",
    "                    'classif_report': eval_classification_report(model, test_loader)\n",
    "                }\n",
    "\n",
    "                if logger is not None:\n",
    "                    logger(eval_info)\n",
    "\n",
    "                if epoch % lr_decay_step_size == 0:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)\n",
    "        except:\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss = 1e1000000\n",
    "                val_losses.append(1e1000000)\n",
    "                val_f1macros.append(0.0)\n",
    "                accs.append(0.0)\n",
    "                f1macros.append(0.0)\n",
    "                eval_info = {\n",
    "                    'fold': fold,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_losses[-1],\n",
    "                    'val_f1macro': val_f1macros[-1],\n",
    "                    'test_acc': accs[-1],\n",
    "                    'test_f1-macro': f1macros[-1],\n",
    "                    'classif_report': eval_classification_report(model, test_loader)\n",
    "                }\n",
    "\n",
    "                if logger is not None:\n",
    "                    logger(eval_info)\n",
    "\n",
    "            t_end = time.perf_counter()\n",
    "            durations.append(t_end - t_start)\n",
    "\n",
    "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "    loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "    loss, argmin = loss.min(dim=1)\n",
    "    val_f1macro = tensor(val_f1macros)\n",
    "    val_f1macro = val_f1macro.view(folds, epochs)\n",
    "    val_f1macro = val_f1macro[torch.arange(folds, dtype=torch.long), argmin]\n",
    "    \n",
    "    acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "    f1macro = tensor(f1macros)\n",
    "    f1macro = f1macro.view(folds, epochs)\n",
    "    f1macro = f1macro[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "    loss_mean = loss.mean().item()\n",
    "    val_f1macro_mean = val_f1macro.mean().item()\n",
    "    acc_mean = acc.mean().item()\n",
    "    acc_std = acc.std().item()\n",
    "    f1macro_mean = f1macro.mean().item()\n",
    "    f1macro_std = f1macro.std().item()\n",
    "    duration_mean = duration.mean().item()\n",
    "    print('Val Loss: {:.4f}, Val f1macro: {:.4f}, Test Accuracy: {:.3f} ± {:.3f},Test F1-macro: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
    "          format(loss_mean, val_f1macro_mean, acc_mean, acc_std, f1macro_mean, f1macro_std, duration_mean))\n",
    "\n",
    "    return loss_mean, acc_mean, acc_std\n",
    "\n",
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    #for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def eval_f1macro(model, loader, labels=list(range(10)) ):\n",
    "    model.eval()\n",
    "\n",
    "    #correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        y_pred.extend(pred.flatten().tolist())\n",
    "        y_true.extend(data.y.flatten().tolist())\n",
    "        #correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    from pprint import pprint  \n",
    "    \n",
    "    #print(len(y_true),len(y_pred))\n",
    "    #pprint(y_true)\n",
    "    #pprint(y_pred)\n",
    "    #pprint(labels)\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro', labels=labels)  \n",
    "    #return correct / len(loader.dataset)\n",
    "\n",
    "    \n",
    "def eval_classification_report(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    #correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        y_pred.extend(pred.flatten().tolist())\n",
    "        y_true.extend(data.y.flatten().tolist())\n",
    "        #correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "        \n",
    "    return classification_report(y_true, y_pred, output_dict=True)  \n",
    "    #return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        loss += F.nll_loss(out, data.y.view(-1), reduction='sum').item()\n",
    "    return loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(info):\n",
    "    from pprint import pprint\n",
    "    fold, epoch = info['fold'] + 1, info['epoch']\n",
    "    val_loss, test_acc, test_f1macro, report = info['val_loss'], info['test_acc'], info['test_f1-macro'], info['classif_report']\n",
    "    val_f1macro = info['val_f1macro']\n",
    "    print('{:02d}/{:03d}: Val Loss: {:.4f}, Val F1macro: {:.4f},Test Accuracy: {:.3f}, Test F1-macro: {:.3f}'.format(\n",
    "        fold, epoch, val_loss,val_f1macro, test_acc,test_f1macro))\n",
    "    #pprint(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf add to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max/training_set/processed\n",
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max/test_set/processed\n"
     ]
    }
   ],
   "source": [
    "from TFM_function_renaming_nlp_models import *\n",
    "#add_tfidf_to_dataset('./tmp/symbols_dataset_3_precomp_split_undersample_max')\n",
    "add_tfidf_to_dataset_with_a_trick('./tmp/symbols_dataset_3_precomp_split_undersample_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set/processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__cat_dim__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__inc__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'apply',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'code_feats',\n",
       " 'contains_isolated_nodes',\n",
       " 'contains_self_loops',\n",
       " 'contiguous',\n",
       " 'debug',\n",
       " 'edge_attr',\n",
       " 'edge_index',\n",
       " 'filename',\n",
       " 'from_dict',\n",
       " 'is_coalesced',\n",
       " 'is_directed',\n",
       " 'is_undirected',\n",
       " 'keys',\n",
       " 'label',\n",
       " 'num_edge_features',\n",
       " 'num_edges',\n",
       " 'num_faces',\n",
       " 'num_features',\n",
       " 'num_node_features',\n",
       " 'num_nodes',\n",
       " 'pos',\n",
       " 'tfidf_vec',\n",
       " 'to',\n",
       " 'x',\n",
       " 'x_topo_feats',\n",
       " 'x_topo_times',\n",
       " 'y']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "#dataset.gnn_mode_off()\n",
    "dir(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1360, 0.0000, 0.0000, 0.0000, 0.0000, 0.0506, 0.0000, 0.0000, 0.0000,\n",
       "        0.1012, 0.0000, 0.0000, 0.4687, 0.4687, 0.2654, 0.0630, 0.0713, 0.0000,\n",
       "        0.0000, 0.4129, 0.0000, 0.0645, 0.0671, 0.0000, 0.0000, 0.1091, 0.0000,\n",
       "        0.0000, 0.0000, 0.1338, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2912, 0.0000, 0.0000,\n",
       "        0.0000, 0.3470, 0.0000, 0.1437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0504, 0.0506, 0.0000,\n",
       "        0.0544, 0.0830, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1360, 0.0000, 0.0000, 0.0000, 0.0000, 0.0506, 0.0000, 0.0000, 0.0000,\n",
       "        0.1012, 0.0000, 0.0000, 0.4687, 0.4687, 0.2654, 0.0630, 0.0713, 0.0000,\n",
       "        0.0000, 0.4129, 0.0000, 0.0645, 0.0671, 0.0000, 0.0000, 0.1091, 0.0000,\n",
       "        0.0000, 0.0000, 0.1338, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2912, 0.0000, 0.0000,\n",
       "        0.0000, 0.3470, 0.0000, 0.1437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0504, 0.0506, 0.0000,\n",
       "        0.0544, 0.0830, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_1_precomp_split_undersample_max/training_set/processed\n",
      "processed_path tmp/symbols_dataset_1_precomp_split_undersample_max/test_set/processed\n"
     ]
    }
   ],
   "source": [
    "from TFM_function_renaming_nlp_models import *\n",
    "add_tfidf_to_dataset('./tmp/symbols_dataset_1_precomp_split_undersample_max')\n",
    "#add_tfidf_to_dataset_with_a_trick('./tmp/symbols_dataset_1_precomp_split_undersample_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set/processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__cat_dim__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__inc__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'apply',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'code_feats',\n",
       " 'contains_isolated_nodes',\n",
       " 'contains_self_loops',\n",
       " 'contiguous',\n",
       " 'debug',\n",
       " 'edge_attr',\n",
       " 'edge_index',\n",
       " 'filename',\n",
       " 'from_dict',\n",
       " 'is_coalesced',\n",
       " 'is_directed',\n",
       " 'is_undirected',\n",
       " 'keys',\n",
       " 'label',\n",
       " 'num_edge_features',\n",
       " 'num_edges',\n",
       " 'num_faces',\n",
       " 'num_features',\n",
       " 'num_node_features',\n",
       " 'num_nodes',\n",
       " 'pos',\n",
       " 'tfidf_vec',\n",
       " 'to',\n",
       " 'x',\n",
       " 'x_topo_feats',\n",
       " 'x_topo_times',\n",
       " 'y']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "#dataset.gnn_mode_off()\n",
    "dir(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0702, 0.0882, 0.0816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0520, 0.0622, 0.0000, 0.0000, 0.0000, 0.4552, 0.2024, 0.0755, 0.2288,\n",
       "        0.0000, 0.3481, 0.0000, 0.1347, 0.1402, 0.0568, 0.0579, 0.1128, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0598, 0.0691,\n",
       "        0.0000, 0.0000, 0.0000, 0.2008, 0.0000, 0.0000, 0.1506, 0.0705, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1487, 0.0797, 0.0628, 0.0560, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1593, 0.0000, 0.0000, 0.0000, 0.1522, 0.0665, 0.0000,\n",
       "        0.1885, 0.0778, 0.0814, 0.0559, 0.0767, 0.0000, 0.0658, 0.3634, 0.0472,\n",
       "        0.0000, 0.0000, 0.0000, 0.0479, 0.2121, 0.0000, 0.0000, 0.1715, 0.1752,\n",
       "        0.0000, 0.1245, 0.0772])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0702, 0.0882, 0.0816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0520, 0.0622, 0.0000, 0.0000, 0.0000, 0.4552, 0.2024, 0.0755, 0.2288,\n",
       "        0.0000, 0.3481, 0.0000, 0.1347, 0.1402, 0.0568, 0.0579, 0.1128, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0598, 0.0691,\n",
       "        0.0000, 0.0000, 0.0000, 0.2008, 0.0000, 0.0000, 0.1506, 0.0705, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.1487, 0.0797, 0.0628, 0.0560, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1593, 0.0000, 0.0000, 0.0000, 0.1522, 0.0665, 0.0000,\n",
       "        0.1885, 0.0778, 0.0814, 0.0559, 0.0767, 0.0000, 0.0658, 0.3634, 0.0472,\n",
       "        0.0000, 0.0000, 0.0000, 0.0479, 0.2121, 0.0000, 0.0000, 0.1715, 0.1752,\n",
       "        0.0000, 0.1245, 0.0772])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].tfidf_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "The precomputed tfidf_vec is now present on the the tfidf_vec and edge_attr attributes of each graph in the dataset. We can now proceed to modify the GNN models to take this into account and concat the GNN embedding with the tfidf embedding(from the edge_attr) to feed the last fc layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying GNN models for precomputed tfidf bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import GINConv, global_mean_pool, JumpingKnowledge\n",
    "from torch_geometric.nn import SAGEConv, GlobalAttention\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, JumpingKnowledge\n",
    "from torch_geometric.nn import SAGEConv, Set2Set\n",
    "from torch_geometric.nn import SAGEConv, global_sort_pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class GIN0(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, bow_dim=84,):\n",
    "        super(GIN0, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=False)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=False))\n",
    "        self.lin1 = Linear(hidden+bow_dim, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "        self.bow_dim=bow_dim\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, bow_embedding = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # combine gnn embedding and bow embedding\n",
    "        x = torch.cat((x,bow_embedding.view(-1,self.bow_dim)),1)\n",
    "        \n",
    "        # fc layers\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GIN0WithJK(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, mode='cat',bow_dim=84):\n",
    "        super(GIN0WithJK, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=False)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=False))\n",
    "        self.jump = JumpingKnowledge(mode)\n",
    "        if mode == 'cat':\n",
    "            self.lin1 = Linear(num_layers * hidden+bow_dim, hidden)\n",
    "        else:\n",
    "            self.lin1 = Linear(hidden+bow_dim, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "        self.bow_dim=bow_dim\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.jump.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, bow_embedding = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            xs += [x]\n",
    "        x = self.jump(xs)\n",
    "        x = global_mean_pool(x, batch)\n",
    "                         \n",
    "        # combine gnn embedding and bow embedding\n",
    "        #print(\"bow.shape\", bow_embedding.shape)\n",
    "        x = torch.cat((x,bow_embedding.view(-1,self.bow_dim)),1)\n",
    "        #print(\"after x\",x.shape)\n",
    "        # fc layers               \n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, bow_dim = 84):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=True)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=True))\n",
    "        self.lin1 = Linear(hidden+bow_dim, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "        self.bow_dim=bow_dim\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, bow_embedding = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # combine gnn embedding and bow embedding\n",
    "        #print(\"x.shape\", x.shape)\n",
    "        #print(\"bow_embedding.shape\",bow_embedding.shape)\n",
    "        # there are 84 dimensions in the bow embedding\n",
    "        x = torch.cat((x,bow_embedding.view(-1,self.bow_dim)),1)\n",
    "        \n",
    "        # fc layers\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GINWithJK(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden, num_classes, mode='cat',bow_dim=84):\n",
    "        super(GINWithJK, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=True)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=True))\n",
    "        self.jump = JumpingKnowledge(mode)\n",
    "        if mode == 'cat':\n",
    "            self.lin1 = Linear(num_layers * hidden+bow_dim, hidden)\n",
    "        else:\n",
    "            self.lin1 = Linear(hidden+bow_dim, hidden)\n",
    "        self.lin2 = Linear(hidden, num_classes)\n",
    "        \n",
    "        self.bow_dim=bow_dim\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.jump.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):         \n",
    "        x, edge_index, batch, bow_embedding = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            xs += [x]\n",
    "        x = self.jump(xs)\n",
    "        x = global_mean_pool(x, batch)\n",
    "                         \n",
    "        # combine gnn embedding and bow embedding\n",
    "        x = torch.cat((x,bow_embedding.view(-1,self.bow_dim)),1)\n",
    "        \n",
    "        # fc layers\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep HP-search on v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set/processed\n",
      " model: GIN  epochs: 5  num_layers: 4  hidden: 16\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "01/001: Val Loss: 2.0627, Val F1macro: 0.0928,Test Accuracy: 0.158, Test F1-macro: 0.097\n",
      "01/002: Val Loss: 2.0426, Val F1macro: 0.0766,Test Accuracy: 0.148, Test F1-macro: 0.077\n",
      "01/003: Val Loss: 2.0543, Val F1macro: 0.0777,Test Accuracy: 0.164, Test F1-macro: 0.081\n",
      "01/004: Val Loss: 2.0375, Val F1macro: 0.0772,Test Accuracy: 0.176, Test F1-macro: 0.085\n",
      "01/005: Val Loss: 2.0704, Val F1macro: 0.0552,Test Accuracy: 0.158, Test F1-macro: 0.069\n",
      "02/001: Val Loss: 2.0824, Val F1macro: 0.0687,Test Accuracy: 0.148, Test F1-macro: 0.077\n",
      "02/002: Val Loss: 2.0478, Val F1macro: 0.0884,Test Accuracy: 0.160, Test F1-macro: 0.078\n",
      "02/003: Val Loss: 2.0470, Val F1macro: 0.1011,Test Accuracy: 0.158, Test F1-macro: 0.094\n",
      "02/004: Val Loss: 2.0410, Val F1macro: 0.1012,Test Accuracy: 0.172, Test F1-macro: 0.109\n",
      "02/005: Val Loss: 2.0257, Val F1macro: 0.1172,Test Accuracy: 0.158, Test F1-macro: 0.105\n",
      "03/001: Val Loss: 2.0567, Val F1macro: 0.1031,Test Accuracy: 0.148, Test F1-macro: 0.095\n",
      "03/002: Val Loss: 2.0943, Val F1macro: 0.0854,Test Accuracy: 0.169, Test F1-macro: 0.086\n",
      "03/003: Val Loss: 2.0338, Val F1macro: 0.1014,Test Accuracy: 0.161, Test F1-macro: 0.104\n",
      "03/004: Val Loss: 2.0277, Val F1macro: 0.1030,Test Accuracy: 0.161, Test F1-macro: 0.098\n",
      "03/005: Val Loss: 2.0593, Val F1macro: 0.0950,Test Accuracy: 0.148, Test F1-macro: 0.102\n",
      "04/001: Val Loss: 2.0740, Val F1macro: 0.0922,Test Accuracy: 0.156, Test F1-macro: 0.094\n",
      "04/002: Val Loss: 2.0638, Val F1macro: 0.0903,Test Accuracy: 0.180, Test F1-macro: 0.102\n",
      "04/003: Val Loss: 2.0582, Val F1macro: 0.0670,Test Accuracy: 0.164, Test F1-macro: 0.091\n",
      "04/004: Val Loss: 2.0576, Val F1macro: 0.0963,Test Accuracy: 0.164, Test F1-macro: 0.101\n",
      "04/005: Val Loss: 2.0649, Val F1macro: 0.0933,Test Accuracy: 0.176, Test F1-macro: 0.112\n",
      "05/001: Val Loss: 2.0724, Val F1macro: 0.0559,Test Accuracy: 0.127, Test F1-macro: 0.061\n",
      "05/002: Val Loss: 2.0428, Val F1macro: 0.0970,Test Accuracy: 0.147, Test F1-macro: 0.067\n",
      "05/003: Val Loss: 2.0366, Val F1macro: 0.0915,Test Accuracy: 0.137, Test F1-macro: 0.075\n",
      "05/004: Val Loss: 2.0483, Val F1macro: 0.1177,Test Accuracy: 0.156, Test F1-macro: 0.087\n",
      "05/005: Val Loss: 2.0356, Val F1macro: 0.0919,Test Accuracy: 0.160, Test F1-macro: 0.072\n",
      "06/001: Val Loss: 2.0650, Val F1macro: 0.0827,Test Accuracy: 0.143, Test F1-macro: 0.074\n",
      "06/002: Val Loss: 2.0562, Val F1macro: 0.0738,Test Accuracy: 0.156, Test F1-macro: 0.083\n",
      "06/003: Val Loss: 2.0434, Val F1macro: 0.0801,Test Accuracy: 0.158, Test F1-macro: 0.085\n",
      "06/004: Val Loss: 2.0452, Val F1macro: 0.0863,Test Accuracy: 0.154, Test F1-macro: 0.081\n",
      "06/005: Val Loss: 2.0447, Val F1macro: 0.0751,Test Accuracy: 0.141, Test F1-macro: 0.077\n",
      "07/001: Val Loss: 2.0825, Val F1macro: 0.0319,Test Accuracy: 0.124, Test F1-macro: 0.030\n",
      "07/002: Val Loss: 2.0690, Val F1macro: 0.0862,Test Accuracy: 0.156, Test F1-macro: 0.100\n",
      "07/003: Val Loss: 2.0566, Val F1macro: 0.0828,Test Accuracy: 0.156, Test F1-macro: 0.086\n",
      "07/004: Val Loss: 2.0532, Val F1macro: 0.0757,Test Accuracy: 0.160, Test F1-macro: 0.075\n",
      "07/005: Val Loss: 2.0571, Val F1macro: 0.0694,Test Accuracy: 0.168, Test F1-macro: 0.062\n",
      "08/001: Val Loss: 2.0770, Val F1macro: 0.0318,Test Accuracy: 0.133, Test F1-macro: 0.034\n",
      "08/002: Val Loss: 2.0633, Val F1macro: 0.1039,Test Accuracy: 0.158, Test F1-macro: 0.087\n",
      "08/003: Val Loss: 2.0545, Val F1macro: 0.0667,Test Accuracy: 0.168, Test F1-macro: 0.079\n",
      "08/004: Val Loss: 2.0427, Val F1macro: 0.0860,Test Accuracy: 0.149, Test F1-macro: 0.085\n",
      "08/005: Val Loss: 2.0512, Val F1macro: 0.0809,Test Accuracy: 0.149, Test F1-macro: 0.076\n",
      "09/001: Val Loss: 2.0630, Val F1macro: 0.0853,Test Accuracy: 0.143, Test F1-macro: 0.087\n",
      "09/002: Val Loss: 2.0458, Val F1macro: 0.1048,Test Accuracy: 0.150, Test F1-macro: 0.095\n",
      "09/003: Val Loss: 2.0462, Val F1macro: 0.0972,Test Accuracy: 0.164, Test F1-macro: 0.096\n",
      "09/004: Val Loss: 2.0457, Val F1macro: 0.1004,Test Accuracy: 0.168, Test F1-macro: 0.098\n",
      "09/005: Val Loss: 2.0469, Val F1macro: 0.0786,Test Accuracy: 0.168, Test F1-macro: 0.087\n",
      "10/001: Val Loss: 2.5847, Val F1macro: 0.0409,Test Accuracy: 0.137, Test F1-macro: 0.038\n",
      "10/002: Val Loss: 2.0566, Val F1macro: 0.0747,Test Accuracy: 0.164, Test F1-macro: 0.088\n",
      "10/003: Val Loss: 2.0439, Val F1macro: 0.0838,Test Accuracy: 0.162, Test F1-macro: 0.106\n",
      "10/004: Val Loss: 2.0732, Val F1macro: 0.0670,Test Accuracy: 0.135, Test F1-macro: 0.062\n",
      "10/005: Val Loss: 2.0982, Val F1macro: 0.0905,Test Accuracy: 0.148, Test F1-macro: 0.076\n",
      "Val Loss: 2.0413, Val f1macro: 0.0912, Test Accuracy: 0.162 ± 0.007,Test F1-macro: 0.091 ± 0.012, Duration: 12.710\n",
      "Best result - 0.162 ± 0.007\n",
      "-----\n",
      "assemblerv1 - GIN: 0.162 ± 0.007\n",
      "Best result - 0.162 ± 0.007\n",
      "-----\n",
      "assemblerv1 - GIN: 0.162 ± 0.007\n",
      "assemblerv1 - GIN: 0.162 ± 0.007\n"
     ]
    }
   ],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.01\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 4,]\n",
    "hiddens = [16, ]  # the first fc layer will have hiddens+84.. \n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [5]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=10,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=0,\n",
    "                logger=logger)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, 4, 6]\n",
    "hiddens = [32, 64, 128, 256]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [50,100]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=10,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=5e-4,\n",
    "                logger=logger)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training  - concentrate on a algorithm and do a lot of epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=56\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2]\n",
    "hiddens = [ 64]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [500,1000]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=10,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=5e-4,\n",
    "                logger=logger)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training  - concentrate on a algorithm and do a lot of epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=56\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2]\n",
    "hiddens = [ 64]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [2000]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=10,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=5e-4,\n",
    "                logger=logger)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training  - concentrate on a algorithm and do a lot of epochs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=56\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2]\n",
    "hiddens = [ 64]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [2000]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=10,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=5e-4,\n",
    "                logger=logger)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use a weight-decay on v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.01\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2,4,8]\n",
    "hiddens = [16,32,64,128,256 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [32]:\n",
    "        for lr in [0.005]:\n",
    "            for batch_size in [128]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in product(layers, hiddens):\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "* layers_hidden : 2-64, 4-64\n",
    "* wd : 5e-3, 5e-4   <- pick one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.01\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2,4,8]\n",
    "hiddens = [ 64 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [32]:\n",
    "        for lr in [0.001]:\n",
    "            for batch_size in [128]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in [(8,128),(4,128),(4,256),(2,128),(2,64),(4,94)]:\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "* wd : 5e-4  \n",
    "* layers_hidden : 2-64, 4-256, 8-128 < 4-94  < 2-128 < 4-128\n",
    "* batch_size: ?\n",
    "* lr: ?\n",
    "* epochs: ?\n",
    "* model: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.01\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 4]\n",
    "hiddens = [ 128 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [32]:\n",
    "        for lr in [0.001,0.005,0.01]:\n",
    "            for batch_size in [128,65,32]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in [(4,128)]:\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "* wd : 5e-4  \n",
    "* layers_hidden : 2-64, 4-256, 8-128 < 4-94  < 2-128 < 4-128  <- pick top 3 or top 1 and 2 others\n",
    "* batch_size: 128\n",
    "* lr:  0.01 < 0.001 < 0.005\n",
    "* epochs: ?\n",
    "* model: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 4]\n",
    "hiddens = [ 128 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    ##GCN,    \n",
    "    GraphSAGE,    \n",
    "    GIN0,    \n",
    "    GIN,    \n",
    "    GlobalAttentionNet,    \n",
    "    Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [50]:\n",
    "        for lr in [0.005]:\n",
    "            for batch_size in [128]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in [(4,128)]:\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 4]\n",
    "hiddens = [ 128 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [50]:\n",
    "        for lr in [0.005]:\n",
    "            for batch_size in [128]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in [(4,128)]:\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "* wd : 5e-4  \n",
    "* layers_hidden : 2-64, 4-256, 8-128 < 4-94  < 2-128 < 4-128  <- pick top 3 or top 1 and 2 others\n",
    "* batch_size: 128\n",
    "* lr:  0.005\n",
    "* model: GIn < GIN0 < GINWJK\n",
    "* epochs: ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set/processed\n",
      "\n",
      "\n",
      " model: GINWithJK  epochs: 100  num_layers: 4  hidden: 128  batch_size: 128  lr: 0.005  wd: 0.0005\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "Val Loss: 2.0105, Val f1macro: 0.1516, Test Accuracy: 0.204 ± 0.016,Test F1-macro: 0.143 ± 0.014, Duration: 439.259\n",
      "\n",
      "\n",
      " model: GINWithJK  epochs: 250  num_layers: 4  hidden: 128  batch_size: 128  lr: 0.005  wd: 0.0005\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "Val Loss: 2.0116, Val f1macro: 0.1449, Test Accuracy: 0.201 ± 0.007,Test F1-macro: 0.138 ± 0.010, Duration: 1100.005\n",
      "\n",
      "\n",
      " model: GINWithJK  epochs: 500  num_layers: 4  hidden: 128  batch_size: 128  lr: 0.005  wd: 0.0005\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "Val Loss: 2.0107, Val f1macro: 0.1473, Test Accuracy: 0.195 ± 0.013,Test F1-macro: 0.131 ± 0.008, Duration: 2172.227\n",
      "\n",
      "\n",
      " model: GINWithJK  epochs: 750  num_layers: 4  hidden: 128  batch_size: 128  lr: 0.005  wd: 0.0005\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "Val Loss: 2.0110, Val f1macro: 0.1478, Test Accuracy: 0.204 ± 0.015,Test F1-macro: 0.145 ± 0.007, Duration: 3318.497\n",
      "\n",
      "\n",
      " model: GINWithJK  epochs: 1000  num_layers: 4  hidden: 128  batch_size: 128  lr: 0.005  wd: 0.0005\n",
      "num_features, data.x.shape\n",
      "torch.Size([3, 1])\n",
      "Val Loss: 2.0111, Val f1macro: 0.1466, Test Accuracy: 0.196 ± 0.002,Test F1-macro: 0.134 ± 0.007, Duration: 4473.312\n",
      "Best result - 0.204 ± 0.016\n",
      "-----\n",
      "assemblerv1 - GINWithJK: 0.204 ± 0.016\n",
      "Best result - 0.204 ± 0.016\n",
      "-----\n",
      "assemblerv1 - GINWithJK: 0.204 ± 0.016\n",
      "assemblerv1 - GINWithJK: 0.204 ± 0.016\n"
     ]
    }
   ],
   "source": [
    "#V1 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.005\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 4]\n",
    "hiddens = [ 128 ]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    #GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_1_precomp_split_undersample_max3/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=8\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [100,250,500,750,1000]:\n",
    "        for lr in [0.005]:\n",
    "            for batch_size in [128]:\n",
    "                for wd in [ 5e-4]:\n",
    "                    for num_layers, hidden in [(4,128)]:\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\n",
    "                              \" epochs:\",epochs,\n",
    "                              \" num_layers:\",num_layers,\n",
    "                              \" hidden:\",hidden,\n",
    "                              \" batch_size:\", batch_size,\n",
    "                              \" lr:\",lr,\n",
    "                              \" wd:\", wd\n",
    "                             )\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hp-search on v3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set/processed\n",
      " model: GIN  epochs: 50  num_layers: 2  hidden: 16\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6807, Val f1macro: 0.1108, Test Accuracy: 0.148 ± 0.005,Test F1-macro: 0.108 ± 0.018, Duration: 332.950\n",
      " model: GIN  epochs: 50  num_layers: 2  hidden: 32\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6613, Val f1macro: 0.1412, Test Accuracy: 0.159 ± 0.010,Test F1-macro: 0.145 ± 0.033, Duration: 364.269\n",
      " model: GIN  epochs: 50  num_layers: 2  hidden: 64\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6431, Val f1macro: 0.1779, Test Accuracy: 0.171 ± 0.009,Test F1-macro: 0.178 ± 0.019, Duration: 415.004\n",
      " model: GIN  epochs: 50  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6260, Val f1macro: 0.1882, Test Accuracy: 0.181 ± 0.004,Test F1-macro: 0.192 ± 0.011, Duration: 418.960\n",
      " model: GIN  epochs: 50  num_layers: 2  hidden: 256\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6458, Val f1macro: 0.1795, Test Accuracy: 0.169 ± 0.012,Test F1-macro: 0.182 ± 0.005, Duration: 418.504\n",
      " model: GIN  epochs: 50  num_layers: 4  hidden: 16\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6849, Val f1macro: 0.0969, Test Accuracy: 0.150 ± 0.008,Test F1-macro: 0.096 ± 0.009, Duration: 331.837\n",
      " model: GIN  epochs: 50  num_layers: 4  hidden: 32\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6611, Val f1macro: 0.1354, Test Accuracy: 0.159 ± 0.011,Test F1-macro: 0.138 ± 0.034, Duration: 340.242\n",
      " model: GIN  epochs: 50  num_layers: 4  hidden: 64\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6631, Val f1macro: 0.1517, Test Accuracy: 0.159 ± 0.015,Test F1-macro: 0.149 ± 0.035, Duration: 361.443\n",
      " model: GIN  epochs: 50  num_layers: 4  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6662, Val f1macro: 0.1560, Test Accuracy: 0.157 ± 0.009,Test F1-macro: 0.149 ± 0.024, Duration: 428.317\n",
      " model: GIN  epochs: 50  num_layers: 4  hidden: 256\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.7118, Val f1macro: 0.1571, Test Accuracy: 0.151 ± 0.010,Test F1-macro: 0.156 ± 0.039, Duration: 589.540\n",
      "Best result - 0.181 ± 0.004\n",
      "-----\n",
      "assemblerv1 - GIN: 0.181 ± 0.004\n",
      "Best result - 0.181 ± 0.004\n",
      "-----\n",
      "assemblerv1 - GIN: 0.181 ± 0.004\n",
      "assemblerv1 - GIN: 0.181 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "#V3 training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.001\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, 4]\n",
    "hiddens = [16, 32, 64, 128, 256]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    ##GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=19\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [50]:\n",
    "        for num_layers, hidden in product(layers, hiddens):\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes, bow_dim=86)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=3,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=0.0005,\n",
    "                logger=None)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set/processed\n",
      " model: GIN0WithJK  epochs: 35  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6380, Val f1macro: 0.1718, Test Accuracy: 0.165 ± 0.009,Test F1-macro: 0.175 ± 0.010, Duration: 260.591\n",
      " model: GIN0WithJK  epochs: 35  num_layers: 2  hidden: 256\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6457, Val f1macro: 0.1615, Test Accuracy: 0.167 ± 0.007,Test F1-macro: 0.167 ± 0.016, Duration: 306.526\n",
      "Best result - 0.165 ± 0.009\n",
      "-----\n",
      "assemblerv1 - GIN0WithJK: 0.165 ± 0.009\n",
      " model: GINWithJK  epochs: 35  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6422, Val f1macro: 0.1719, Test Accuracy: 0.172 ± 0.009,Test F1-macro: 0.175 ± 0.007, Duration: 265.397\n",
      " model: GINWithJK  epochs: 35  num_layers: 2  hidden: 256\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6443, Val f1macro: 0.1662, Test Accuracy: 0.162 ± 0.009,Test F1-macro: 0.167 ± 0.010, Duration: 310.011\n",
      "Best result - 0.165 ± 0.009\n",
      "-----\n",
      "assemblerv1 - GIN0WithJK: 0.165 ± 0.009\n",
      "assemblerv1 - GINWithJK: 0.165 ± 0.009\n",
      " model: GIN0  epochs: 35  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6662, Val f1macro: 0.1466, Test Accuracy: 0.151 ± 0.021,Test F1-macro: 0.148 ± 0.034, Duration: 259.824\n",
      " model: GIN0  epochs: 35  num_layers: 2  hidden: 256\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "('error getting: ', 3235)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 8: 'ometric.'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2296\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mchksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[0;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2308\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEmptyHeaderError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrealfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_graph_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# .zip is used for torch.jit.save and will throw an un-pickling error here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mis_zipfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_check_zipfile\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_EndRecData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m         \u001b[0;31m# file has correct magic number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_EndRecData\u001b[0;34m(fpin)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     if (len(data) == sizeEndCentDir and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-02cbaed252c8>\u001b[0m in \u001b[0;36mcross_validation_with_val_set\u001b[0;34m(dataset, model, folds, epochs, batch_size, lr, lr_decay_factor, lr_decay_step_size, weight_decay, logger)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mval_f1macros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_f1macro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mf1macros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_f1macro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-02cbaed252c8>\u001b[0m in \u001b[0;36meval_acc\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    133\u001b[0m         a :obj:`self.transform` is given).\"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m#print(idx, \"self.processed_file_names:\", self.processed_file_names[:10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error getting: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: ('error getting: ', 955)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 8: 'ometric.'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2296\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2297\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mchksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[0;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[1;32m   1481\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2308\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEmptyHeaderError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrealfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_graph_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# if not a tarfile, reset file offset and proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-db0a74ab3dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlr_decay_step_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay_step_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 logger=None)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-02cbaed252c8>\u001b[0m in \u001b[0;36mcross_validation_with_val_set\u001b[0;34m(dataset, model, folds, epochs, batch_size, lr, lr_decay_factor, lr_decay_step_size, weight_decay, logger)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;34m'test_f1-macro'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1macros\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0;34m'classif_report'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_classification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 }\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-02cbaed252c8>\u001b[0m in \u001b[0;36meval_classification_report\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    132\u001b[0m         r\"\"\"Gets the data object at index :obj:`idx` and transforms it (in case\n\u001b[1;32m    133\u001b[0m         a :obj:`self.transform` is given).\"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m#print(idx, \"self.processed_file_names:\", self.processed_file_names[:10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error getting: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mpointers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: ('error getting: ', 3235)"
     ]
    }
   ],
   "source": [
    "#V3 training\n",
    "# previous winner is 2layer 64 hidden units\n",
    "# now testing 2l 64h + GCN/GraphSAGE/GIN + 30,35,40 epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=128\n",
    "lr=0.001\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, ]\n",
    "hiddens = [128]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    #GCN,    \n",
    "    #GraphSAGE,    \n",
    "    GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=19\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for epochs in [35]:\n",
    "        for num_layers, hidden in [(2,128),(2,256)]:\n",
    "            print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "            # rewrite here & definition to pass num_classes and num_features manually\n",
    "            model = Net(dataset, num_layers, hidden, num_classes, bow_dim=86)\n",
    "\n",
    "            # verify how validation set is obtained\n",
    "            loss, acc, std = cross_validation_with_val_set(\n",
    "                dataset,\n",
    "                model,\n",
    "                folds=3,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                lr_decay_factor=lr_decay_factor,\n",
    "                lr_decay_step_size=lr_decay_step_size,\n",
    "                weight_decay=0.0005,\n",
    "                logger=None)\n",
    "\n",
    "            if loss < best_result[0]:\n",
    "                best_result = (loss, acc, std)\n",
    "\n",
    "        desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "        print('Best result - {}'.format(desc))\n",
    "        results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "        print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "342-86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#V3 training\n",
    "# previous winner is 2layer 64 hidden units\n",
    "# now testing 2l 64h + GCN/GraphSAGE/GIN + 30,35,40 epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=[128,64,32]\n",
    "lr=[0.001,0.005,0.01]\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, ]\n",
    "hiddens = [128]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    #GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=19\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for lr in [0.001,0.005]:\n",
    "        for batch_size in [128,64,32]:\n",
    "            for wd in [5e-4,]:\n",
    "                for epochs in [32]:\n",
    "                    for num_layers, hidden in product(layers, hiddens):\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes, bow_dim=86)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "                    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "                    print('Best result - {}'.format(desc))\n",
    "                    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "                    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "# best ones: 1, 2, 3, 5, 6, 10, 13\n",
    "#  wd 5e-5, 5e-4, 5e-3\n",
    "#  batch_size 128,64,32\n",
    "#  lr 0.001\n",
    "\n",
    "#10: wd 5-4, batch 32, lr 0.001\n",
    "#13: wd 5e-5, batch 128, lr 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set/processed\n",
      " model: GINWithJK  epochs: 30  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6680, Val f1macro: 0.1492, Test Accuracy: 0.157 ± 0.002,Test F1-macro: 0.150 ± 0.023, Duration: 253.006\n",
      "Best result - 0.157 ± 0.002\n",
      "-----\n",
      "assemblerv1 - GINWithJK: 0.157 ± 0.002\n",
      " model: GINWithJK  epochs: 35  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "Val Loss: 2.6609, Val f1macro: 0.1580, Test Accuracy: 0.156 ± 0.009,Test F1-macro: 0.157 ± 0.017, Duration: 288.700\n",
      "Best result - 0.156 ± 0.009\n",
      "-----\n",
      "assemblerv1 - GINWithJK: 0.157 ± 0.002\n",
      "assemblerv1 - GINWithJK: 0.156 ± 0.009\n",
      " model: GINWithJK  epochs: 40  num_layers: 2  hidden: 128\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "#V3 training\n",
    "# previous winner is 2layer 64 hidden units\n",
    "# now testing 2l 64h + GCN/GraphSAGE/GIN + 30,35,40 epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "#batch_size=[128,64,32]\n",
    "#lr=[0.001,0.005,0.01]\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, ]\n",
    "hiddens = [128]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    GINWithJK,    \n",
    "    #GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    #GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=19\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for lr in [0.001]:\n",
    "        for batch_size in [32]:\n",
    "            for wd in [ 5e-4]:\n",
    "                for epochs in [30, 35, 40, 45, 50, 55, 60, 70, 80, 100, 200]:\n",
    "                    for num_layers, hidden in product(layers, hiddens):\n",
    "                        print(\" model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes, bow_dim=86)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=None)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "                    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "                    print('Best result - {}'.format(desc))\n",
    "                    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "                    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set/processed\n",
      "\n",
      "\n",
      " model: GIN  epochs: 500  num_layers: 2  hidden: 64\n",
      "num_features, data.x.shape\n",
      "torch.Size([7, 4])\n",
      "01/001: Val Loss: 2.7963, Val F1macro: 0.0693,Test Accuracy: 0.125, Test F1-macro: 0.069\n",
      "01/002: Val Loss: 2.7593, Val F1macro: 0.0644,Test Accuracy: 0.103, Test F1-macro: 0.060\n",
      "01/003: Val Loss: 2.8024, Val F1macro: 0.0893,Test Accuracy: 0.113, Test F1-macro: 0.100\n",
      "01/004: Val Loss: 3.1248, Val F1macro: 0.0333,Test Accuracy: 0.084, Test F1-macro: 0.036\n",
      "01/005: Val Loss: 2.7358, Val F1macro: 0.1189,Test Accuracy: 0.128, Test F1-macro: 0.124\n",
      "01/006: Val Loss: 2.8043, Val F1macro: 0.0667,Test Accuracy: 0.098, Test F1-macro: 0.058\n",
      "01/007: Val Loss: 2.7249, Val F1macro: 0.0633,Test Accuracy: 0.123, Test F1-macro: 0.057\n",
      "01/008: Val Loss: 2.8596, Val F1macro: 0.0630,Test Accuracy: 0.112, Test F1-macro: 0.054\n",
      "01/009: Val Loss: 2.9712, Val F1macro: 0.0462,Test Accuracy: 0.096, Test F1-macro: 0.041\n",
      "01/010: Val Loss: 2.7664, Val F1macro: 0.0711,Test Accuracy: 0.107, Test F1-macro: 0.067\n",
      "01/011: Val Loss: 2.8561, Val F1macro: 0.0507,Test Accuracy: 0.096, Test F1-macro: 0.047\n",
      "01/012: Val Loss: 2.8468, Val F1macro: 0.0722,Test Accuracy: 0.101, Test F1-macro: 0.067\n",
      "01/013: Val Loss: 4.2304, Val F1macro: 0.1034,Test Accuracy: 0.136, Test F1-macro: 0.099\n",
      "01/014: Val Loss: 2.7146, Val F1macro: 0.0852,Test Accuracy: 0.129, Test F1-macro: 0.078\n",
      "01/015: Val Loss: 2.7119, Val F1macro: 0.0774,Test Accuracy: 0.120, Test F1-macro: 0.070\n",
      "01/016: Val Loss: 2.7164, Val F1macro: 0.0721,Test Accuracy: 0.126, Test F1-macro: 0.070\n",
      "01/017: Val Loss: 2.7315, Val F1macro: 0.0599,Test Accuracy: 0.118, Test F1-macro: 0.061\n",
      "01/018: Val Loss: 2.7776, Val F1macro: 0.0683,Test Accuracy: 0.125, Test F1-macro: 0.067\n",
      "01/019: Val Loss: 2.7372, Val F1macro: 0.0583,Test Accuracy: 0.118, Test F1-macro: 0.061\n",
      "01/020: Val Loss: 2.6930, Val F1macro: 0.0740,Test Accuracy: 0.134, Test F1-macro: 0.069\n",
      "01/021: Val Loss: 2.7048, Val F1macro: 0.0746,Test Accuracy: 0.128, Test F1-macro: 0.072\n",
      "01/022: Val Loss: 2.7179, Val F1macro: 0.0925,Test Accuracy: 0.134, Test F1-macro: 0.091\n",
      "01/023: Val Loss: 2.6995, Val F1macro: 0.0507,Test Accuracy: 0.120, Test F1-macro: 0.048\n",
      "01/024: Val Loss: 2.7035, Val F1macro: 0.0783,Test Accuracy: 0.135, Test F1-macro: 0.071\n",
      "01/025: Val Loss: 2.7938, Val F1macro: 0.0681,Test Accuracy: 0.107, Test F1-macro: 0.066\n",
      "01/026: Val Loss: 2.7279, Val F1macro: 0.1061,Test Accuracy: 0.129, Test F1-macro: 0.092\n",
      "01/027: Val Loss: 2.6905, Val F1macro: 0.0956,Test Accuracy: 0.144, Test F1-macro: 0.089\n",
      "01/028: Val Loss: 2.6960, Val F1macro: 0.0781,Test Accuracy: 0.138, Test F1-macro: 0.075\n",
      "01/029: Val Loss: 2.7024, Val F1macro: 0.0962,Test Accuracy: 0.133, Test F1-macro: 0.089\n",
      "01/030: Val Loss: 2.8082, Val F1macro: 0.0711,Test Accuracy: 0.101, Test F1-macro: 0.067\n",
      "01/031: Val Loss: 2.8239, Val F1macro: 0.0547,Test Accuracy: 0.104, Test F1-macro: 0.054\n",
      "01/032: Val Loss: 2.7374, Val F1macro: 0.1049,Test Accuracy: 0.133, Test F1-macro: 0.096\n",
      "01/033: Val Loss: 2.6840, Val F1macro: 0.0969,Test Accuracy: 0.134, Test F1-macro: 0.085\n",
      "01/034: Val Loss: 2.7045, Val F1macro: 0.1212,Test Accuracy: 0.138, Test F1-macro: 0.113\n",
      "01/035: Val Loss: 2.6842, Val F1macro: 0.1107,Test Accuracy: 0.147, Test F1-macro: 0.101\n",
      "01/036: Val Loss: 2.6919, Val F1macro: 0.0857,Test Accuracy: 0.139, Test F1-macro: 0.078\n",
      "01/037: Val Loss: 2.6923, Val F1macro: 0.1344,Test Accuracy: 0.150, Test F1-macro: 0.124\n",
      "01/038: Val Loss: 2.6880, Val F1macro: 0.0966,Test Accuracy: 0.141, Test F1-macro: 0.088\n",
      "01/039: Val Loss: 2.7132, Val F1macro: 0.1110,Test Accuracy: 0.129, Test F1-macro: 0.101\n",
      "01/040: Val Loss: 2.7196, Val F1macro: 0.1081,Test Accuracy: 0.137, Test F1-macro: 0.105\n",
      "01/041: Val Loss: 2.6785, Val F1macro: 0.1100,Test Accuracy: 0.144, Test F1-macro: 0.099\n",
      "01/042: Val Loss: 2.7344, Val F1macro: 0.0992,Test Accuracy: 0.147, Test F1-macro: 0.096\n",
      "01/043: Val Loss: 2.6969, Val F1macro: 0.1138,Test Accuracy: 0.148, Test F1-macro: 0.101\n",
      "01/044: Val Loss: 2.6981, Val F1macro: 0.1085,Test Accuracy: 0.147, Test F1-macro: 0.102\n",
      "01/045: Val Loss: 2.6707, Val F1macro: 0.1113,Test Accuracy: 0.149, Test F1-macro: 0.105\n",
      "01/046: Val Loss: 2.6724, Val F1macro: 0.1109,Test Accuracy: 0.146, Test F1-macro: 0.092\n",
      "01/047: Val Loss: 2.6936, Val F1macro: 0.1065,Test Accuracy: 0.134, Test F1-macro: 0.096\n",
      "01/048: Val Loss: 2.7483, Val F1macro: 0.0926,Test Accuracy: 0.124, Test F1-macro: 0.094\n",
      "01/049: Val Loss: 6.2749, Val F1macro: 0.1186,Test Accuracy: 0.142, Test F1-macro: 0.112\n",
      "01/050: Val Loss: 2.7983, Val F1macro: 0.0818,Test Accuracy: 0.122, Test F1-macro: 0.079\n",
      "01/051: Val Loss: 2.9913, Val F1macro: 0.0717,Test Accuracy: 0.094, Test F1-macro: 0.062\n",
      "01/052: Val Loss: 2.8638, Val F1macro: 0.0770,Test Accuracy: 0.099, Test F1-macro: 0.068\n",
      "01/053: Val Loss: 3.1306, Val F1macro: 0.0739,Test Accuracy: 0.116, Test F1-macro: 0.069\n",
      "01/054: Val Loss: 2.7797, Val F1macro: 0.0710,Test Accuracy: 0.106, Test F1-macro: 0.070\n",
      "01/055: Val Loss: 2.7910, Val F1macro: 0.0798,Test Accuracy: 0.109, Test F1-macro: 0.077\n",
      "01/056: Val Loss: 2.6929, Val F1macro: 0.1183,Test Accuracy: 0.137, Test F1-macro: 0.105\n",
      "01/057: Val Loss: 2.7372, Val F1macro: 0.1071,Test Accuracy: 0.125, Test F1-macro: 0.095\n",
      "01/058: Val Loss: 2.6827, Val F1macro: 0.1241,Test Accuracy: 0.153, Test F1-macro: 0.114\n",
      "01/059: Val Loss: 2.7320, Val F1macro: 0.0926,Test Accuracy: 0.120, Test F1-macro: 0.089\n",
      "01/060: Val Loss: 2.6747, Val F1macro: 0.1196,Test Accuracy: 0.151, Test F1-macro: 0.107\n",
      "01/061: Val Loss: 2.6972, Val F1macro: 0.1258,Test Accuracy: 0.138, Test F1-macro: 0.116\n",
      "01/062: Val Loss: 2.7111, Val F1macro: 0.1441,Test Accuracy: 0.150, Test F1-macro: 0.133\n",
      "01/063: Val Loss: 2.6925, Val F1macro: 0.1411,Test Accuracy: 0.154, Test F1-macro: 0.135\n",
      "01/064: Val Loss: 2.6803, Val F1macro: 0.1148,Test Accuracy: 0.148, Test F1-macro: 0.103\n",
      "01/065: Val Loss: 2.7567, Val F1macro: 0.1288,Test Accuracy: 0.130, Test F1-macro: 0.118\n",
      "01/066: Val Loss: 2.8661, Val F1macro: 0.0912,Test Accuracy: 0.107, Test F1-macro: 0.086\n",
      "01/067: Val Loss: 2.6791, Val F1macro: 0.1125,Test Accuracy: 0.141, Test F1-macro: 0.103\n",
      "01/068: Val Loss: 2.6674, Val F1macro: 0.1182,Test Accuracy: 0.154, Test F1-macro: 0.113\n",
      "01/069: Val Loss: 2.6705, Val F1macro: 0.1280,Test Accuracy: 0.152, Test F1-macro: 0.119\n",
      "01/070: Val Loss: 2.6810, Val F1macro: 0.1217,Test Accuracy: 0.145, Test F1-macro: 0.106\n",
      "01/071: Val Loss: 2.6891, Val F1macro: 0.1011,Test Accuracy: 0.135, Test F1-macro: 0.101\n",
      "01/072: Val Loss: 2.6727, Val F1macro: 0.1207,Test Accuracy: 0.148, Test F1-macro: 0.106\n",
      "01/073: Val Loss: 2.7313, Val F1macro: 0.1143,Test Accuracy: 0.130, Test F1-macro: 0.112\n",
      "01/074: Val Loss: 2.6644, Val F1macro: 0.1681,Test Accuracy: 0.163, Test F1-macro: 0.167\n",
      "01/075: Val Loss: 2.6563, Val F1macro: 0.1671,Test Accuracy: 0.165, Test F1-macro: 0.164\n",
      "01/076: Val Loss: 2.6576, Val F1macro: 0.1657,Test Accuracy: 0.166, Test F1-macro: 0.161\n",
      "01/077: Val Loss: 2.6625, Val F1macro: 0.1831,Test Accuracy: 0.174, Test F1-macro: 0.190\n",
      "01/078: Val Loss: 2.6588, Val F1macro: 0.1727,Test Accuracy: 0.163, Test F1-macro: 0.162\n",
      "01/079: Val Loss: 2.6526, Val F1macro: 0.1830,Test Accuracy: 0.167, Test F1-macro: 0.183\n",
      "01/080: Val Loss: 2.6856, Val F1macro: 0.0889,Test Accuracy: 0.135, Test F1-macro: 0.097\n",
      "01/081: Val Loss: 2.6545, Val F1macro: 0.1728,Test Accuracy: 0.170, Test F1-macro: 0.176\n",
      "01/082: Val Loss: 2.6904, Val F1macro: 0.1741,Test Accuracy: 0.161, Test F1-macro: 0.178\n",
      "01/083: Val Loss: 2.6546, Val F1macro: 0.1816,Test Accuracy: 0.170, Test F1-macro: 0.186\n",
      "01/084: Val Loss: 2.6529, Val F1macro: 0.1828,Test Accuracy: 0.167, Test F1-macro: 0.178\n",
      "01/085: Val Loss: 2.6451, Val F1macro: 0.1857,Test Accuracy: 0.171, Test F1-macro: 0.186\n",
      "01/086: Val Loss: 2.6769, Val F1macro: 0.1736,Test Accuracy: 0.151, Test F1-macro: 0.157\n",
      "01/087: Val Loss: 2.6525, Val F1macro: 0.1871,Test Accuracy: 0.167, Test F1-macro: 0.183\n",
      "01/088: Val Loss: 2.6413, Val F1macro: 0.1661,Test Accuracy: 0.162, Test F1-macro: 0.157\n",
      "01/089: Val Loss: 2.6431, Val F1macro: 0.1803,Test Accuracy: 0.174, Test F1-macro: 0.189\n",
      "01/090: Val Loss: 2.6524, Val F1macro: 0.1864,Test Accuracy: 0.170, Test F1-macro: 0.185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/091: Val Loss: 2.6816, Val F1macro: 0.1838,Test Accuracy: 0.174, Test F1-macro: 0.190\n",
      "01/092: Val Loss: 2.6531, Val F1macro: 0.1843,Test Accuracy: 0.162, Test F1-macro: 0.176\n",
      "01/093: Val Loss: 2.6461, Val F1macro: 0.1840,Test Accuracy: 0.166, Test F1-macro: 0.180\n",
      "01/094: Val Loss: 2.6427, Val F1macro: 0.1578,Test Accuracy: 0.160, Test F1-macro: 0.162\n",
      "01/095: Val Loss: 2.6415, Val F1macro: 0.1796,Test Accuracy: 0.177, Test F1-macro: 0.192\n",
      "01/096: Val Loss: 2.6576, Val F1macro: 0.1877,Test Accuracy: 0.171, Test F1-macro: 0.191\n",
      "01/097: Val Loss: 2.6531, Val F1macro: 0.1609,Test Accuracy: 0.163, Test F1-macro: 0.157\n",
      "01/098: Val Loss: 2.6524, Val F1macro: 0.1816,Test Accuracy: 0.172, Test F1-macro: 0.188\n",
      "01/099: Val Loss: 2.6578, Val F1macro: 0.1770,Test Accuracy: 0.169, Test F1-macro: 0.172\n",
      "01/100: Val Loss: 2.6404, Val F1macro: 0.1793,Test Accuracy: 0.173, Test F1-macro: 0.186\n",
      "01/101: Val Loss: 2.6439, Val F1macro: 0.1808,Test Accuracy: 0.175, Test F1-macro: 0.188\n",
      "01/102: Val Loss: 2.6390, Val F1macro: 0.1877,Test Accuracy: 0.176, Test F1-macro: 0.195\n",
      "01/103: Val Loss: 2.6342, Val F1macro: 0.1864,Test Accuracy: 0.179, Test F1-macro: 0.196\n",
      "01/104: Val Loss: 2.6375, Val F1macro: 0.1818,Test Accuracy: 0.172, Test F1-macro: 0.184\n",
      "01/105: Val Loss: 2.6353, Val F1macro: 0.1871,Test Accuracy: 0.180, Test F1-macro: 0.195\n",
      "01/106: Val Loss: 2.6790, Val F1macro: 0.1771,Test Accuracy: 0.154, Test F1-macro: 0.172\n",
      "01/107: Val Loss: 2.6317, Val F1macro: 0.1746,Test Accuracy: 0.173, Test F1-macro: 0.179\n",
      "01/108: Val Loss: 2.6336, Val F1macro: 0.1792,Test Accuracy: 0.174, Test F1-macro: 0.182\n",
      "01/109: Val Loss: 2.6383, Val F1macro: 0.1867,Test Accuracy: 0.176, Test F1-macro: 0.195\n",
      "01/110: Val Loss: 2.6720, Val F1macro: 0.1799,Test Accuracy: 0.174, Test F1-macro: 0.183\n",
      "01/111: Val Loss: 2.6370, Val F1macro: 0.1771,Test Accuracy: 0.169, Test F1-macro: 0.174\n",
      "01/112: Val Loss: 2.6399, Val F1macro: 0.1873,Test Accuracy: 0.173, Test F1-macro: 0.190\n",
      "01/113: Val Loss: 2.6306, Val F1macro: 0.1861,Test Accuracy: 0.177, Test F1-macro: 0.193\n",
      "01/114: Val Loss: 2.6463, Val F1macro: 0.1887,Test Accuracy: 0.176, Test F1-macro: 0.198\n",
      "01/115: Val Loss: 2.6412, Val F1macro: 0.1892,Test Accuracy: 0.176, Test F1-macro: 0.196\n",
      "01/116: Val Loss: 2.6350, Val F1macro: 0.1775,Test Accuracy: 0.170, Test F1-macro: 0.179\n",
      "01/117: Val Loss: 2.6502, Val F1macro: 0.1797,Test Accuracy: 0.171, Test F1-macro: 0.182\n",
      "01/118: Val Loss: 2.6559, Val F1macro: 0.1877,Test Accuracy: 0.169, Test F1-macro: 0.191\n",
      "01/119: Val Loss: 2.6347, Val F1macro: 0.1861,Test Accuracy: 0.173, Test F1-macro: 0.194\n",
      "01/120: Val Loss: 2.6419, Val F1macro: 0.1900,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/121: Val Loss: 2.6455, Val F1macro: 0.1871,Test Accuracy: 0.179, Test F1-macro: 0.196\n",
      "01/122: Val Loss: 2.6432, Val F1macro: 0.1849,Test Accuracy: 0.177, Test F1-macro: 0.195\n",
      "01/123: Val Loss: 2.6425, Val F1macro: 0.1878,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/124: Val Loss: 2.6385, Val F1macro: 0.1871,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/125: Val Loss: 2.6694, Val F1macro: 0.1829,Test Accuracy: 0.173, Test F1-macro: 0.187\n",
      "01/126: Val Loss: 2.6346, Val F1macro: 0.1838,Test Accuracy: 0.178, Test F1-macro: 0.193\n",
      "01/127: Val Loss: 2.6420, Val F1macro: 0.1897,Test Accuracy: 0.178, Test F1-macro: 0.195\n",
      "01/128: Val Loss: 2.6607, Val F1macro: 0.1887,Test Accuracy: 0.176, Test F1-macro: 0.196\n",
      "01/129: Val Loss: 2.6491, Val F1macro: 0.1895,Test Accuracy: 0.180, Test F1-macro: 0.196\n",
      "01/130: Val Loss: 2.6313, Val F1macro: 0.1827,Test Accuracy: 0.178, Test F1-macro: 0.191\n",
      "01/131: Val Loss: 2.6329, Val F1macro: 0.1828,Test Accuracy: 0.175, Test F1-macro: 0.192\n",
      "01/132: Val Loss: 2.6310, Val F1macro: 0.1821,Test Accuracy: 0.177, Test F1-macro: 0.189\n",
      "01/133: Val Loss: 2.6329, Val F1macro: 0.1884,Test Accuracy: 0.178, Test F1-macro: 0.196\n",
      "01/134: Val Loss: 2.6326, Val F1macro: 0.1877,Test Accuracy: 0.179, Test F1-macro: 0.196\n",
      "01/135: Val Loss: 2.7296, Val F1macro: 0.1782,Test Accuracy: 0.152, Test F1-macro: 0.178\n",
      "01/136: Val Loss: 2.6608, Val F1macro: 0.1818,Test Accuracy: 0.164, Test F1-macro: 0.169\n",
      "01/137: Val Loss: 2.6336, Val F1macro: 0.1907,Test Accuracy: 0.180, Test F1-macro: 0.194\n",
      "01/138: Val Loss: 2.6384, Val F1macro: 0.1869,Test Accuracy: 0.175, Test F1-macro: 0.195\n",
      "01/139: Val Loss: 2.6302, Val F1macro: 0.1873,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/140: Val Loss: 2.6426, Val F1macro: 0.1877,Test Accuracy: 0.168, Test F1-macro: 0.185\n",
      "01/141: Val Loss: 2.6374, Val F1macro: 0.1916,Test Accuracy: 0.180, Test F1-macro: 0.196\n",
      "01/142: Val Loss: 2.6493, Val F1macro: 0.1792,Test Accuracy: 0.174, Test F1-macro: 0.182\n",
      "01/143: Val Loss: 2.6312, Val F1macro: 0.1887,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/144: Val Loss: 2.6533, Val F1macro: 0.1915,Test Accuracy: 0.169, Test F1-macro: 0.187\n",
      "01/145: Val Loss: 2.6314, Val F1macro: 0.1894,Test Accuracy: 0.174, Test F1-macro: 0.192\n",
      "01/146: Val Loss: 2.7025, Val F1macro: 0.1746,Test Accuracy: 0.155, Test F1-macro: 0.177\n",
      "01/147: Val Loss: 2.6790, Val F1macro: 0.1783,Test Accuracy: 0.158, Test F1-macro: 0.179\n",
      "01/148: Val Loss: 2.6303, Val F1macro: 0.1882,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/149: Val Loss: 2.6432, Val F1macro: 0.1888,Test Accuracy: 0.175, Test F1-macro: 0.200\n",
      "01/150: Val Loss: 2.6419, Val F1macro: 0.1776,Test Accuracy: 0.175, Test F1-macro: 0.186\n",
      "01/151: Val Loss: 2.6467, Val F1macro: 0.1898,Test Accuracy: 0.179, Test F1-macro: 0.200\n",
      "01/152: Val Loss: 2.6331, Val F1macro: 0.1871,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/153: Val Loss: 2.6442, Val F1macro: 0.1878,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/154: Val Loss: 2.6418, Val F1macro: 0.1883,Test Accuracy: 0.176, Test F1-macro: 0.199\n",
      "01/155: Val Loss: 2.6445, Val F1macro: 0.1889,Test Accuracy: 0.175, Test F1-macro: 0.199\n",
      "01/156: Val Loss: 2.6343, Val F1macro: 0.1865,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/157: Val Loss: 2.6393, Val F1macro: 0.1859,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/158: Val Loss: 2.6512, Val F1macro: 0.1859,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/159: Val Loss: 2.6534, Val F1macro: 0.1922,Test Accuracy: 0.173, Test F1-macro: 0.195\n",
      "01/160: Val Loss: 2.6339, Val F1macro: 0.1903,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/161: Val Loss: 2.6408, Val F1macro: 0.1877,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/162: Val Loss: 2.6502, Val F1macro: 0.1855,Test Accuracy: 0.180, Test F1-macro: 0.200\n",
      "01/163: Val Loss: 2.6386, Val F1macro: 0.1876,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/164: Val Loss: 2.6404, Val F1macro: 0.1919,Test Accuracy: 0.179, Test F1-macro: 0.201\n",
      "01/165: Val Loss: 2.6421, Val F1macro: 0.1920,Test Accuracy: 0.177, Test F1-macro: 0.201\n",
      "01/166: Val Loss: 2.6717, Val F1macro: 0.1937,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/167: Val Loss: 2.6383, Val F1macro: 0.1896,Test Accuracy: 0.181, Test F1-macro: 0.202\n",
      "01/168: Val Loss: 2.6347, Val F1macro: 0.1879,Test Accuracy: 0.175, Test F1-macro: 0.190\n",
      "01/169: Val Loss: 2.6320, Val F1macro: 0.1903,Test Accuracy: 0.176, Test F1-macro: 0.197\n",
      "01/170: Val Loss: 2.6310, Val F1macro: 0.1903,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/171: Val Loss: 2.6338, Val F1macro: 0.1891,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/172: Val Loss: 2.6512, Val F1macro: 0.1887,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/173: Val Loss: 2.6583, Val F1macro: 0.1978,Test Accuracy: 0.168, Test F1-macro: 0.195\n",
      "01/174: Val Loss: 2.6325, Val F1macro: 0.1884,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/175: Val Loss: 2.6389, Val F1macro: 0.1893,Test Accuracy: 0.178, Test F1-macro: 0.200\n",
      "01/176: Val Loss: 2.6325, Val F1macro: 0.1887,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/177: Val Loss: 2.6413, Val F1macro: 0.1876,Test Accuracy: 0.177, Test F1-macro: 0.200\n",
      "01/178: Val Loss: 2.6408, Val F1macro: 0.1939,Test Accuracy: 0.180, Test F1-macro: 0.200\n",
      "01/179: Val Loss: 2.6352, Val F1macro: 0.1856,Test Accuracy: 0.178, Test F1-macro: 0.196\n",
      "01/180: Val Loss: 2.6312, Val F1macro: 0.1889,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/181: Val Loss: 2.6509, Val F1macro: 0.1903,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/182: Val Loss: 2.6529, Val F1macro: 0.1916,Test Accuracy: 0.179, Test F1-macro: 0.201\n",
      "01/183: Val Loss: 2.6458, Val F1macro: 0.1886,Test Accuracy: 0.178, Test F1-macro: 0.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/184: Val Loss: 2.7344, Val F1macro: 0.1898,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/185: Val Loss: 2.6361, Val F1macro: 0.1891,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/186: Val Loss: 2.6442, Val F1macro: 0.1893,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/187: Val Loss: 2.6360, Val F1macro: 0.1769,Test Accuracy: 0.171, Test F1-macro: 0.178\n",
      "01/188: Val Loss: 2.6306, Val F1macro: 0.1920,Test Accuracy: 0.176, Test F1-macro: 0.197\n",
      "01/189: Val Loss: 2.6510, Val F1macro: 0.1844,Test Accuracy: 0.175, Test F1-macro: 0.192\n",
      "01/190: Val Loss: 2.6336, Val F1macro: 0.1803,Test Accuracy: 0.176, Test F1-macro: 0.187\n",
      "01/191: Val Loss: 2.6341, Val F1macro: 0.1899,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/192: Val Loss: 2.6300, Val F1macro: 0.1906,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/193: Val Loss: 2.6712, Val F1macro: 0.1881,Test Accuracy: 0.179, Test F1-macro: 0.196\n",
      "01/194: Val Loss: 2.6342, Val F1macro: 0.1888,Test Accuracy: 0.178, Test F1-macro: 0.196\n",
      "01/195: Val Loss: 2.6313, Val F1macro: 0.1876,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/196: Val Loss: 2.6533, Val F1macro: 0.1905,Test Accuracy: 0.177, Test F1-macro: 0.198\n",
      "01/197: Val Loss: 2.6496, Val F1macro: 0.1877,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/198: Val Loss: 2.6302, Val F1macro: 0.1888,Test Accuracy: 0.181, Test F1-macro: 0.202\n",
      "01/199: Val Loss: 2.6443, Val F1macro: 0.2007,Test Accuracy: 0.178, Test F1-macro: 0.202\n",
      "01/200: Val Loss: 2.6494, Val F1macro: 0.1900,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/201: Val Loss: 2.6376, Val F1macro: 0.1889,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/202: Val Loss: 2.6709, Val F1macro: 0.1889,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/203: Val Loss: 2.6230, Val F1macro: 0.1891,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/204: Val Loss: 2.6294, Val F1macro: 0.1877,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/205: Val Loss: 2.6266, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/206: Val Loss: 2.6532, Val F1macro: 0.1904,Test Accuracy: 0.179, Test F1-macro: 0.202\n",
      "01/207: Val Loss: 2.6236, Val F1macro: 0.1876,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/208: Val Loss: 2.6321, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/209: Val Loss: 2.6280, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/210: Val Loss: 2.6403, Val F1macro: 0.1873,Test Accuracy: 0.181, Test F1-macro: 0.200\n",
      "01/211: Val Loss: 2.6305, Val F1macro: 0.1903,Test Accuracy: 0.179, Test F1-macro: 0.200\n",
      "01/212: Val Loss: 2.6267, Val F1macro: 0.1887,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/213: Val Loss: 2.6337, Val F1macro: 0.1895,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/214: Val Loss: 2.6360, Val F1macro: 0.1888,Test Accuracy: 0.181, Test F1-macro: 0.198\n",
      "01/215: Val Loss: 2.6291, Val F1macro: 0.1895,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/216: Val Loss: 2.6356, Val F1macro: 0.1893,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/217: Val Loss: 2.6331, Val F1macro: 0.1903,Test Accuracy: 0.182, Test F1-macro: 0.203\n",
      "01/218: Val Loss: 2.6478, Val F1macro: 0.1902,Test Accuracy: 0.178, Test F1-macro: 0.201\n",
      "01/219: Val Loss: 2.7137, Val F1macro: 0.1341,Test Accuracy: 0.149, Test F1-macro: 0.134\n",
      "01/220: Val Loss: 2.6340, Val F1macro: 0.1895,Test Accuracy: 0.177, Test F1-macro: 0.198\n",
      "01/221: Val Loss: 2.6299, Val F1macro: 0.1869,Test Accuracy: 0.179, Test F1-macro: 0.201\n",
      "01/222: Val Loss: 2.6419, Val F1macro: 0.1896,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/223: Val Loss: 2.6283, Val F1macro: 0.1889,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/224: Val Loss: 2.6415, Val F1macro: 0.1965,Test Accuracy: 0.173, Test F1-macro: 0.196\n",
      "01/225: Val Loss: 2.6230, Val F1macro: 0.1894,Test Accuracy: 0.181, Test F1-macro: 0.202\n",
      "01/226: Val Loss: 2.6363, Val F1macro: 0.1860,Test Accuracy: 0.172, Test F1-macro: 0.185\n",
      "01/227: Val Loss: 2.6388, Val F1macro: 0.1847,Test Accuracy: 0.175, Test F1-macro: 0.190\n",
      "01/228: Val Loss: 2.6546, Val F1macro: 0.1899,Test Accuracy: 0.177, Test F1-macro: 0.197\n",
      "01/229: Val Loss: 2.6412, Val F1macro: 0.1890,Test Accuracy: 0.182, Test F1-macro: 0.202\n",
      "01/230: Val Loss: 2.6716, Val F1macro: 0.1919,Test Accuracy: 0.179, Test F1-macro: 0.195\n",
      "01/231: Val Loss: 2.6346, Val F1macro: 0.1851,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/232: Val Loss: 2.6272, Val F1macro: 0.1903,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/233: Val Loss: 2.6308, Val F1macro: 0.1908,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/234: Val Loss: 2.6309, Val F1macro: 0.1915,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/235: Val Loss: 2.6550, Val F1macro: 0.1891,Test Accuracy: 0.184, Test F1-macro: 0.203\n",
      "01/236: Val Loss: 2.6716, Val F1macro: 0.1810,Test Accuracy: 0.167, Test F1-macro: 0.191\n",
      "01/237: Val Loss: 2.6405, Val F1macro: 0.1910,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/238: Val Loss: 2.7007, Val F1macro: 0.1902,Test Accuracy: 0.182, Test F1-macro: 0.202\n",
      "01/239: Val Loss: 2.6305, Val F1macro: 0.1881,Test Accuracy: 0.182, Test F1-macro: 0.202\n",
      "01/240: Val Loss: 2.6232, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/241: Val Loss: 2.6343, Val F1macro: 0.1933,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/242: Val Loss: 2.6365, Val F1macro: 0.1935,Test Accuracy: 0.173, Test F1-macro: 0.199\n",
      "01/243: Val Loss: 2.6286, Val F1macro: 0.1886,Test Accuracy: 0.181, Test F1-macro: 0.200\n",
      "01/244: Val Loss: 2.6244, Val F1macro: 0.1889,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/245: Val Loss: 2.6296, Val F1macro: 0.1893,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/246: Val Loss: 2.6268, Val F1macro: 0.1874,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/247: Val Loss: 2.6411, Val F1macro: 0.1886,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/248: Val Loss: 2.6416, Val F1macro: 0.1894,Test Accuracy: 0.178, Test F1-macro: 0.202\n",
      "01/249: Val Loss: 2.6752, Val F1macro: 0.1913,Test Accuracy: 0.181, Test F1-macro: 0.202\n",
      "01/250: Val Loss: 2.6260, Val F1macro: 0.1883,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/251: Val Loss: 2.6480, Val F1macro: 0.1914,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/252: Val Loss: 2.6386, Val F1macro: 0.1903,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/253: Val Loss: 2.6289, Val F1macro: 0.1880,Test Accuracy: 0.180, Test F1-macro: 0.200\n",
      "01/254: Val Loss: 2.6632, Val F1macro: 0.1866,Test Accuracy: 0.180, Test F1-macro: 0.201\n",
      "01/255: Val Loss: 2.6302, Val F1macro: 0.1902,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/256: Val Loss: 2.6299, Val F1macro: 0.1871,Test Accuracy: 0.177, Test F1-macro: 0.195\n",
      "01/257: Val Loss: 2.6548, Val F1macro: 0.1787,Test Accuracy: 0.165, Test F1-macro: 0.185\n",
      "01/258: Val Loss: 2.6549, Val F1macro: 0.1872,Test Accuracy: 0.182, Test F1-macro: 0.202\n",
      "01/259: Val Loss: 2.6228, Val F1macro: 0.1876,Test Accuracy: 0.183, Test F1-macro: 0.200\n",
      "01/260: Val Loss: 2.6345, Val F1macro: 0.1881,Test Accuracy: 0.184, Test F1-macro: 0.201\n",
      "01/261: Val Loss: 2.6229, Val F1macro: 0.1886,Test Accuracy: 0.184, Test F1-macro: 0.203\n",
      "01/262: Val Loss: 2.6341, Val F1macro: 0.1877,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/263: Val Loss: 2.6316, Val F1macro: 0.1924,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/264: Val Loss: 2.6392, Val F1macro: 0.1885,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/265: Val Loss: 2.6308, Val F1macro: 0.1914,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/266: Val Loss: 2.6238, Val F1macro: 0.1870,Test Accuracy: 0.184, Test F1-macro: 0.201\n",
      "01/267: Val Loss: 2.6246, Val F1macro: 0.1884,Test Accuracy: 0.183, Test F1-macro: 0.202\n",
      "01/268: Val Loss: 2.6374, Val F1macro: 0.1976,Test Accuracy: 0.179, Test F1-macro: 0.201\n",
      "01/269: Val Loss: 2.6238, Val F1macro: 0.1879,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/270: Val Loss: 2.6238, Val F1macro: 0.1868,Test Accuracy: 0.184, Test F1-macro: 0.201\n",
      "01/271: Val Loss: 2.6265, Val F1macro: 0.1892,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/272: Val Loss: 2.6295, Val F1macro: 0.1906,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/273: Val Loss: 2.6314, Val F1macro: 0.1902,Test Accuracy: 0.177, Test F1-macro: 0.197\n",
      "01/274: Val Loss: 2.6368, Val F1macro: 0.1868,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/275: Val Loss: 2.6330, Val F1macro: 0.1867,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/276: Val Loss: 2.6252, Val F1macro: 0.1900,Test Accuracy: 0.181, Test F1-macro: 0.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/277: Val Loss: 2.6230, Val F1macro: 0.1880,Test Accuracy: 0.181, Test F1-macro: 0.202\n",
      "01/278: Val Loss: 2.6272, Val F1macro: 0.1885,Test Accuracy: 0.180, Test F1-macro: 0.202\n",
      "01/279: Val Loss: 2.6207, Val F1macro: 0.1886,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/280: Val Loss: 2.6205, Val F1macro: 0.1875,Test Accuracy: 0.183, Test F1-macro: 0.201\n",
      "01/281: Val Loss: 2.6276, Val F1macro: 0.1949,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/282: Val Loss: 2.6333, Val F1macro: 0.1918,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/283: Val Loss: 2.6528, Val F1macro: 0.1919,Test Accuracy: 0.184, Test F1-macro: 0.203\n",
      "01/284: Val Loss: 2.6467, Val F1macro: 0.1855,Test Accuracy: 0.172, Test F1-macro: 0.179\n",
      "01/285: Val Loss: 2.6859, Val F1macro: 0.1954,Test Accuracy: 0.174, Test F1-macro: 0.200\n",
      "01/286: Val Loss: 2.6335, Val F1macro: 0.1962,Test Accuracy: 0.178, Test F1-macro: 0.202\n",
      "01/287: Val Loss: 2.6272, Val F1macro: 0.1894,Test Accuracy: 0.177, Test F1-macro: 0.194\n",
      "01/288: Val Loss: 2.6360, Val F1macro: 0.1952,Test Accuracy: 0.178, Test F1-macro: 0.201\n",
      "01/289: Val Loss: 2.6791, Val F1macro: 0.1801,Test Accuracy: 0.163, Test F1-macro: 0.183\n",
      "01/290: Val Loss: 2.6253, Val F1macro: 0.1897,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/291: Val Loss: 2.6393, Val F1macro: 0.1919,Test Accuracy: 0.167, Test F1-macro: 0.192\n",
      "01/292: Val Loss: 2.6330, Val F1macro: 0.1899,Test Accuracy: 0.176, Test F1-macro: 0.195\n",
      "01/293: Val Loss: 2.6918, Val F1macro: 0.1946,Test Accuracy: 0.173, Test F1-macro: 0.199\n",
      "01/294: Val Loss: 2.6573, Val F1macro: 0.1781,Test Accuracy: 0.164, Test F1-macro: 0.185\n",
      "01/295: Val Loss: 2.6274, Val F1macro: 0.1878,Test Accuracy: 0.180, Test F1-macro: 0.200\n",
      "01/296: Val Loss: 2.6501, Val F1macro: 0.1942,Test Accuracy: 0.167, Test F1-macro: 0.192\n",
      "01/297: Val Loss: 2.6548, Val F1macro: 0.1794,Test Accuracy: 0.166, Test F1-macro: 0.186\n",
      "01/298: Val Loss: 2.6770, Val F1macro: 0.1802,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/299: Val Loss: 2.6563, Val F1macro: 0.1946,Test Accuracy: 0.170, Test F1-macro: 0.195\n",
      "01/300: Val Loss: 2.6556, Val F1macro: 0.1787,Test Accuracy: 0.166, Test F1-macro: 0.185\n",
      "01/301: Val Loss: 2.6789, Val F1macro: 0.1883,Test Accuracy: 0.171, Test F1-macro: 0.191\n",
      "01/302: Val Loss: 2.6296, Val F1macro: 0.1876,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/303: Val Loss: 2.7856, Val F1macro: 0.1346,Test Accuracy: 0.153, Test F1-macro: 0.130\n",
      "01/304: Val Loss: 2.6674, Val F1macro: 0.1786,Test Accuracy: 0.166, Test F1-macro: 0.184\n",
      "01/305: Val Loss: 2.6265, Val F1macro: 0.1894,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/306: Val Loss: 2.6477, Val F1macro: 0.1920,Test Accuracy: 0.171, Test F1-macro: 0.192\n",
      "01/307: Val Loss: 2.6345, Val F1macro: 0.1910,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/308: Val Loss: 2.6326, Val F1macro: 0.1899,Test Accuracy: 0.174, Test F1-macro: 0.191\n",
      "01/309: Val Loss: 2.6445, Val F1macro: 0.1912,Test Accuracy: 0.174, Test F1-macro: 0.193\n",
      "01/310: Val Loss: 2.6476, Val F1macro: 0.1867,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/311: Val Loss: 2.6387, Val F1macro: 0.1941,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/312: Val Loss: 2.6407, Val F1macro: 0.1985,Test Accuracy: 0.177, Test F1-macro: 0.201\n",
      "01/313: Val Loss: 2.6288, Val F1macro: 0.1910,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/314: Val Loss: 2.6310, Val F1macro: 0.1901,Test Accuracy: 0.177, Test F1-macro: 0.198\n",
      "01/315: Val Loss: 2.6492, Val F1macro: 0.1900,Test Accuracy: 0.173, Test F1-macro: 0.195\n",
      "01/316: Val Loss: 2.6295, Val F1macro: 0.1886,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/317: Val Loss: 2.6314, Val F1macro: 0.1896,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/318: Val Loss: 2.6803, Val F1macro: 0.1343,Test Accuracy: 0.152, Test F1-macro: 0.135\n",
      "01/319: Val Loss: 2.6288, Val F1macro: 0.1909,Test Accuracy: 0.179, Test F1-macro: 0.201\n",
      "01/320: Val Loss: 2.6783, Val F1macro: 0.1348,Test Accuracy: 0.155, Test F1-macro: 0.134\n",
      "01/321: Val Loss: 2.6509, Val F1macro: 0.1856,Test Accuracy: 0.168, Test F1-macro: 0.190\n",
      "01/322: Val Loss: 2.7045, Val F1macro: 0.1779,Test Accuracy: 0.164, Test F1-macro: 0.183\n",
      "01/323: Val Loss: 2.6705, Val F1macro: 0.1791,Test Accuracy: 0.164, Test F1-macro: 0.183\n",
      "01/324: Val Loss: 2.6648, Val F1macro: 0.1939,Test Accuracy: 0.167, Test F1-macro: 0.192\n",
      "01/325: Val Loss: 2.6558, Val F1macro: 0.1835,Test Accuracy: 0.171, Test F1-macro: 0.190\n",
      "01/326: Val Loss: 2.6286, Val F1macro: 0.1865,Test Accuracy: 0.178, Test F1-macro: 0.194\n",
      "01/327: Val Loss: 2.6330, Val F1macro: 0.1874,Test Accuracy: 0.177, Test F1-macro: 0.197\n",
      "01/328: Val Loss: 2.7191, Val F1macro: 0.1104,Test Accuracy: 0.140, Test F1-macro: 0.117\n",
      "01/329: Val Loss: 2.6483, Val F1macro: 0.1807,Test Accuracy: 0.166, Test F1-macro: 0.186\n",
      "01/330: Val Loss: 2.6308, Val F1macro: 0.1889,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/331: Val Loss: 2.6281, Val F1macro: 0.1882,Test Accuracy: 0.180, Test F1-macro: 0.198\n",
      "01/332: Val Loss: 2.6426, Val F1macro: 0.1893,Test Accuracy: 0.180, Test F1-macro: 0.200\n",
      "01/333: Val Loss: 2.6241, Val F1macro: 0.1897,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/334: Val Loss: 2.6281, Val F1macro: 0.1881,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/335: Val Loss: 2.6355, Val F1macro: 0.1893,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/336: Val Loss: 2.6403, Val F1macro: 0.1845,Test Accuracy: 0.174, Test F1-macro: 0.194\n",
      "01/337: Val Loss: 2.6246, Val F1macro: 0.1885,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/338: Val Loss: 2.6366, Val F1macro: 0.1955,Test Accuracy: 0.175, Test F1-macro: 0.200\n",
      "01/339: Val Loss: 2.6261, Val F1macro: 0.1888,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/340: Val Loss: 2.6491, Val F1macro: 0.1862,Test Accuracy: 0.173, Test F1-macro: 0.192\n",
      "01/341: Val Loss: 2.6299, Val F1macro: 0.1892,Test Accuracy: 0.183, Test F1-macro: 0.199\n",
      "01/342: Val Loss: 2.6525, Val F1macro: 0.1846,Test Accuracy: 0.174, Test F1-macro: 0.193\n",
      "01/343: Val Loss: 2.6409, Val F1macro: 0.1918,Test Accuracy: 0.177, Test F1-macro: 0.197\n",
      "01/344: Val Loss: 2.6739, Val F1macro: 0.1817,Test Accuracy: 0.164, Test F1-macro: 0.183\n",
      "01/345: Val Loss: 2.6516, Val F1macro: 0.1764,Test Accuracy: 0.163, Test F1-macro: 0.182\n",
      "01/346: Val Loss: 2.6239, Val F1macro: 0.1887,Test Accuracy: 0.180, Test F1-macro: 0.197\n",
      "01/347: Val Loss: 2.6460, Val F1macro: 0.1939,Test Accuracy: 0.176, Test F1-macro: 0.198\n",
      "01/348: Val Loss: 2.6457, Val F1macro: 0.1797,Test Accuracy: 0.167, Test F1-macro: 0.191\n",
      "01/349: Val Loss: 2.6344, Val F1macro: 0.1849,Test Accuracy: 0.174, Test F1-macro: 0.194\n",
      "01/350: Val Loss: 2.6405, Val F1macro: 0.1887,Test Accuracy: 0.181, Test F1-macro: 0.197\n",
      "01/351: Val Loss: 2.6414, Val F1macro: 0.1890,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/352: Val Loss: 2.6387, Val F1macro: 0.1929,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/353: Val Loss: 2.6391, Val F1macro: 0.1834,Test Accuracy: 0.168, Test F1-macro: 0.189\n",
      "01/354: Val Loss: 2.6391, Val F1macro: 0.1906,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/355: Val Loss: 2.6536, Val F1macro: 0.1895,Test Accuracy: 0.178, Test F1-macro: 0.199\n",
      "01/356: Val Loss: 2.6316, Val F1macro: 0.1887,Test Accuracy: 0.179, Test F1-macro: 0.200\n",
      "01/357: Val Loss: 2.7742, Val F1macro: 0.1871,Test Accuracy: 0.174, Test F1-macro: 0.193\n",
      "01/358: Val Loss: 2.6403, Val F1macro: 0.1901,Test Accuracy: 0.177, Test F1-macro: 0.198\n",
      "01/359: Val Loss: 2.6491, Val F1macro: 0.1733,Test Accuracy: 0.167, Test F1-macro: 0.184\n",
      "01/360: Val Loss: 2.6280, Val F1macro: 0.1863,Test Accuracy: 0.177, Test F1-macro: 0.195\n",
      "01/361: Val Loss: 2.6516, Val F1macro: 0.1763,Test Accuracy: 0.164, Test F1-macro: 0.184\n",
      "01/362: Val Loss: 2.6380, Val F1macro: 0.1887,Test Accuracy: 0.184, Test F1-macro: 0.200\n",
      "01/363: Val Loss: 2.6267, Val F1macro: 0.1892,Test Accuracy: 0.182, Test F1-macro: 0.198\n",
      "01/364: Val Loss: 2.6339, Val F1macro: 0.1884,Test Accuracy: 0.179, Test F1-macro: 0.199\n",
      "01/365: Val Loss: 2.6550, Val F1macro: 0.1959,Test Accuracy: 0.170, Test F1-macro: 0.196\n",
      "01/366: Val Loss: 2.6311, Val F1macro: 0.1949,Test Accuracy: 0.177, Test F1-macro: 0.200\n",
      "01/367: Val Loss: 2.6487, Val F1macro: 0.1939,Test Accuracy: 0.169, Test F1-macro: 0.194\n",
      "01/368: Val Loss: 2.6321, Val F1macro: 0.1956,Test Accuracy: 0.180, Test F1-macro: 0.202\n",
      "01/369: Val Loss: 2.6554, Val F1macro: 0.1906,Test Accuracy: 0.167, Test F1-macro: 0.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/370: Val Loss: 2.6268, Val F1macro: 0.1875,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/371: Val Loss: 2.6833, Val F1macro: 0.1835,Test Accuracy: 0.163, Test F1-macro: 0.180\n",
      "01/372: Val Loss: 2.6413, Val F1macro: 0.1880,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/373: Val Loss: 2.6633, Val F1macro: 0.1750,Test Accuracy: 0.165, Test F1-macro: 0.184\n",
      "01/374: Val Loss: 2.6345, Val F1macro: 0.1934,Test Accuracy: 0.178, Test F1-macro: 0.202\n",
      "01/375: Val Loss: 2.6386, Val F1macro: 0.1925,Test Accuracy: 0.172, Test F1-macro: 0.190\n",
      "01/376: Val Loss: 2.6406, Val F1macro: 0.1870,Test Accuracy: 0.177, Test F1-macro: 0.195\n",
      "01/377: Val Loss: 2.7640, Val F1macro: 0.1852,Test Accuracy: 0.168, Test F1-macro: 0.187\n",
      "01/378: Val Loss: 2.6501, Val F1macro: 0.1862,Test Accuracy: 0.173, Test F1-macro: 0.192\n",
      "01/379: Val Loss: 2.6266, Val F1macro: 0.1876,Test Accuracy: 0.182, Test F1-macro: 0.199\n",
      "01/380: Val Loss: 2.7267, Val F1macro: 0.1921,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/381: Val Loss: 2.6744, Val F1macro: 0.1893,Test Accuracy: 0.176, Test F1-macro: 0.195\n",
      "01/382: Val Loss: 2.6355, Val F1macro: 0.1796,Test Accuracy: 0.174, Test F1-macro: 0.189\n",
      "01/383: Val Loss: 2.6294, Val F1macro: 0.1900,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/384: Val Loss: 2.6227, Val F1macro: 0.1877,Test Accuracy: 0.179, Test F1-macro: 0.196\n",
      "01/385: Val Loss: 2.6415, Val F1macro: 0.1937,Test Accuracy: 0.177, Test F1-macro: 0.199\n",
      "01/386: Val Loss: 2.6897, Val F1macro: 0.1362,Test Accuracy: 0.150, Test F1-macro: 0.134\n",
      "01/387: Val Loss: 2.6893, Val F1macro: 0.1352,Test Accuracy: 0.155, Test F1-macro: 0.135\n",
      "01/388: Val Loss: 2.6259, Val F1macro: 0.1870,Test Accuracy: 0.178, Test F1-macro: 0.196\n",
      "01/389: Val Loss: 2.6892, Val F1macro: 0.1924,Test Accuracy: 0.166, Test F1-macro: 0.187\n",
      "01/390: Val Loss: 2.6249, Val F1macro: 0.1888,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/391: Val Loss: 2.6416, Val F1macro: 0.1796,Test Accuracy: 0.171, Test F1-macro: 0.188\n",
      "01/392: Val Loss: 2.6898, Val F1macro: 0.1776,Test Accuracy: 0.165, Test F1-macro: 0.185\n",
      "01/393: Val Loss: 2.6435, Val F1macro: 0.1894,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/394: Val Loss: 2.6855, Val F1macro: 0.1778,Test Accuracy: 0.159, Test F1-macro: 0.173\n",
      "01/395: Val Loss: 2.6218, Val F1macro: 0.1870,Test Accuracy: 0.178, Test F1-macro: 0.193\n",
      "01/396: Val Loss: 2.6345, Val F1macro: 0.1907,Test Accuracy: 0.176, Test F1-macro: 0.196\n",
      "01/397: Val Loss: 2.6653, Val F1macro: 0.1761,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/398: Val Loss: 2.6301, Val F1macro: 0.1870,Test Accuracy: 0.176, Test F1-macro: 0.195\n",
      "01/399: Val Loss: 2.6353, Val F1macro: 0.1941,Test Accuracy: 0.175, Test F1-macro: 0.199\n",
      "01/400: Val Loss: 2.6400, Val F1macro: 0.1880,Test Accuracy: 0.169, Test F1-macro: 0.189\n",
      "01/401: Val Loss: 2.6495, Val F1macro: 0.1801,Test Accuracy: 0.166, Test F1-macro: 0.183\n",
      "01/402: Val Loss: 2.6275, Val F1macro: 0.1917,Test Accuracy: 0.181, Test F1-macro: 0.197\n",
      "01/403: Val Loss: 2.6311, Val F1macro: 0.1901,Test Accuracy: 0.181, Test F1-macro: 0.200\n",
      "01/404: Val Loss: 2.6870, Val F1macro: 0.1354,Test Accuracy: 0.155, Test F1-macro: 0.136\n",
      "01/405: Val Loss: 2.6637, Val F1macro: 0.1774,Test Accuracy: 0.165, Test F1-macro: 0.185\n",
      "01/406: Val Loss: 2.7221, Val F1macro: 0.1114,Test Accuracy: 0.137, Test F1-macro: 0.114\n",
      "01/407: Val Loss: 2.6353, Val F1macro: 0.1885,Test Accuracy: 0.183, Test F1-macro: 0.202\n",
      "01/408: Val Loss: 2.6409, Val F1macro: 0.1866,Test Accuracy: 0.174, Test F1-macro: 0.195\n",
      "01/409: Val Loss: 2.7131, Val F1macro: 0.1114,Test Accuracy: 0.138, Test F1-macro: 0.117\n",
      "01/410: Val Loss: 2.6417, Val F1macro: 0.1923,Test Accuracy: 0.171, Test F1-macro: 0.193\n",
      "01/411: Val Loss: 2.6473, Val F1macro: 0.1899,Test Accuracy: 0.164, Test F1-macro: 0.180\n",
      "01/412: Val Loss: 2.6415, Val F1macro: 0.1887,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "01/413: Val Loss: 2.6513, Val F1macro: 0.1881,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/414: Val Loss: 2.6193, Val F1macro: 0.1910,Test Accuracy: 0.185, Test F1-macro: 0.201\n",
      "01/415: Val Loss: 2.6401, Val F1macro: 0.1927,Test Accuracy: 0.170, Test F1-macro: 0.194\n",
      "01/416: Val Loss: 2.6204, Val F1macro: 0.1891,Test Accuracy: 0.181, Test F1-macro: 0.201\n",
      "01/417: Val Loss: 2.6349, Val F1macro: 0.1873,Test Accuracy: 0.178, Test F1-macro: 0.196\n",
      "01/418: Val Loss: 2.6303, Val F1macro: 0.1882,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/419: Val Loss: 2.6254, Val F1macro: 0.1884,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/420: Val Loss: 2.6458, Val F1macro: 0.1893,Test Accuracy: 0.183, Test F1-macro: 0.198\n",
      "01/421: Val Loss: 2.6422, Val F1macro: 0.1885,Test Accuracy: 0.183, Test F1-macro: 0.199\n",
      "01/422: Val Loss: 2.6508, Val F1macro: 0.1777,Test Accuracy: 0.169, Test F1-macro: 0.187\n",
      "01/423: Val Loss: 2.6420, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.200\n",
      "01/424: Val Loss: 2.6624, Val F1macro: 0.1868,Test Accuracy: 0.174, Test F1-macro: 0.196\n",
      "01/425: Val Loss: 2.6534, Val F1macro: 0.1914,Test Accuracy: 0.175, Test F1-macro: 0.196\n",
      "01/426: Val Loss: 2.6290, Val F1macro: 0.1884,Test Accuracy: 0.183, Test F1-macro: 0.196\n",
      "01/427: Val Loss: 2.6697, Val F1macro: 0.1785,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/428: Val Loss: 2.7035, Val F1macro: 0.1805,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/429: Val Loss: 2.6506, Val F1macro: 0.1950,Test Accuracy: 0.172, Test F1-macro: 0.193\n",
      "01/430: Val Loss: 2.6866, Val F1macro: 0.1893,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/431: Val Loss: 2.6295, Val F1macro: 0.1877,Test Accuracy: 0.182, Test F1-macro: 0.201\n",
      "01/432: Val Loss: 2.6211, Val F1macro: 0.1853,Test Accuracy: 0.180, Test F1-macro: 0.193\n",
      "01/433: Val Loss: 2.6764, Val F1macro: 0.1894,Test Accuracy: 0.166, Test F1-macro: 0.186\n",
      "01/434: Val Loss: 2.6479, Val F1macro: 0.1775,Test Accuracy: 0.170, Test F1-macro: 0.188\n",
      "01/435: Val Loss: 2.6790, Val F1macro: 0.1363,Test Accuracy: 0.155, Test F1-macro: 0.134\n",
      "01/436: Val Loss: 2.6327, Val F1macro: 0.1892,Test Accuracy: 0.181, Test F1-macro: 0.200\n",
      "01/437: Val Loss: 2.6403, Val F1macro: 0.1916,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/438: Val Loss: 2.6237, Val F1macro: 0.1867,Test Accuracy: 0.184, Test F1-macro: 0.200\n",
      "01/439: Val Loss: 2.6253, Val F1macro: 0.1877,Test Accuracy: 0.179, Test F1-macro: 0.195\n",
      "01/440: Val Loss: 2.6865, Val F1macro: 0.1365,Test Accuracy: 0.154, Test F1-macro: 0.134\n",
      "01/441: Val Loss: 2.6773, Val F1macro: 0.1792,Test Accuracy: 0.166, Test F1-macro: 0.184\n",
      "01/442: Val Loss: 2.6907, Val F1macro: 0.1939,Test Accuracy: 0.168, Test F1-macro: 0.191\n",
      "01/443: Val Loss: 2.6469, Val F1macro: 0.1766,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/444: Val Loss: 2.6386, Val F1macro: 0.1876,Test Accuracy: 0.183, Test F1-macro: 0.198\n",
      "01/445: Val Loss: 2.7312, Val F1macro: 0.1091,Test Accuracy: 0.140, Test F1-macro: 0.113\n",
      "01/446: Val Loss: 2.6520, Val F1macro: 0.1783,Test Accuracy: 0.165, Test F1-macro: 0.184\n",
      "01/447: Val Loss: 2.6337, Val F1macro: 0.1881,Test Accuracy: 0.179, Test F1-macro: 0.198\n",
      "01/448: Val Loss: 2.6312, Val F1macro: 0.1920,Test Accuracy: 0.182, Test F1-macro: 0.197\n",
      "01/449: Val Loss: 2.6378, Val F1macro: 0.1835,Test Accuracy: 0.174, Test F1-macro: 0.190\n",
      "01/450: Val Loss: 2.6787, Val F1macro: 0.1799,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/451: Val Loss: 2.6376, Val F1macro: 0.1895,Test Accuracy: 0.172, Test F1-macro: 0.194\n",
      "01/452: Val Loss: 2.6246, Val F1macro: 0.1849,Test Accuracy: 0.178, Test F1-macro: 0.192\n",
      "01/453: Val Loss: 2.6299, Val F1macro: 0.1841,Test Accuracy: 0.172, Test F1-macro: 0.185\n",
      "01/454: Val Loss: 2.6720, Val F1macro: 0.1762,Test Accuracy: 0.167, Test F1-macro: 0.185\n",
      "01/455: Val Loss: 2.6310, Val F1macro: 0.1936,Test Accuracy: 0.177, Test F1-macro: 0.193\n",
      "01/456: Val Loss: 2.6272, Val F1macro: 0.1860,Test Accuracy: 0.175, Test F1-macro: 0.191\n",
      "01/457: Val Loss: 2.6262, Val F1macro: 0.1865,Test Accuracy: 0.177, Test F1-macro: 0.195\n",
      "01/458: Val Loss: 2.6348, Val F1macro: 0.1901,Test Accuracy: 0.184, Test F1-macro: 0.201\n",
      "01/459: Val Loss: 2.6365, Val F1macro: 0.1866,Test Accuracy: 0.171, Test F1-macro: 0.192\n",
      "01/460: Val Loss: 2.6339, Val F1macro: 0.1886,Test Accuracy: 0.180, Test F1-macro: 0.196\n",
      "01/461: Val Loss: 2.6358, Val F1macro: 0.1930,Test Accuracy: 0.177, Test F1-macro: 0.196\n",
      "01/462: Val Loss: 2.6351, Val F1macro: 0.1923,Test Accuracy: 0.181, Test F1-macro: 0.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/463: Val Loss: 2.6836, Val F1macro: 0.1921,Test Accuracy: 0.175, Test F1-macro: 0.196\n",
      "01/464: Val Loss: 2.6249, Val F1macro: 0.1889,Test Accuracy: 0.182, Test F1-macro: 0.198\n",
      "01/465: Val Loss: 2.7476, Val F1macro: 0.1734,Test Accuracy: 0.164, Test F1-macro: 0.181\n",
      "01/466: Val Loss: 2.6440, Val F1macro: 0.1775,Test Accuracy: 0.170, Test F1-macro: 0.187\n",
      "01/467: Val Loss: 2.6356, Val F1macro: 0.1887,Test Accuracy: 0.181, Test F1-macro: 0.199\n",
      "01/468: Val Loss: 2.6423, Val F1macro: 0.1923,Test Accuracy: 0.174, Test F1-macro: 0.192\n",
      "01/469: Val Loss: 2.6274, Val F1macro: 0.1892,Test Accuracy: 0.183, Test F1-macro: 0.197\n",
      "01/470: Val Loss: 2.6292, Val F1macro: 0.1878,Test Accuracy: 0.173, Test F1-macro: 0.194\n",
      "01/471: Val Loss: 2.6310, Val F1macro: 0.1878,Test Accuracy: 0.175, Test F1-macro: 0.188\n",
      "01/472: Val Loss: 2.7060, Val F1macro: 0.1879,Test Accuracy: 0.167, Test F1-macro: 0.189\n",
      "01/473: Val Loss: 2.6803, Val F1macro: 0.1816,Test Accuracy: 0.161, Test F1-macro: 0.183\n",
      "01/474: Val Loss: 2.6395, Val F1macro: 0.1885,Test Accuracy: 0.182, Test F1-macro: 0.197\n",
      "01/475: Val Loss: 2.6397, Val F1macro: 0.1776,Test Accuracy: 0.167, Test F1-macro: 0.186\n",
      "01/476: Val Loss: 2.6293, Val F1macro: 0.1875,Test Accuracy: 0.177, Test F1-macro: 0.197\n",
      "01/477: Val Loss: 2.6405, Val F1macro: 0.1887,Test Accuracy: 0.179, Test F1-macro: 0.197\n",
      "01/478: Val Loss: 2.6586, Val F1macro: 0.1837,Test Accuracy: 0.168, Test F1-macro: 0.187\n",
      "01/479: Val Loss: 2.6442, Val F1macro: 0.1890,Test Accuracy: 0.170, Test F1-macro: 0.191\n",
      "01/480: Val Loss: 2.6287, Val F1macro: 0.1905,Test Accuracy: 0.181, Test F1-macro: 0.198\n",
      "01/481: Val Loss: 2.6312, Val F1macro: 0.1903,Test Accuracy: 0.182, Test F1-macro: 0.197\n",
      "01/482: Val Loss: 2.6302, Val F1macro: 0.1847,Test Accuracy: 0.173, Test F1-macro: 0.188\n",
      "01/483: Val Loss: 2.6450, Val F1macro: 0.1917,Test Accuracy: 0.168, Test F1-macro: 0.186\n",
      "01/484: Val Loss: 2.6330, Val F1macro: 0.1883,Test Accuracy: 0.184, Test F1-macro: 0.200\n",
      "01/485: Val Loss: 2.6547, Val F1macro: 0.1908,Test Accuracy: 0.175, Test F1-macro: 0.194\n",
      "01/486: Val Loss: 2.6437, Val F1macro: 0.1747,Test Accuracy: 0.169, Test F1-macro: 0.186\n",
      "01/487: Val Loss: 2.6433, Val F1macro: 0.1965,Test Accuracy: 0.169, Test F1-macro: 0.190\n",
      "01/488: Val Loss: 2.6460, Val F1macro: 0.1863,Test Accuracy: 0.176, Test F1-macro: 0.193\n",
      "01/489: Val Loss: 2.6406, Val F1macro: 0.1870,Test Accuracy: 0.173, Test F1-macro: 0.192\n",
      "01/490: Val Loss: 2.6381, Val F1macro: 0.1881,Test Accuracy: 0.178, Test F1-macro: 0.197\n",
      "01/491: Val Loss: 2.6578, Val F1macro: 0.1774,Test Accuracy: 0.163, Test F1-macro: 0.183\n",
      "01/492: Val Loss: 2.6370, Val F1macro: 0.1891,Test Accuracy: 0.178, Test F1-macro: 0.198\n",
      "01/493: Val Loss: 2.6416, Val F1macro: 0.1742,Test Accuracy: 0.170, Test F1-macro: 0.182\n",
      "01/494: Val Loss: 2.6301, Val F1macro: 0.1825,Test Accuracy: 0.175, Test F1-macro: 0.189\n",
      "01/495: Val Loss: 2.6422, Val F1macro: 0.1896,Test Accuracy: 0.175, Test F1-macro: 0.197\n",
      "01/496: Val Loss: 2.6518, Val F1macro: 0.1756,Test Accuracy: 0.165, Test F1-macro: 0.183\n",
      "01/497: Val Loss: 2.6644, Val F1macro: 0.1783,Test Accuracy: 0.165, Test F1-macro: 0.184\n",
      "01/498: Val Loss: 2.6265, Val F1macro: 0.1892,Test Accuracy: 0.181, Test F1-macro: 0.200\n",
      "01/499: Val Loss: 2.6230, Val F1macro: 0.1885,Test Accuracy: 0.182, Test F1-macro: 0.198\n",
      "01/500: Val Loss: 2.6469, Val F1macro: 0.1878,Test Accuracy: 0.180, Test F1-macro: 0.199\n",
      "02/001: Val Loss: 3.1134, Val F1macro: 0.0530,Test Accuracy: 0.126, Test F1-macro: 0.053\n",
      "02/002: Val Loss: 2.7865, Val F1macro: 0.0496,Test Accuracy: 0.117, Test F1-macro: 0.048\n",
      "02/003: Val Loss: 2.7355, Val F1macro: 0.0656,Test Accuracy: 0.132, Test F1-macro: 0.067\n",
      "02/004: Val Loss: 8.0775, Val F1macro: 0.0535,Test Accuracy: 0.092, Test F1-macro: 0.055\n",
      "02/005: Val Loss: 2.7295, Val F1macro: 0.0845,Test Accuracy: 0.133, Test F1-macro: 0.083\n",
      "02/006: Val Loss: 2.7310, Val F1macro: 0.0533,Test Accuracy: 0.122, Test F1-macro: 0.056\n",
      "02/007: Val Loss: 2.7467, Val F1macro: 0.0664,Test Accuracy: 0.132, Test F1-macro: 0.074\n",
      "02/008: Val Loss: 2.7561, Val F1macro: 0.0914,Test Accuracy: 0.139, Test F1-macro: 0.096\n",
      "02/009: Val Loss: 2.7748, Val F1macro: 0.0894,Test Accuracy: 0.141, Test F1-macro: 0.100\n",
      "02/010: Val Loss: 2.7647, Val F1macro: 0.0772,Test Accuracy: 0.139, Test F1-macro: 0.087\n",
      "02/011: Val Loss: 2.7551, Val F1macro: 0.0672,Test Accuracy: 0.132, Test F1-macro: 0.072\n",
      "02/012: Val Loss: 2.7267, Val F1macro: 0.0654,Test Accuracy: 0.131, Test F1-macro: 0.075\n",
      "02/013: Val Loss: 2.7567, Val F1macro: 0.0697,Test Accuracy: 0.107, Test F1-macro: 0.078\n",
      "02/014: Val Loss: 2.7303, Val F1macro: 0.0638,Test Accuracy: 0.123, Test F1-macro: 0.067\n",
      "02/015: Val Loss: 2.8438, Val F1macro: 0.0831,Test Accuracy: 0.123, Test F1-macro: 0.091\n",
      "02/016: Val Loss: 2.7043, Val F1macro: 0.0899,Test Accuracy: 0.143, Test F1-macro: 0.098\n",
      "02/017: Val Loss: 2.7107, Val F1macro: 0.0850,Test Accuracy: 0.143, Test F1-macro: 0.093\n",
      "02/018: Val Loss: 2.6973, Val F1macro: 0.0966,Test Accuracy: 0.149, Test F1-macro: 0.112\n",
      "02/019: Val Loss: 2.7128, Val F1macro: 0.0800,Test Accuracy: 0.143, Test F1-macro: 0.090\n",
      "02/020: Val Loss: 2.6908, Val F1macro: 0.0980,Test Accuracy: 0.148, Test F1-macro: 0.112\n",
      "02/021: Val Loss: 2.7036, Val F1macro: 0.0539,Test Accuracy: 0.126, Test F1-macro: 0.055\n",
      "02/022: Val Loss: 2.6891, Val F1macro: 0.0872,Test Accuracy: 0.145, Test F1-macro: 0.098\n",
      "02/023: Val Loss: 2.7156, Val F1macro: 0.0942,Test Accuracy: 0.142, Test F1-macro: 0.103\n",
      "02/024: Val Loss: 2.6950, Val F1macro: 0.0919,Test Accuracy: 0.142, Test F1-macro: 0.099\n",
      "02/025: Val Loss: 2.6981, Val F1macro: 0.0956,Test Accuracy: 0.146, Test F1-macro: 0.099\n",
      "02/026: Val Loss: 2.7428, Val F1macro: 0.0592,Test Accuracy: 0.122, Test F1-macro: 0.066\n",
      "02/027: Val Loss: 2.6893, Val F1macro: 0.0918,Test Accuracy: 0.146, Test F1-macro: 0.098\n",
      "02/028: Val Loss: 2.6973, Val F1macro: 0.0909,Test Accuracy: 0.146, Test F1-macro: 0.099\n",
      "02/029: Val Loss: 2.6999, Val F1macro: 0.0930,Test Accuracy: 0.147, Test F1-macro: 0.103\n",
      "02/030: Val Loss: 2.7116, Val F1macro: 0.0965,Test Accuracy: 0.144, Test F1-macro: 0.104\n",
      "02/031: Val Loss: 2.6913, Val F1macro: 0.0997,Test Accuracy: 0.152, Test F1-macro: 0.118\n",
      "02/032: Val Loss: 2.6879, Val F1macro: 0.1079,Test Accuracy: 0.153, Test F1-macro: 0.123\n",
      "02/033: Val Loss: 2.6909, Val F1macro: 0.0807,Test Accuracy: 0.146, Test F1-macro: 0.096\n",
      "02/034: Val Loss: 2.8073, Val F1macro: 0.0738,Test Accuracy: 0.113, Test F1-macro: 0.079\n",
      "02/035: Val Loss: 2.6932, Val F1macro: 0.0904,Test Accuracy: 0.149, Test F1-macro: 0.099\n",
      "02/036: Val Loss: 2.9993, Val F1macro: 0.0630,Test Accuracy: 0.108, Test F1-macro: 0.064\n",
      "02/037: Val Loss: 2.7575, Val F1macro: 0.0597,Test Accuracy: 0.127, Test F1-macro: 0.063\n",
      "02/038: Val Loss: 2.9203, Val F1macro: 0.0729,Test Accuracy: 0.107, Test F1-macro: 0.080\n",
      "02/039: Val Loss: 2.9984, Val F1macro: 0.0639,Test Accuracy: 0.115, Test F1-macro: 0.076\n",
      "02/040: Val Loss: 2.6808, Val F1macro: 0.1142,Test Accuracy: 0.156, Test F1-macro: 0.126\n",
      "02/041: Val Loss: 2.6897, Val F1macro: 0.0989,Test Accuracy: 0.155, Test F1-macro: 0.106\n",
      "02/042: Val Loss: 2.7353, Val F1macro: 0.0718,Test Accuracy: 0.136, Test F1-macro: 0.081\n",
      "02/043: Val Loss: 2.6916, Val F1macro: 0.0959,Test Accuracy: 0.150, Test F1-macro: 0.106\n",
      "02/044: Val Loss: 2.6930, Val F1macro: 0.0931,Test Accuracy: 0.149, Test F1-macro: 0.098\n",
      "02/045: Val Loss: 3.4846, Val F1macro: 0.0881,Test Accuracy: 0.118, Test F1-macro: 0.089\n",
      "02/046: Val Loss: 2.6846, Val F1macro: 0.0915,Test Accuracy: 0.155, Test F1-macro: 0.103\n",
      "02/047: Val Loss: 2.8239, Val F1macro: 0.1168,Test Accuracy: 0.160, Test F1-macro: 0.122\n",
      "02/048: Val Loss: 2.6922, Val F1macro: 0.0954,Test Accuracy: 0.151, Test F1-macro: 0.109\n",
      "02/049: Val Loss: 2.7208, Val F1macro: 0.0735,Test Accuracy: 0.135, Test F1-macro: 0.088\n",
      "02/050: Val Loss: 2.9483, Val F1macro: 0.0792,Test Accuracy: 0.152, Test F1-macro: 0.090\n",
      "02/051: Val Loss: 2.7768, Val F1macro: 0.0903,Test Accuracy: 0.135, Test F1-macro: 0.107\n",
      "02/052: Val Loss: 2.7183, Val F1macro: 0.0825,Test Accuracy: 0.147, Test F1-macro: 0.099\n",
      "02/053: Val Loss: 2.7051, Val F1macro: 0.0997,Test Accuracy: 0.154, Test F1-macro: 0.117\n",
      "02/054: Val Loss: 2.7457, Val F1macro: 0.0696,Test Accuracy: 0.128, Test F1-macro: 0.081\n",
      "02/055: Val Loss: 2.8062, Val F1macro: 0.0829,Test Accuracy: 0.117, Test F1-macro: 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/056: Val Loss: 2.6728, Val F1macro: 0.1089,Test Accuracy: 0.161, Test F1-macro: 0.123\n",
      "02/057: Val Loss: 2.6803, Val F1macro: 0.0935,Test Accuracy: 0.155, Test F1-macro: 0.107\n",
      "02/058: Val Loss: 2.8411, Val F1macro: 0.0708,Test Accuracy: 0.114, Test F1-macro: 0.082\n",
      "02/059: Val Loss: 2.8271, Val F1macro: 0.0749,Test Accuracy: 0.118, Test F1-macro: 0.089\n",
      "02/060: Val Loss: 2.6948, Val F1macro: 0.0892,Test Accuracy: 0.149, Test F1-macro: 0.115\n",
      "02/061: Val Loss: 2.6801, Val F1macro: 0.1062,Test Accuracy: 0.153, Test F1-macro: 0.119\n",
      "02/062: Val Loss: 2.7229, Val F1macro: 0.0813,Test Accuracy: 0.139, Test F1-macro: 0.104\n",
      "02/063: Val Loss: 2.6740, Val F1macro: 0.0985,Test Accuracy: 0.155, Test F1-macro: 0.112\n",
      "02/064: Val Loss: 2.6765, Val F1macro: 0.1130,Test Accuracy: 0.158, Test F1-macro: 0.124\n",
      "02/065: Val Loss: 2.6737, Val F1macro: 0.1074,Test Accuracy: 0.157, Test F1-macro: 0.121\n",
      "02/066: Val Loss: 2.6818, Val F1macro: 0.0978,Test Accuracy: 0.152, Test F1-macro: 0.107\n",
      "02/067: Val Loss: 2.6720, Val F1macro: 0.1059,Test Accuracy: 0.156, Test F1-macro: 0.118\n",
      "02/068: Val Loss: 2.6744, Val F1macro: 0.1049,Test Accuracy: 0.154, Test F1-macro: 0.118\n",
      "02/069: Val Loss: 2.6763, Val F1macro: 0.1064,Test Accuracy: 0.156, Test F1-macro: 0.119\n",
      "02/070: Val Loss: 2.6707, Val F1macro: 0.1043,Test Accuracy: 0.156, Test F1-macro: 0.118\n",
      "02/071: Val Loss: 2.6689, Val F1macro: 0.1052,Test Accuracy: 0.157, Test F1-macro: 0.121\n",
      "02/072: Val Loss: 2.6748, Val F1macro: 0.1034,Test Accuracy: 0.152, Test F1-macro: 0.116\n",
      "02/073: Val Loss: 2.6794, Val F1macro: 0.1025,Test Accuracy: 0.149, Test F1-macro: 0.116\n",
      "02/074: Val Loss: 2.6690, Val F1macro: 0.1032,Test Accuracy: 0.154, Test F1-macro: 0.116\n",
      "02/075: Val Loss: 2.6736, Val F1macro: 0.1062,Test Accuracy: 0.160, Test F1-macro: 0.121\n",
      "02/076: Val Loss: 2.6912, Val F1macro: 0.1007,Test Accuracy: 0.155, Test F1-macro: 0.117\n",
      "02/077: Val Loss: 2.6753, Val F1macro: 0.1111,Test Accuracy: 0.155, Test F1-macro: 0.118\n",
      "02/078: Val Loss: 2.7505, Val F1macro: 0.0714,Test Accuracy: 0.129, Test F1-macro: 0.085\n",
      "02/079: Val Loss: 2.6705, Val F1macro: 0.1051,Test Accuracy: 0.155, Test F1-macro: 0.117\n",
      "02/080: Val Loss: 2.6651, Val F1macro: 0.1217,Test Accuracy: 0.158, Test F1-macro: 0.125\n",
      "02/081: Val Loss: 2.6681, Val F1macro: 0.1214,Test Accuracy: 0.162, Test F1-macro: 0.126\n",
      "02/082: Val Loss: 2.6672, Val F1macro: 0.1178,Test Accuracy: 0.160, Test F1-macro: 0.124\n",
      "02/083: Val Loss: 2.6745, Val F1macro: 0.1062,Test Accuracy: 0.156, Test F1-macro: 0.117\n",
      "02/084: Val Loss: 2.6727, Val F1macro: 0.1223,Test Accuracy: 0.159, Test F1-macro: 0.125\n",
      "02/085: Val Loss: 2.6711, Val F1macro: 0.0969,Test Accuracy: 0.154, Test F1-macro: 0.110\n",
      "02/086: Val Loss: 2.6762, Val F1macro: 0.0978,Test Accuracy: 0.154, Test F1-macro: 0.110\n",
      "02/087: Val Loss: 2.6680, Val F1macro: 0.1200,Test Accuracy: 0.155, Test F1-macro: 0.123\n",
      "02/088: Val Loss: 2.6660, Val F1macro: 0.1237,Test Accuracy: 0.161, Test F1-macro: 0.126\n",
      "02/089: Val Loss: 2.6690, Val F1macro: 0.1219,Test Accuracy: 0.159, Test F1-macro: 0.126\n",
      "02/090: Val Loss: 2.6899, Val F1macro: 0.1190,Test Accuracy: 0.159, Test F1-macro: 0.130\n",
      "02/091: Val Loss: 2.6646, Val F1macro: 0.1228,Test Accuracy: 0.160, Test F1-macro: 0.128\n",
      "02/092: Val Loss: 2.6734, Val F1macro: 0.1242,Test Accuracy: 0.159, Test F1-macro: 0.129\n",
      "02/093: Val Loss: 2.6646, Val F1macro: 0.1201,Test Accuracy: 0.159, Test F1-macro: 0.123\n",
      "02/094: Val Loss: 2.6610, Val F1macro: 0.1227,Test Accuracy: 0.160, Test F1-macro: 0.124\n",
      "02/095: Val Loss: 2.6700, Val F1macro: 0.1229,Test Accuracy: 0.157, Test F1-macro: 0.124\n",
      "02/096: Val Loss: 2.6683, Val F1macro: 0.1229,Test Accuracy: 0.158, Test F1-macro: 0.126\n",
      "02/097: Val Loss: 2.6694, Val F1macro: 0.1242,Test Accuracy: 0.158, Test F1-macro: 0.128\n",
      "02/098: Val Loss: 2.6682, Val F1macro: 0.1209,Test Accuracy: 0.164, Test F1-macro: 0.127\n",
      "02/099: Val Loss: 2.6671, Val F1macro: 0.1238,Test Accuracy: 0.161, Test F1-macro: 0.129\n",
      "02/100: Val Loss: 2.6803, Val F1macro: 0.1208,Test Accuracy: 0.157, Test F1-macro: 0.125\n",
      "02/101: Val Loss: 2.6659, Val F1macro: 0.1240,Test Accuracy: 0.153, Test F1-macro: 0.124\n",
      "02/102: Val Loss: 2.8395, Val F1macro: 0.0826,Test Accuracy: 0.116, Test F1-macro: 0.081\n",
      "02/103: Val Loss: 2.6637, Val F1macro: 0.1219,Test Accuracy: 0.151, Test F1-macro: 0.124\n",
      "02/104: Val Loss: 2.6636, Val F1macro: 0.1241,Test Accuracy: 0.159, Test F1-macro: 0.127\n",
      "02/105: Val Loss: 2.6639, Val F1macro: 0.1221,Test Accuracy: 0.161, Test F1-macro: 0.126\n",
      "02/106: Val Loss: 2.6632, Val F1macro: 0.1243,Test Accuracy: 0.160, Test F1-macro: 0.125\n",
      "02/107: Val Loss: 2.6642, Val F1macro: 0.1231,Test Accuracy: 0.159, Test F1-macro: 0.125\n",
      "02/108: Val Loss: 2.6678, Val F1macro: 0.1236,Test Accuracy: 0.162, Test F1-macro: 0.125\n",
      "02/109: Val Loss: 2.6622, Val F1macro: 0.1234,Test Accuracy: 0.157, Test F1-macro: 0.124\n",
      "02/110: Val Loss: 2.6632, Val F1macro: 0.1234,Test Accuracy: 0.158, Test F1-macro: 0.125\n",
      "02/111: Val Loss: 2.6645, Val F1macro: 0.1258,Test Accuracy: 0.155, Test F1-macro: 0.124\n",
      "02/112: Val Loss: 2.6620, Val F1macro: 0.1238,Test Accuracy: 0.162, Test F1-macro: 0.128\n",
      "02/113: Val Loss: 2.6686, Val F1macro: 0.1246,Test Accuracy: 0.155, Test F1-macro: 0.123\n",
      "02/114: Val Loss: 2.6663, Val F1macro: 0.1258,Test Accuracy: 0.157, Test F1-macro: 0.126\n",
      "02/115: Val Loss: 2.7595, Val F1macro: 0.0918,Test Accuracy: 0.133, Test F1-macro: 0.089\n",
      "02/116: Val Loss: 2.6715, Val F1macro: 0.1246,Test Accuracy: 0.161, Test F1-macro: 0.129\n",
      "02/117: Val Loss: 2.6638, Val F1macro: 0.1260,Test Accuracy: 0.161, Test F1-macro: 0.128\n",
      "02/118: Val Loss: 2.6628, Val F1macro: 0.1268,Test Accuracy: 0.157, Test F1-macro: 0.125\n",
      "02/119: Val Loss: 2.6650, Val F1macro: 0.1240,Test Accuracy: 0.163, Test F1-macro: 0.129\n",
      "02/120: Val Loss: 2.6648, Val F1macro: 0.1243,Test Accuracy: 0.160, Test F1-macro: 0.129\n",
      "02/121: Val Loss: 2.6743, Val F1macro: 0.1095,Test Accuracy: 0.150, Test F1-macro: 0.109\n",
      "02/122: Val Loss: 2.6874, Val F1macro: 0.1075,Test Accuracy: 0.150, Test F1-macro: 0.109\n",
      "02/123: Val Loss: 2.6677, Val F1macro: 0.1210,Test Accuracy: 0.157, Test F1-macro: 0.126\n",
      "02/124: Val Loss: 2.6803, Val F1macro: 0.1270,Test Accuracy: 0.158, Test F1-macro: 0.125\n",
      "02/125: Val Loss: 2.6641, Val F1macro: 0.1221,Test Accuracy: 0.164, Test F1-macro: 0.128\n",
      "02/126: Val Loss: 2.6636, Val F1macro: 0.1217,Test Accuracy: 0.161, Test F1-macro: 0.128\n",
      "02/127: Val Loss: 2.6676, Val F1macro: 0.1223,Test Accuracy: 0.162, Test F1-macro: 0.129\n",
      "02/128: Val Loss: 2.6660, Val F1macro: 0.1245,Test Accuracy: 0.165, Test F1-macro: 0.131\n",
      "02/129: Val Loss: 2.7782, Val F1macro: 0.0913,Test Accuracy: 0.137, Test F1-macro: 0.090\n",
      "02/130: Val Loss: 2.6639, Val F1macro: 0.1258,Test Accuracy: 0.161, Test F1-macro: 0.128\n",
      "02/131: Val Loss: 2.6649, Val F1macro: 0.1211,Test Accuracy: 0.158, Test F1-macro: 0.125\n",
      "02/132: Val Loss: 2.6625, Val F1macro: 0.1270,Test Accuracy: 0.164, Test F1-macro: 0.129\n",
      "02/133: Val Loss: 2.6632, Val F1macro: 0.1252,Test Accuracy: 0.166, Test F1-macro: 0.130\n",
      "02/134: Val Loss: 2.6681, Val F1macro: 0.1184,Test Accuracy: 0.165, Test F1-macro: 0.129\n",
      "02/135: Val Loss: 2.6665, Val F1macro: 0.1285,Test Accuracy: 0.160, Test F1-macro: 0.127\n",
      "02/136: Val Loss: 2.6652, Val F1macro: 0.1269,Test Accuracy: 0.159, Test F1-macro: 0.126\n",
      "02/137: Val Loss: 2.6634, Val F1macro: 0.1083,Test Accuracy: 0.152, Test F1-macro: 0.112\n",
      "02/138: Val Loss: 2.7008, Val F1macro: 0.1088,Test Accuracy: 0.149, Test F1-macro: 0.111\n",
      "02/139: Val Loss: 2.6700, Val F1macro: 0.1253,Test Accuracy: 0.158, Test F1-macro: 0.127\n",
      "02/140: Val Loss: 2.6717, Val F1macro: 0.1141,Test Accuracy: 0.153, Test F1-macro: 0.126\n",
      "02/141: Val Loss: 2.6847, Val F1macro: 0.1282,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/142: Val Loss: 2.6715, Val F1macro: 0.1262,Test Accuracy: 0.158, Test F1-macro: 0.127\n",
      "02/143: Val Loss: 2.6707, Val F1macro: 0.1246,Test Accuracy: 0.159, Test F1-macro: 0.126\n",
      "02/144: Val Loss: 2.6668, Val F1macro: 0.1264,Test Accuracy: 0.157, Test F1-macro: 0.127\n",
      "02/145: Val Loss: 2.7370, Val F1macro: 0.0852,Test Accuracy: 0.143, Test F1-macro: 0.092\n",
      "02/146: Val Loss: 2.7222, Val F1macro: 0.0898,Test Accuracy: 0.141, Test F1-macro: 0.089\n",
      "02/147: Val Loss: 2.6614, Val F1macro: 0.1282,Test Accuracy: 0.158, Test F1-macro: 0.125\n",
      "02/148: Val Loss: 2.6905, Val F1macro: 0.1082,Test Accuracy: 0.150, Test F1-macro: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/149: Val Loss: 2.6645, Val F1macro: 0.1219,Test Accuracy: 0.158, Test F1-macro: 0.126\n",
      "02/150: Val Loss: 2.6585, Val F1macro: 0.1251,Test Accuracy: 0.161, Test F1-macro: 0.129\n",
      "02/151: Val Loss: 2.6865, Val F1macro: 0.1094,Test Accuracy: 0.149, Test F1-macro: 0.110\n",
      "02/152: Val Loss: 2.6713, Val F1macro: 0.1071,Test Accuracy: 0.149, Test F1-macro: 0.106\n",
      "02/153: Val Loss: 2.6885, Val F1macro: 0.1088,Test Accuracy: 0.150, Test F1-macro: 0.110\n",
      "02/154: Val Loss: 2.6676, Val F1macro: 0.1249,Test Accuracy: 0.158, Test F1-macro: 0.126\n",
      "02/155: Val Loss: 2.6778, Val F1macro: 0.1096,Test Accuracy: 0.151, Test F1-macro: 0.116\n",
      "02/156: Val Loss: 2.6726, Val F1macro: 0.1203,Test Accuracy: 0.157, Test F1-macro: 0.128\n",
      "02/157: Val Loss: 2.6635, Val F1macro: 0.1245,Test Accuracy: 0.158, Test F1-macro: 0.128\n",
      "02/158: Val Loss: 2.6648, Val F1macro: 0.1252,Test Accuracy: 0.160, Test F1-macro: 0.129\n",
      "02/159: Val Loss: 2.6636, Val F1macro: 0.1233,Test Accuracy: 0.157, Test F1-macro: 0.126\n",
      "02/160: Val Loss: 2.6674, Val F1macro: 0.1206,Test Accuracy: 0.158, Test F1-macro: 0.126\n",
      "02/161: Val Loss: 2.6683, Val F1macro: 0.1240,Test Accuracy: 0.160, Test F1-macro: 0.127\n",
      "02/162: Val Loss: 2.6591, Val F1macro: 0.1289,Test Accuracy: 0.158, Test F1-macro: 0.128\n",
      "02/163: Val Loss: 2.7169, Val F1macro: 0.1119,Test Accuracy: 0.151, Test F1-macro: 0.112\n",
      "02/164: Val Loss: 2.6693, Val F1macro: 0.1263,Test Accuracy: 0.161, Test F1-macro: 0.127\n",
      "02/165: Val Loss: 2.6555, Val F1macro: 0.1285,Test Accuracy: 0.162, Test F1-macro: 0.128\n",
      "02/166: Val Loss: 2.6822, Val F1macro: 0.1204,Test Accuracy: 0.158, Test F1-macro: 0.129\n",
      "02/167: Val Loss: 2.6548, Val F1macro: 0.1297,Test Accuracy: 0.161, Test F1-macro: 0.128\n",
      "02/168: Val Loss: 2.6547, Val F1macro: 0.1295,Test Accuracy: 0.161, Test F1-macro: 0.127\n",
      "02/169: Val Loss: 2.6833, Val F1macro: 0.1189,Test Accuracy: 0.157, Test F1-macro: 0.131\n",
      "02/170: Val Loss: 2.7702, Val F1macro: 0.0984,Test Accuracy: 0.133, Test F1-macro: 0.091\n",
      "02/171: Val Loss: 2.6834, Val F1macro: 0.1059,Test Accuracy: 0.153, Test F1-macro: 0.107\n",
      "02/172: Val Loss: 2.6773, Val F1macro: 0.1085,Test Accuracy: 0.153, Test F1-macro: 0.109\n",
      "02/173: Val Loss: 2.6586, Val F1macro: 0.1272,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/174: Val Loss: 2.6643, Val F1macro: 0.1238,Test Accuracy: 0.158, Test F1-macro: 0.129\n",
      "02/175: Val Loss: 2.6596, Val F1macro: 0.1267,Test Accuracy: 0.162, Test F1-macro: 0.130\n",
      "02/176: Val Loss: 2.7179, Val F1macro: 0.1095,Test Accuracy: 0.147, Test F1-macro: 0.112\n",
      "02/177: Val Loss: 2.6615, Val F1macro: 0.1260,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/178: Val Loss: 2.6873, Val F1macro: 0.1053,Test Accuracy: 0.151, Test F1-macro: 0.109\n",
      "02/179: Val Loss: 2.6659, Val F1macro: 0.1254,Test Accuracy: 0.158, Test F1-macro: 0.129\n",
      "02/180: Val Loss: 2.6817, Val F1macro: 0.1100,Test Accuracy: 0.148, Test F1-macro: 0.112\n",
      "02/181: Val Loss: 2.6592, Val F1macro: 0.1315,Test Accuracy: 0.158, Test F1-macro: 0.130\n",
      "02/182: Val Loss: 2.6577, Val F1macro: 0.1239,Test Accuracy: 0.165, Test F1-macro: 0.130\n",
      "02/183: Val Loss: 2.6651, Val F1macro: 0.1182,Test Accuracy: 0.157, Test F1-macro: 0.132\n",
      "02/184: Val Loss: 2.6550, Val F1macro: 0.1264,Test Accuracy: 0.165, Test F1-macro: 0.132\n",
      "02/185: Val Loss: 2.6638, Val F1macro: 0.1113,Test Accuracy: 0.153, Test F1-macro: 0.118\n",
      "02/186: Val Loss: 2.6568, Val F1macro: 0.1101,Test Accuracy: 0.156, Test F1-macro: 0.117\n",
      "02/187: Val Loss: 2.6594, Val F1macro: 0.1224,Test Accuracy: 0.162, Test F1-macro: 0.134\n",
      "02/188: Val Loss: 2.6663, Val F1macro: 0.1073,Test Accuracy: 0.155, Test F1-macro: 0.116\n",
      "02/189: Val Loss: 2.6663, Val F1macro: 0.1133,Test Accuracy: 0.158, Test F1-macro: 0.130\n",
      "02/190: Val Loss: 2.6605, Val F1macro: 0.1262,Test Accuracy: 0.164, Test F1-macro: 0.130\n",
      "02/191: Val Loss: 2.6653, Val F1macro: 0.1207,Test Accuracy: 0.164, Test F1-macro: 0.133\n",
      "02/192: Val Loss: 2.6638, Val F1macro: 0.1125,Test Accuracy: 0.158, Test F1-macro: 0.130\n",
      "02/193: Val Loss: 2.6604, Val F1macro: 0.1213,Test Accuracy: 0.161, Test F1-macro: 0.129\n",
      "02/194: Val Loss: 2.6552, Val F1macro: 0.1255,Test Accuracy: 0.166, Test F1-macro: 0.133\n",
      "02/195: Val Loss: 2.6678, Val F1macro: 0.1243,Test Accuracy: 0.162, Test F1-macro: 0.133\n",
      "02/196: Val Loss: 2.6606, Val F1macro: 0.1262,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/197: Val Loss: 2.6615, Val F1macro: 0.1263,Test Accuracy: 0.161, Test F1-macro: 0.130\n",
      "02/198: Val Loss: 2.6539, Val F1macro: 0.1257,Test Accuracy: 0.162, Test F1-macro: 0.131\n",
      "02/199: Val Loss: 2.6967, Val F1macro: 0.1085,Test Accuracy: 0.150, Test F1-macro: 0.116\n",
      "02/200: Val Loss: 2.6616, Val F1macro: 0.1188,Test Accuracy: 0.165, Test F1-macro: 0.132\n",
      "02/201: Val Loss: 2.6568, Val F1macro: 0.1089,Test Accuracy: 0.159, Test F1-macro: 0.116\n",
      "02/202: Val Loss: 2.6613, Val F1macro: 0.1115,Test Accuracy: 0.159, Test F1-macro: 0.119\n",
      "02/203: Val Loss: 2.6715, Val F1macro: 0.1102,Test Accuracy: 0.151, Test F1-macro: 0.112\n",
      "02/204: Val Loss: 2.6604, Val F1macro: 0.1177,Test Accuracy: 0.165, Test F1-macro: 0.134\n",
      "02/205: Val Loss: 2.6595, Val F1macro: 0.1247,Test Accuracy: 0.165, Test F1-macro: 0.130\n",
      "02/206: Val Loss: 2.6577, Val F1macro: 0.1293,Test Accuracy: 0.166, Test F1-macro: 0.135\n",
      "02/207: Val Loss: 2.6617, Val F1macro: 0.1311,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/208: Val Loss: 2.6579, Val F1macro: 0.1233,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/209: Val Loss: 2.6701, Val F1macro: 0.1228,Test Accuracy: 0.162, Test F1-macro: 0.130\n",
      "02/210: Val Loss: 2.6585, Val F1macro: 0.1253,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/211: Val Loss: 2.6759, Val F1macro: 0.1085,Test Accuracy: 0.150, Test F1-macro: 0.111\n",
      "02/212: Val Loss: 2.6545, Val F1macro: 0.1257,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/213: Val Loss: 2.6657, Val F1macro: 0.1099,Test Accuracy: 0.153, Test F1-macro: 0.113\n",
      "02/214: Val Loss: 2.6540, Val F1macro: 0.1288,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/215: Val Loss: 2.6567, Val F1macro: 0.1253,Test Accuracy: 0.169, Test F1-macro: 0.135\n",
      "02/216: Val Loss: 2.6562, Val F1macro: 0.1245,Test Accuracy: 0.166, Test F1-macro: 0.131\n",
      "02/217: Val Loss: 2.6574, Val F1macro: 0.1223,Test Accuracy: 0.165, Test F1-macro: 0.130\n",
      "02/218: Val Loss: 2.6609, Val F1macro: 0.1196,Test Accuracy: 0.164, Test F1-macro: 0.133\n",
      "02/219: Val Loss: 2.6654, Val F1macro: 0.1195,Test Accuracy: 0.157, Test F1-macro: 0.129\n",
      "02/220: Val Loss: 2.6563, Val F1macro: 0.1242,Test Accuracy: 0.166, Test F1-macro: 0.133\n",
      "02/221: Val Loss: 2.6623, Val F1macro: 0.1120,Test Accuracy: 0.159, Test F1-macro: 0.129\n",
      "02/222: Val Loss: 2.6580, Val F1macro: 0.1129,Test Accuracy: 0.163, Test F1-macro: 0.129\n",
      "02/223: Val Loss: 2.6546, Val F1macro: 0.1247,Test Accuracy: 0.167, Test F1-macro: 0.133\n",
      "02/224: Val Loss: 2.6720, Val F1macro: 0.1085,Test Accuracy: 0.151, Test F1-macro: 0.110\n",
      "02/225: Val Loss: 2.6811, Val F1macro: 0.1125,Test Accuracy: 0.150, Test F1-macro: 0.112\n",
      "02/226: Val Loss: 2.6570, Val F1macro: 0.1294,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/227: Val Loss: 2.6569, Val F1macro: 0.1242,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/228: Val Loss: 2.6558, Val F1macro: 0.1259,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/229: Val Loss: 2.6598, Val F1macro: 0.1273,Test Accuracy: 0.165, Test F1-macro: 0.132\n",
      "02/230: Val Loss: 2.6682, Val F1macro: 0.1122,Test Accuracy: 0.153, Test F1-macro: 0.113\n",
      "02/231: Val Loss: 2.7424, Val F1macro: 0.1163,Test Accuracy: 0.150, Test F1-macro: 0.120\n",
      "02/232: Val Loss: 2.6539, Val F1macro: 0.1269,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/233: Val Loss: 2.6660, Val F1macro: 0.1200,Test Accuracy: 0.159, Test F1-macro: 0.130\n",
      "02/234: Val Loss: 2.6645, Val F1macro: 0.1242,Test Accuracy: 0.157, Test F1-macro: 0.132\n",
      "02/235: Val Loss: 2.6583, Val F1macro: 0.1158,Test Accuracy: 0.162, Test F1-macro: 0.120\n",
      "02/236: Val Loss: 2.6560, Val F1macro: 0.1207,Test Accuracy: 0.170, Test F1-macro: 0.133\n",
      "02/237: Val Loss: 2.6583, Val F1macro: 0.1154,Test Accuracy: 0.161, Test F1-macro: 0.117\n",
      "02/238: Val Loss: 2.6526, Val F1macro: 0.1259,Test Accuracy: 0.167, Test F1-macro: 0.133\n",
      "02/239: Val Loss: 2.6557, Val F1macro: 0.1277,Test Accuracy: 0.164, Test F1-macro: 0.132\n",
      "02/240: Val Loss: 2.6539, Val F1macro: 0.1244,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/241: Val Loss: 2.6929, Val F1macro: 0.1121,Test Accuracy: 0.148, Test F1-macro: 0.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/242: Val Loss: 2.6559, Val F1macro: 0.1188,Test Accuracy: 0.169, Test F1-macro: 0.131\n",
      "02/243: Val Loss: 2.6558, Val F1macro: 0.1275,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/244: Val Loss: 2.6535, Val F1macro: 0.1175,Test Accuracy: 0.162, Test F1-macro: 0.129\n",
      "02/245: Val Loss: 2.6561, Val F1macro: 0.1227,Test Accuracy: 0.169, Test F1-macro: 0.136\n",
      "02/246: Val Loss: 2.6544, Val F1macro: 0.1254,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/247: Val Loss: 2.6784, Val F1macro: 0.1114,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/248: Val Loss: 2.6572, Val F1macro: 0.1262,Test Accuracy: 0.162, Test F1-macro: 0.130\n",
      "02/249: Val Loss: 2.6597, Val F1macro: 0.1192,Test Accuracy: 0.167, Test F1-macro: 0.132\n",
      "02/250: Val Loss: 2.6604, Val F1macro: 0.1245,Test Accuracy: 0.162, Test F1-macro: 0.134\n",
      "02/251: Val Loss: 2.6570, Val F1macro: 0.1296,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/252: Val Loss: 2.6624, Val F1macro: 0.1185,Test Accuracy: 0.166, Test F1-macro: 0.131\n",
      "02/253: Val Loss: 2.6628, Val F1macro: 0.1232,Test Accuracy: 0.167, Test F1-macro: 0.135\n",
      "02/254: Val Loss: 2.6609, Val F1macro: 0.1298,Test Accuracy: 0.170, Test F1-macro: 0.138\n",
      "02/255: Val Loss: 2.6585, Val F1macro: 0.1241,Test Accuracy: 0.169, Test F1-macro: 0.134\n",
      "02/256: Val Loss: 2.6564, Val F1macro: 0.1260,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/257: Val Loss: 2.6643, Val F1macro: 0.1262,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/258: Val Loss: 2.6612, Val F1macro: 0.1245,Test Accuracy: 0.161, Test F1-macro: 0.132\n",
      "02/259: Val Loss: 2.6587, Val F1macro: 0.1220,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/260: Val Loss: 2.6760, Val F1macro: 0.1093,Test Accuracy: 0.150, Test F1-macro: 0.111\n",
      "02/261: Val Loss: 2.6563, Val F1macro: 0.1247,Test Accuracy: 0.169, Test F1-macro: 0.133\n",
      "02/262: Val Loss: 2.6611, Val F1macro: 0.1248,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/263: Val Loss: 2.6592, Val F1macro: 0.1206,Test Accuracy: 0.166, Test F1-macro: 0.133\n",
      "02/264: Val Loss: 2.6611, Val F1macro: 0.1249,Test Accuracy: 0.164, Test F1-macro: 0.131\n",
      "02/265: Val Loss: 2.6599, Val F1macro: 0.1243,Test Accuracy: 0.167, Test F1-macro: 0.132\n",
      "02/266: Val Loss: 2.7028, Val F1macro: 0.1248,Test Accuracy: 0.166, Test F1-macro: 0.129\n",
      "02/267: Val Loss: 2.6585, Val F1macro: 0.1228,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/268: Val Loss: 2.6718, Val F1macro: 0.1093,Test Accuracy: 0.150, Test F1-macro: 0.110\n",
      "02/269: Val Loss: 2.6604, Val F1macro: 0.1242,Test Accuracy: 0.163, Test F1-macro: 0.134\n",
      "02/270: Val Loss: 2.6585, Val F1macro: 0.1221,Test Accuracy: 0.163, Test F1-macro: 0.134\n",
      "02/271: Val Loss: 2.6578, Val F1macro: 0.1241,Test Accuracy: 0.161, Test F1-macro: 0.130\n",
      "02/272: Val Loss: 2.6599, Val F1macro: 0.1228,Test Accuracy: 0.160, Test F1-macro: 0.131\n",
      "02/273: Val Loss: 2.6581, Val F1macro: 0.1268,Test Accuracy: 0.165, Test F1-macro: 0.134\n",
      "02/274: Val Loss: 2.6616, Val F1macro: 0.1225,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/275: Val Loss: 2.6538, Val F1macro: 0.1279,Test Accuracy: 0.165, Test F1-macro: 0.134\n",
      "02/276: Val Loss: 2.6665, Val F1macro: 0.1113,Test Accuracy: 0.150, Test F1-macro: 0.112\n",
      "02/277: Val Loss: 2.6547, Val F1macro: 0.1256,Test Accuracy: 0.164, Test F1-macro: 0.131\n",
      "02/278: Val Loss: 2.6573, Val F1macro: 0.1224,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/279: Val Loss: 2.6518, Val F1macro: 0.1272,Test Accuracy: 0.169, Test F1-macro: 0.135\n",
      "02/280: Val Loss: 2.6694, Val F1macro: 0.1096,Test Accuracy: 0.151, Test F1-macro: 0.109\n",
      "02/281: Val Loss: 2.6619, Val F1macro: 0.1260,Test Accuracy: 0.164, Test F1-macro: 0.131\n",
      "02/282: Val Loss: 2.6596, Val F1macro: 0.1270,Test Accuracy: 0.159, Test F1-macro: 0.131\n",
      "02/283: Val Loss: 2.6782, Val F1macro: 0.1075,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/284: Val Loss: 2.6644, Val F1macro: 0.1235,Test Accuracy: 0.158, Test F1-macro: 0.132\n",
      "02/285: Val Loss: 2.6578, Val F1macro: 0.1268,Test Accuracy: 0.160, Test F1-macro: 0.131\n",
      "02/286: Val Loss: 3.2994, Val F1macro: 0.1237,Test Accuracy: 0.163, Test F1-macro: 0.132\n",
      "02/287: Val Loss: 2.7437, Val F1macro: 0.0935,Test Accuracy: 0.137, Test F1-macro: 0.094\n",
      "02/288: Val Loss: 2.6643, Val F1macro: 0.1215,Test Accuracy: 0.155, Test F1-macro: 0.126\n",
      "02/289: Val Loss: 2.7484, Val F1macro: 0.1145,Test Accuracy: 0.148, Test F1-macro: 0.117\n",
      "02/290: Val Loss: 2.7020, Val F1macro: 0.1116,Test Accuracy: 0.152, Test F1-macro: 0.113\n",
      "02/291: Val Loss: 2.7123, Val F1macro: 0.1130,Test Accuracy: 0.148, Test F1-macro: 0.113\n",
      "02/292: Val Loss: 2.6609, Val F1macro: 0.1102,Test Accuracy: 0.158, Test F1-macro: 0.113\n",
      "02/293: Val Loss: 2.6680, Val F1macro: 0.1088,Test Accuracy: 0.152, Test F1-macro: 0.110\n",
      "02/294: Val Loss: 2.6689, Val F1macro: 0.1124,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/295: Val Loss: 2.6597, Val F1macro: 0.1151,Test Accuracy: 0.156, Test F1-macro: 0.117\n",
      "02/296: Val Loss: 2.7296, Val F1macro: 0.1137,Test Accuracy: 0.148, Test F1-macro: 0.117\n",
      "02/297: Val Loss: 2.6605, Val F1macro: 0.1156,Test Accuracy: 0.161, Test F1-macro: 0.129\n",
      "02/298: Val Loss: 2.6735, Val F1macro: 0.1106,Test Accuracy: 0.154, Test F1-macro: 0.112\n",
      "02/299: Val Loss: 2.6603, Val F1macro: 0.1152,Test Accuracy: 0.160, Test F1-macro: 0.128\n",
      "02/300: Val Loss: 2.7424, Val F1macro: 0.0927,Test Accuracy: 0.139, Test F1-macro: 0.098\n",
      "02/301: Val Loss: 2.6656, Val F1macro: 0.1166,Test Accuracy: 0.161, Test F1-macro: 0.130\n",
      "02/302: Val Loss: 2.7273, Val F1macro: 0.1135,Test Accuracy: 0.149, Test F1-macro: 0.113\n",
      "02/303: Val Loss: 2.6589, Val F1macro: 0.1286,Test Accuracy: 0.160, Test F1-macro: 0.132\n",
      "02/304: Val Loss: 2.6624, Val F1macro: 0.1098,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/305: Val Loss: 2.7416, Val F1macro: 0.0919,Test Accuracy: 0.137, Test F1-macro: 0.093\n",
      "02/306: Val Loss: 2.6597, Val F1macro: 0.1079,Test Accuracy: 0.157, Test F1-macro: 0.109\n",
      "02/307: Val Loss: 2.6813, Val F1macro: 0.1119,Test Accuracy: 0.149, Test F1-macro: 0.111\n",
      "02/308: Val Loss: 2.7494, Val F1macro: 0.0958,Test Accuracy: 0.137, Test F1-macro: 0.093\n",
      "02/309: Val Loss: 2.6599, Val F1macro: 0.1242,Test Accuracy: 0.163, Test F1-macro: 0.132\n",
      "02/310: Val Loss: 2.6664, Val F1macro: 0.1133,Test Accuracy: 0.155, Test F1-macro: 0.115\n",
      "02/311: Val Loss: 2.6731, Val F1macro: 0.1139,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/312: Val Loss: 2.6594, Val F1macro: 0.1233,Test Accuracy: 0.164, Test F1-macro: 0.132\n",
      "02/313: Val Loss: 2.6547, Val F1macro: 0.1237,Test Accuracy: 0.163, Test F1-macro: 0.132\n",
      "02/314: Val Loss: 2.6598, Val F1macro: 0.1266,Test Accuracy: 0.164, Test F1-macro: 0.132\n",
      "02/315: Val Loss: 2.6583, Val F1macro: 0.1220,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/316: Val Loss: 2.6614, Val F1macro: 0.1104,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/317: Val Loss: 2.6529, Val F1macro: 0.1118,Test Accuracy: 0.162, Test F1-macro: 0.113\n",
      "02/318: Val Loss: 2.6610, Val F1macro: 0.1130,Test Accuracy: 0.162, Test F1-macro: 0.115\n",
      "02/319: Val Loss: 2.6655, Val F1macro: 0.1134,Test Accuracy: 0.159, Test F1-macro: 0.114\n",
      "02/320: Val Loss: 2.6622, Val F1macro: 0.1110,Test Accuracy: 0.158, Test F1-macro: 0.113\n",
      "02/321: Val Loss: 2.6588, Val F1macro: 0.1072,Test Accuracy: 0.158, Test F1-macro: 0.113\n",
      "02/322: Val Loss: 2.6610, Val F1macro: 0.1107,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/323: Val Loss: 2.7078, Val F1macro: 0.1087,Test Accuracy: 0.160, Test F1-macro: 0.115\n",
      "02/324: Val Loss: 2.6702, Val F1macro: 0.1159,Test Accuracy: 0.161, Test F1-macro: 0.115\n",
      "02/325: Val Loss: 2.6624, Val F1macro: 0.1076,Test Accuracy: 0.159, Test F1-macro: 0.111\n",
      "02/326: Val Loss: 2.6560, Val F1macro: 0.1103,Test Accuracy: 0.162, Test F1-macro: 0.114\n",
      "02/327: Val Loss: 2.6565, Val F1macro: 0.1096,Test Accuracy: 0.162, Test F1-macro: 0.113\n",
      "02/328: Val Loss: 2.6608, Val F1macro: 0.1128,Test Accuracy: 0.162, Test F1-macro: 0.116\n",
      "02/329: Val Loss: 2.6575, Val F1macro: 0.1104,Test Accuracy: 0.161, Test F1-macro: 0.114\n",
      "02/330: Val Loss: 2.6534, Val F1macro: 0.1133,Test Accuracy: 0.162, Test F1-macro: 0.117\n",
      "02/331: Val Loss: 2.6845, Val F1macro: 0.1089,Test Accuracy: 0.162, Test F1-macro: 0.113\n",
      "02/332: Val Loss: 2.6602, Val F1macro: 0.1089,Test Accuracy: 0.152, Test F1-macro: 0.111\n",
      "02/333: Val Loss: 2.6589, Val F1macro: 0.1138,Test Accuracy: 0.162, Test F1-macro: 0.114\n",
      "02/334: Val Loss: 2.6572, Val F1macro: 0.1091,Test Accuracy: 0.162, Test F1-macro: 0.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/335: Val Loss: 2.6922, Val F1macro: 0.1132,Test Accuracy: 0.149, Test F1-macro: 0.112\n",
      "02/336: Val Loss: 2.6549, Val F1macro: 0.1185,Test Accuracy: 0.166, Test F1-macro: 0.132\n",
      "02/337: Val Loss: 2.6615, Val F1macro: 0.1101,Test Accuracy: 0.162, Test F1-macro: 0.113\n",
      "02/338: Val Loss: 2.7184, Val F1macro: 0.1111,Test Accuracy: 0.155, Test F1-macro: 0.119\n",
      "02/339: Val Loss: 2.7244, Val F1macro: 0.1169,Test Accuracy: 0.153, Test F1-macro: 0.118\n",
      "02/340: Val Loss: 2.6529, Val F1macro: 0.1248,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/341: Val Loss: 2.6599, Val F1macro: 0.1145,Test Accuracy: 0.164, Test F1-macro: 0.115\n",
      "02/342: Val Loss: 2.6550, Val F1macro: 0.1132,Test Accuracy: 0.161, Test F1-macro: 0.114\n",
      "02/343: Val Loss: 2.6595, Val F1macro: 0.1231,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/344: Val Loss: 2.6564, Val F1macro: 0.1153,Test Accuracy: 0.162, Test F1-macro: 0.115\n",
      "02/345: Val Loss: 2.6601, Val F1macro: 0.1213,Test Accuracy: 0.168, Test F1-macro: 0.135\n",
      "02/346: Val Loss: 2.6643, Val F1macro: 0.1232,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/347: Val Loss: 2.6595, Val F1macro: 0.1233,Test Accuracy: 0.168, Test F1-macro: 0.134\n",
      "02/348: Val Loss: 2.6622, Val F1macro: 0.1242,Test Accuracy: 0.159, Test F1-macro: 0.131\n",
      "02/349: Val Loss: 2.6574, Val F1macro: 0.1224,Test Accuracy: 0.165, Test F1-macro: 0.134\n",
      "02/350: Val Loss: 2.6615, Val F1macro: 0.1138,Test Accuracy: 0.155, Test F1-macro: 0.114\n",
      "02/351: Val Loss: 2.6693, Val F1macro: 0.1161,Test Accuracy: 0.164, Test F1-macro: 0.120\n",
      "02/352: Val Loss: 2.6616, Val F1macro: 0.1152,Test Accuracy: 0.163, Test F1-macro: 0.115\n",
      "02/353: Val Loss: 2.6606, Val F1macro: 0.1137,Test Accuracy: 0.156, Test F1-macro: 0.115\n",
      "02/354: Val Loss: 2.6591, Val F1macro: 0.1129,Test Accuracy: 0.157, Test F1-macro: 0.115\n",
      "02/355: Val Loss: 2.6558, Val F1macro: 0.1117,Test Accuracy: 0.161, Test F1-macro: 0.115\n",
      "02/356: Val Loss: 2.6583, Val F1macro: 0.1100,Test Accuracy: 0.154, Test F1-macro: 0.110\n",
      "02/357: Val Loss: 2.6568, Val F1macro: 0.1137,Test Accuracy: 0.156, Test F1-macro: 0.112\n",
      "02/358: Val Loss: 2.6535, Val F1macro: 0.1171,Test Accuracy: 0.168, Test F1-macro: 0.133\n",
      "02/359: Val Loss: 2.6591, Val F1macro: 0.1128,Test Accuracy: 0.160, Test F1-macro: 0.112\n",
      "02/360: Val Loss: 2.6563, Val F1macro: 0.1191,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/361: Val Loss: 2.6567, Val F1macro: 0.1090,Test Accuracy: 0.166, Test F1-macro: 0.115\n",
      "02/362: Val Loss: 2.6679, Val F1macro: 0.1096,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/363: Val Loss: 2.6557, Val F1macro: 0.1148,Test Accuracy: 0.163, Test F1-macro: 0.114\n",
      "02/364: Val Loss: 2.6560, Val F1macro: 0.1143,Test Accuracy: 0.163, Test F1-macro: 0.115\n",
      "02/365: Val Loss: 2.6538, Val F1macro: 0.1099,Test Accuracy: 0.158, Test F1-macro: 0.113\n",
      "02/366: Val Loss: 2.6603, Val F1macro: 0.1078,Test Accuracy: 0.155, Test F1-macro: 0.112\n",
      "02/367: Val Loss: 2.6533, Val F1macro: 0.1138,Test Accuracy: 0.162, Test F1-macro: 0.114\n",
      "02/368: Val Loss: 2.6588, Val F1macro: 0.1121,Test Accuracy: 0.157, Test F1-macro: 0.113\n",
      "02/369: Val Loss: 2.6586, Val F1macro: 0.1129,Test Accuracy: 0.159, Test F1-macro: 0.113\n",
      "02/370: Val Loss: 2.6657, Val F1macro: 0.1109,Test Accuracy: 0.152, Test F1-macro: 0.112\n",
      "02/371: Val Loss: 2.6572, Val F1macro: 0.1099,Test Accuracy: 0.158, Test F1-macro: 0.111\n",
      "02/372: Val Loss: 2.6531, Val F1macro: 0.1230,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/373: Val Loss: 2.6572, Val F1macro: 0.1157,Test Accuracy: 0.158, Test F1-macro: 0.120\n",
      "02/374: Val Loss: 2.6665, Val F1macro: 0.1118,Test Accuracy: 0.153, Test F1-macro: 0.111\n",
      "02/375: Val Loss: 2.6562, Val F1macro: 0.1090,Test Accuracy: 0.164, Test F1-macro: 0.113\n",
      "02/376: Val Loss: 3.2118, Val F1macro: 0.1232,Test Accuracy: 0.160, Test F1-macro: 0.123\n",
      "02/377: Val Loss: 2.6548, Val F1macro: 0.1096,Test Accuracy: 0.161, Test F1-macro: 0.113\n",
      "02/378: Val Loss: 2.6598, Val F1macro: 0.1130,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/379: Val Loss: 2.6673, Val F1macro: 0.1166,Test Accuracy: 0.169, Test F1-macro: 0.133\n",
      "02/380: Val Loss: 2.6569, Val F1macro: 0.1085,Test Accuracy: 0.157, Test F1-macro: 0.113\n",
      "02/381: Val Loss: 2.6527, Val F1macro: 0.1245,Test Accuracy: 0.166, Test F1-macro: 0.133\n",
      "02/382: Val Loss: 2.6596, Val F1macro: 0.1114,Test Accuracy: 0.156, Test F1-macro: 0.113\n",
      "02/383: Val Loss: 2.6581, Val F1macro: 0.1082,Test Accuracy: 0.155, Test F1-macro: 0.112\n",
      "02/384: Val Loss: 2.6560, Val F1macro: 0.1135,Test Accuracy: 0.162, Test F1-macro: 0.115\n",
      "02/385: Val Loss: 2.6578, Val F1macro: 0.1120,Test Accuracy: 0.161, Test F1-macro: 0.113\n",
      "02/386: Val Loss: 2.6810, Val F1macro: 0.1136,Test Accuracy: 0.152, Test F1-macro: 0.114\n",
      "02/387: Val Loss: 2.6550, Val F1macro: 0.1154,Test Accuracy: 0.163, Test F1-macro: 0.114\n",
      "02/388: Val Loss: 2.8729, Val F1macro: 0.1248,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/389: Val Loss: 2.6985, Val F1macro: 0.1136,Test Accuracy: 0.152, Test F1-macro: 0.115\n",
      "02/390: Val Loss: 2.6565, Val F1macro: 0.1280,Test Accuracy: 0.164, Test F1-macro: 0.135\n",
      "02/391: Val Loss: 2.6699, Val F1macro: 0.1137,Test Accuracy: 0.154, Test F1-macro: 0.113\n",
      "02/392: Val Loss: 2.6553, Val F1macro: 0.1223,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/393: Val Loss: 2.6659, Val F1macro: 0.1235,Test Accuracy: 0.159, Test F1-macro: 0.131\n",
      "02/394: Val Loss: 2.6711, Val F1macro: 0.1091,Test Accuracy: 0.152, Test F1-macro: 0.108\n",
      "02/395: Val Loss: 2.6576, Val F1macro: 0.1284,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/396: Val Loss: 2.6708, Val F1macro: 0.1117,Test Accuracy: 0.154, Test F1-macro: 0.114\n",
      "02/397: Val Loss: 2.6549, Val F1macro: 0.1247,Test Accuracy: 0.167, Test F1-macro: 0.134\n",
      "02/398: Val Loss: 2.6645, Val F1macro: 0.1117,Test Accuracy: 0.155, Test F1-macro: 0.118\n",
      "02/399: Val Loss: 2.6605, Val F1macro: 0.1178,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/400: Val Loss: 2.6582, Val F1macro: 0.1199,Test Accuracy: 0.163, Test F1-macro: 0.133\n",
      "02/401: Val Loss: 2.6750, Val F1macro: 0.1079,Test Accuracy: 0.153, Test F1-macro: 0.114\n",
      "02/402: Val Loss: 2.6530, Val F1macro: 0.1265,Test Accuracy: 0.163, Test F1-macro: 0.133\n",
      "02/403: Val Loss: 2.6599, Val F1macro: 0.1162,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/404: Val Loss: 2.6636, Val F1macro: 0.1226,Test Accuracy: 0.161, Test F1-macro: 0.135\n",
      "02/405: Val Loss: 2.6704, Val F1macro: 0.1130,Test Accuracy: 0.156, Test F1-macro: 0.113\n",
      "02/406: Val Loss: 2.6676, Val F1macro: 0.1081,Test Accuracy: 0.156, Test F1-macro: 0.112\n",
      "02/407: Val Loss: 2.6681, Val F1macro: 0.1184,Test Accuracy: 0.164, Test F1-macro: 0.133\n",
      "02/408: Val Loss: 2.6598, Val F1macro: 0.1293,Test Accuracy: 0.164, Test F1-macro: 0.135\n",
      "02/409: Val Loss: 2.6559, Val F1macro: 0.1182,Test Accuracy: 0.164, Test F1-macro: 0.132\n",
      "02/410: Val Loss: 2.6593, Val F1macro: 0.1228,Test Accuracy: 0.164, Test F1-macro: 0.132\n",
      "02/411: Val Loss: 2.6588, Val F1macro: 0.1216,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/412: Val Loss: 2.6770, Val F1macro: 0.1091,Test Accuracy: 0.152, Test F1-macro: 0.110\n",
      "02/413: Val Loss: 2.6812, Val F1macro: 0.1135,Test Accuracy: 0.151, Test F1-macro: 0.111\n",
      "02/414: Val Loss: 2.6557, Val F1macro: 0.1241,Test Accuracy: 0.165, Test F1-macro: 0.134\n",
      "02/415: Val Loss: 2.6598, Val F1macro: 0.1207,Test Accuracy: 0.158, Test F1-macro: 0.130\n",
      "02/416: Val Loss: 2.6559, Val F1macro: 0.1258,Test Accuracy: 0.163, Test F1-macro: 0.134\n",
      "02/417: Val Loss: 2.6660, Val F1macro: 0.1143,Test Accuracy: 0.160, Test F1-macro: 0.125\n",
      "02/418: Val Loss: 2.6637, Val F1macro: 0.1097,Test Accuracy: 0.158, Test F1-macro: 0.112\n",
      "02/419: Val Loss: 2.6781, Val F1macro: 0.1124,Test Accuracy: 0.152, Test F1-macro: 0.111\n",
      "02/420: Val Loss: 2.6693, Val F1macro: 0.1187,Test Accuracy: 0.163, Test F1-macro: 0.132\n",
      "02/421: Val Loss: 2.6926, Val F1macro: 0.1094,Test Accuracy: 0.152, Test F1-macro: 0.111\n",
      "02/422: Val Loss: 2.6754, Val F1macro: 0.1087,Test Accuracy: 0.157, Test F1-macro: 0.113\n",
      "02/423: Val Loss: 2.6611, Val F1macro: 0.1249,Test Accuracy: 0.163, Test F1-macro: 0.132\n",
      "02/424: Val Loss: 2.6568, Val F1macro: 0.1223,Test Accuracy: 0.162, Test F1-macro: 0.133\n",
      "02/425: Val Loss: 2.6806, Val F1macro: 0.1111,Test Accuracy: 0.154, Test F1-macro: 0.115\n",
      "02/426: Val Loss: 2.6722, Val F1macro: 0.1142,Test Accuracy: 0.152, Test F1-macro: 0.112\n",
      "02/427: Val Loss: 2.6599, Val F1macro: 0.1164,Test Accuracy: 0.160, Test F1-macro: 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/428: Val Loss: 2.6550, Val F1macro: 0.1125,Test Accuracy: 0.162, Test F1-macro: 0.115\n",
      "02/429: Val Loss: 2.6575, Val F1macro: 0.1092,Test Accuracy: 0.157, Test F1-macro: 0.116\n",
      "02/430: Val Loss: 2.6593, Val F1macro: 0.1065,Test Accuracy: 0.155, Test F1-macro: 0.114\n",
      "02/431: Val Loss: 2.6798, Val F1macro: 0.1104,Test Accuracy: 0.153, Test F1-macro: 0.112\n",
      "02/432: Val Loss: 2.6591, Val F1macro: 0.1140,Test Accuracy: 0.158, Test F1-macro: 0.116\n",
      "02/433: Val Loss: 2.6555, Val F1macro: 0.1251,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/434: Val Loss: 2.7191, Val F1macro: 0.1141,Test Accuracy: 0.151, Test F1-macro: 0.116\n",
      "02/435: Val Loss: 2.6548, Val F1macro: 0.1220,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/436: Val Loss: 2.6673, Val F1macro: 0.1090,Test Accuracy: 0.156, Test F1-macro: 0.112\n",
      "02/437: Val Loss: 2.6664, Val F1macro: 0.1199,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/438: Val Loss: 2.6709, Val F1macro: 0.1107,Test Accuracy: 0.154, Test F1-macro: 0.112\n",
      "02/439: Val Loss: 2.6582, Val F1macro: 0.1212,Test Accuracy: 0.162, Test F1-macro: 0.133\n",
      "02/440: Val Loss: 2.6650, Val F1macro: 0.1211,Test Accuracy: 0.160, Test F1-macro: 0.130\n",
      "02/441: Val Loss: 2.6719, Val F1macro: 0.1110,Test Accuracy: 0.151, Test F1-macro: 0.110\n",
      "02/442: Val Loss: 2.6918, Val F1macro: 0.1123,Test Accuracy: 0.150, Test F1-macro: 0.111\n",
      "02/443: Val Loss: 2.6555, Val F1macro: 0.1243,Test Accuracy: 0.162, Test F1-macro: 0.132\n",
      "02/444: Val Loss: 2.6597, Val F1macro: 0.1139,Test Accuracy: 0.159, Test F1-macro: 0.113\n",
      "02/445: Val Loss: 2.6694, Val F1macro: 0.1142,Test Accuracy: 0.154, Test F1-macro: 0.114\n",
      "02/446: Val Loss: 2.6825, Val F1macro: 0.1129,Test Accuracy: 0.154, Test F1-macro: 0.112\n",
      "02/447: Val Loss: 2.6611, Val F1macro: 0.1144,Test Accuracy: 0.156, Test F1-macro: 0.118\n",
      "02/448: Val Loss: 2.6623, Val F1macro: 0.1097,Test Accuracy: 0.156, Test F1-macro: 0.111\n",
      "02/449: Val Loss: 2.6782, Val F1macro: 0.1201,Test Accuracy: 0.163, Test F1-macro: 0.133\n",
      "02/450: Val Loss: 2.6581, Val F1macro: 0.1223,Test Accuracy: 0.164, Test F1-macro: 0.130\n",
      "02/451: Val Loss: 2.6518, Val F1macro: 0.1318,Test Accuracy: 0.163, Test F1-macro: 0.134\n",
      "02/452: Val Loss: 2.6908, Val F1macro: 0.1124,Test Accuracy: 0.150, Test F1-macro: 0.111\n",
      "02/453: Val Loss: 2.6729, Val F1macro: 0.1126,Test Accuracy: 0.156, Test F1-macro: 0.116\n",
      "02/454: Val Loss: 2.6618, Val F1macro: 0.1257,Test Accuracy: 0.162, Test F1-macro: 0.135\n",
      "02/455: Val Loss: 2.6682, Val F1macro: 0.1123,Test Accuracy: 0.153, Test F1-macro: 0.111\n",
      "02/456: Val Loss: 2.6536, Val F1macro: 0.1304,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/457: Val Loss: 2.6872, Val F1macro: 0.1103,Test Accuracy: 0.151, Test F1-macro: 0.112\n",
      "02/458: Val Loss: 2.6547, Val F1macro: 0.1283,Test Accuracy: 0.165, Test F1-macro: 0.133\n",
      "02/459: Val Loss: 2.6587, Val F1macro: 0.1203,Test Accuracy: 0.162, Test F1-macro: 0.131\n",
      "02/460: Val Loss: 2.6819, Val F1macro: 0.1119,Test Accuracy: 0.153, Test F1-macro: 0.112\n",
      "02/461: Val Loss: 2.7134, Val F1macro: 0.1124,Test Accuracy: 0.150, Test F1-macro: 0.114\n",
      "02/462: Val Loss: 2.6875, Val F1macro: 0.1131,Test Accuracy: 0.154, Test F1-macro: 0.112\n",
      "02/463: Val Loss: 2.6584, Val F1macro: 0.1261,Test Accuracy: 0.161, Test F1-macro: 0.132\n",
      "02/464: Val Loss: 2.6622, Val F1macro: 0.1097,Test Accuracy: 0.156, Test F1-macro: 0.113\n",
      "02/465: Val Loss: 2.6610, Val F1macro: 0.1242,Test Accuracy: 0.161, Test F1-macro: 0.131\n",
      "02/466: Val Loss: 2.7070, Val F1macro: 0.1121,Test Accuracy: 0.151, Test F1-macro: 0.112\n",
      "02/467: Val Loss: 2.6526, Val F1macro: 0.1283,Test Accuracy: 0.163, Test F1-macro: 0.134\n",
      "02/468: Val Loss: 2.6522, Val F1macro: 0.1312,Test Accuracy: 0.166, Test F1-macro: 0.135\n",
      "02/469: Val Loss: 2.6707, Val F1macro: 0.1132,Test Accuracy: 0.153, Test F1-macro: 0.113\n",
      "02/470: Val Loss: 2.6565, Val F1macro: 0.1259,Test Accuracy: 0.160, Test F1-macro: 0.132\n",
      "02/471: Val Loss: 2.7030, Val F1macro: 0.1117,Test Accuracy: 0.150, Test F1-macro: 0.112\n",
      "02/472: Val Loss: 2.7413, Val F1macro: 0.1135,Test Accuracy: 0.154, Test F1-macro: 0.116\n",
      "02/473: Val Loss: 2.6613, Val F1macro: 0.1231,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/474: Val Loss: 2.7024, Val F1macro: 0.1145,Test Accuracy: 0.153, Test F1-macro: 0.113\n",
      "02/475: Val Loss: 2.6524, Val F1macro: 0.1288,Test Accuracy: 0.162, Test F1-macro: 0.133\n",
      "02/476: Val Loss: 2.6647, Val F1macro: 0.1122,Test Accuracy: 0.157, Test F1-macro: 0.116\n",
      "02/477: Val Loss: 2.6557, Val F1macro: 0.1217,Test Accuracy: 0.166, Test F1-macro: 0.134\n",
      "02/478: Val Loss: 2.6540, Val F1macro: 0.1206,Test Accuracy: 0.161, Test F1-macro: 0.130\n",
      "02/479: Val Loss: 2.6584, Val F1macro: 0.1133,Test Accuracy: 0.155, Test F1-macro: 0.112\n",
      "02/480: Val Loss: 2.6571, Val F1macro: 0.1112,Test Accuracy: 0.155, Test F1-macro: 0.112\n",
      "02/481: Val Loss: 2.6615, Val F1macro: 0.1188,Test Accuracy: 0.160, Test F1-macro: 0.127\n",
      "02/482: Val Loss: 2.7581, Val F1macro: 0.0945,Test Accuracy: 0.137, Test F1-macro: 0.093\n",
      "02/483: Val Loss: 2.6938, Val F1macro: 0.1153,Test Accuracy: 0.150, Test F1-macro: 0.112\n",
      "02/484: Val Loss: 2.6576, Val F1macro: 0.1145,Test Accuracy: 0.159, Test F1-macro: 0.122\n",
      "02/485: Val Loss: 2.6645, Val F1macro: 0.1126,Test Accuracy: 0.156, Test F1-macro: 0.114\n",
      "02/486: Val Loss: 2.6668, Val F1macro: 0.1133,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/487: Val Loss: 2.6535, Val F1macro: 0.1268,Test Accuracy: 0.165, Test F1-macro: 0.132\n",
      "02/488: Val Loss: 2.6701, Val F1macro: 0.1109,Test Accuracy: 0.154, Test F1-macro: 0.113\n",
      "02/489: Val Loss: 2.6583, Val F1macro: 0.1253,Test Accuracy: 0.159, Test F1-macro: 0.131\n",
      "02/490: Val Loss: 2.6596, Val F1macro: 0.1271,Test Accuracy: 0.164, Test F1-macro: 0.133\n",
      "02/491: Val Loss: 2.6893, Val F1macro: 0.1113,Test Accuracy: 0.155, Test F1-macro: 0.113\n",
      "02/492: Val Loss: 2.6610, Val F1macro: 0.1123,Test Accuracy: 0.159, Test F1-macro: 0.119\n",
      "02/493: Val Loss: 2.6590, Val F1macro: 0.1263,Test Accuracy: 0.159, Test F1-macro: 0.131\n",
      "02/494: Val Loss: 2.6731, Val F1macro: 0.1100,Test Accuracy: 0.153, Test F1-macro: 0.112\n",
      "02/495: Val Loss: 2.6610, Val F1macro: 0.1083,Test Accuracy: 0.158, Test F1-macro: 0.115\n",
      "02/496: Val Loss: 2.6753, Val F1macro: 0.1116,Test Accuracy: 0.151, Test F1-macro: 0.110\n",
      "02/497: Val Loss: 2.6596, Val F1macro: 0.1287,Test Accuracy: 0.165, Test F1-macro: 0.135\n",
      "02/498: Val Loss: 2.6601, Val F1macro: 0.1210,Test Accuracy: 0.160, Test F1-macro: 0.127\n",
      "02/499: Val Loss: 2.6950, Val F1macro: 0.1126,Test Accuracy: 0.151, Test F1-macro: 0.113\n",
      "02/500: Val Loss: 2.6751, Val F1macro: 0.1136,Test Accuracy: 0.153, Test F1-macro: 0.113\n",
      "03/001: Val Loss: 24.3666, Val F1macro: 0.0732,Test Accuracy: 0.087, Test F1-macro: 0.064\n",
      "03/002: Val Loss: 2.8189, Val F1macro: 0.0663,Test Accuracy: 0.090, Test F1-macro: 0.064\n",
      "03/003: Val Loss: 3.0169, Val F1macro: 0.0476,Test Accuracy: 0.081, Test F1-macro: 0.044\n",
      "03/004: Val Loss: 2.7280, Val F1macro: 0.1116,Test Accuracy: 0.159, Test F1-macro: 0.109\n",
      "03/005: Val Loss: 2.6960, Val F1macro: 0.1035,Test Accuracy: 0.154, Test F1-macro: 0.103\n",
      "03/006: Val Loss: 2.6849, Val F1macro: 0.1399,Test Accuracy: 0.151, Test F1-macro: 0.134\n",
      "03/007: Val Loss: 2.6844, Val F1macro: 0.1440,Test Accuracy: 0.159, Test F1-macro: 0.147\n",
      "03/008: Val Loss: 2.7364, Val F1macro: 0.0674,Test Accuracy: 0.129, Test F1-macro: 0.074\n",
      "03/009: Val Loss: 2.9370, Val F1macro: 0.0740,Test Accuracy: 0.084, Test F1-macro: 0.073\n",
      "03/010: Val Loss: 2.7820, Val F1macro: 0.0977,Test Accuracy: 0.108, Test F1-macro: 0.099\n",
      "03/011: Val Loss: 2.7351, Val F1macro: 0.0923,Test Accuracy: 0.106, Test F1-macro: 0.086\n",
      "03/012: Val Loss: 2.7141, Val F1macro: 0.1274,Test Accuracy: 0.136, Test F1-macro: 0.122\n",
      "03/013: Val Loss: 2.7113, Val F1macro: 0.0823,Test Accuracy: 0.139, Test F1-macro: 0.079\n",
      "03/014: Val Loss: 2.7518, Val F1macro: 0.0896,Test Accuracy: 0.137, Test F1-macro: 0.089\n",
      "03/015: Val Loss: 2.7567, Val F1macro: 0.1098,Test Accuracy: 0.136, Test F1-macro: 0.115\n",
      "03/016: Val Loss: 2.7058, Val F1macro: 0.0741,Test Accuracy: 0.140, Test F1-macro: 0.074\n",
      "03/017: Val Loss: 2.6934, Val F1macro: 0.1145,Test Accuracy: 0.152, Test F1-macro: 0.105\n",
      "03/018: Val Loss: 2.7267, Val F1macro: 0.0887,Test Accuracy: 0.140, Test F1-macro: 0.089\n",
      "03/019: Val Loss: 2.6986, Val F1macro: 0.0896,Test Accuracy: 0.141, Test F1-macro: 0.085\n",
      "03/020: Val Loss: 2.7551, Val F1macro: 0.1054,Test Accuracy: 0.141, Test F1-macro: 0.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/021: Val Loss: 2.6714, Val F1macro: 0.1349,Test Accuracy: 0.151, Test F1-macro: 0.128\n",
      "03/022: Val Loss: 2.7990, Val F1macro: 0.0732,Test Accuracy: 0.110, Test F1-macro: 0.065\n",
      "03/023: Val Loss: 2.7392, Val F1macro: 0.1623,Test Accuracy: 0.155, Test F1-macro: 0.159\n",
      "03/024: Val Loss: 2.8920, Val F1macro: 0.0868,Test Accuracy: 0.108, Test F1-macro: 0.080\n",
      "03/025: Val Loss: 2.9414, Val F1macro: 0.0719,Test Accuracy: 0.099, Test F1-macro: 0.071\n",
      "03/026: Val Loss: 2.7692, Val F1macro: 0.1364,Test Accuracy: 0.152, Test F1-macro: 0.148\n",
      "03/027: Val Loss: 2.7872, Val F1macro: 0.1162,Test Accuracy: 0.123, Test F1-macro: 0.109\n",
      "03/028: Val Loss: 2.7867, Val F1macro: 0.1485,Test Accuracy: 0.125, Test F1-macro: 0.135\n",
      "03/029: Val Loss: 2.6760, Val F1macro: 0.1558,Test Accuracy: 0.159, Test F1-macro: 0.150\n",
      "03/030: Val Loss: 2.7895, Val F1macro: 0.1456,Test Accuracy: 0.136, Test F1-macro: 0.136\n",
      "03/031: Val Loss: 2.7373, Val F1macro: 0.1740,Test Accuracy: 0.157, Test F1-macro: 0.173\n",
      "03/032: Val Loss: 2.7133, Val F1macro: 0.1450,Test Accuracy: 0.160, Test F1-macro: 0.138\n",
      "03/033: Val Loss: 2.7783, Val F1macro: 0.0853,Test Accuracy: 0.108, Test F1-macro: 0.076\n",
      "03/034: Val Loss: 2.6946, Val F1macro: 0.1768,Test Accuracy: 0.168, Test F1-macro: 0.171\n",
      "03/035: Val Loss: 2.6908, Val F1macro: 0.1402,Test Accuracy: 0.138, Test F1-macro: 0.127\n",
      "03/036: Val Loss: 2.6605, Val F1macro: 0.1500,Test Accuracy: 0.156, Test F1-macro: 0.146\n",
      "03/037: Val Loss: 2.7301, Val F1macro: 0.1324,Test Accuracy: 0.145, Test F1-macro: 0.130\n",
      "03/038: Val Loss: 2.7235, Val F1macro: 0.0967,Test Accuracy: 0.122, Test F1-macro: 0.097\n",
      "03/039: Val Loss: 2.6680, Val F1macro: 0.1383,Test Accuracy: 0.156, Test F1-macro: 0.129\n",
      "03/040: Val Loss: 2.6611, Val F1macro: 0.1517,Test Accuracy: 0.159, Test F1-macro: 0.143\n",
      "03/041: Val Loss: 2.6812, Val F1macro: 0.1391,Test Accuracy: 0.160, Test F1-macro: 0.134\n",
      "03/042: Val Loss: 2.7306, Val F1macro: 0.1474,Test Accuracy: 0.129, Test F1-macro: 0.137\n",
      "03/043: Val Loss: 2.6884, Val F1macro: 0.1377,Test Accuracy: 0.156, Test F1-macro: 0.132\n",
      "03/044: Val Loss: 2.6857, Val F1macro: 0.1380,Test Accuracy: 0.158, Test F1-macro: 0.131\n",
      "03/045: Val Loss: 2.6700, Val F1macro: 0.1411,Test Accuracy: 0.151, Test F1-macro: 0.132\n",
      "03/046: Val Loss: 2.6825, Val F1macro: 0.1698,Test Accuracy: 0.163, Test F1-macro: 0.161\n",
      "03/047: Val Loss: 2.7402, Val F1macro: 0.1749,Test Accuracy: 0.149, Test F1-macro: 0.173\n",
      "03/048: Val Loss: 2.7573, Val F1macro: 0.1726,Test Accuracy: 0.144, Test F1-macro: 0.172\n",
      "03/049: Val Loss: 2.6986, Val F1macro: 0.1609,Test Accuracy: 0.159, Test F1-macro: 0.157\n",
      "03/050: Val Loss: 2.6923, Val F1macro: 0.1007,Test Accuracy: 0.145, Test F1-macro: 0.092\n",
      "03/051: Val Loss: 2.9353, Val F1macro: 0.0706,Test Accuracy: 0.103, Test F1-macro: 0.065\n",
      "03/052: Val Loss: 2.8494, Val F1macro: 0.1594,Test Accuracy: 0.161, Test F1-macro: 0.153\n",
      "03/053: Val Loss: 2.6685, Val F1macro: 0.1493,Test Accuracy: 0.163, Test F1-macro: 0.142\n",
      "03/054: Val Loss: 2.7420, Val F1macro: 0.1798,Test Accuracy: 0.152, Test F1-macro: 0.173\n",
      "03/055: Val Loss: 2.6905, Val F1macro: 0.1740,Test Accuracy: 0.168, Test F1-macro: 0.164\n",
      "03/056: Val Loss: 2.8129, Val F1macro: 0.1022,Test Accuracy: 0.118, Test F1-macro: 0.096\n",
      "03/057: Val Loss: 2.7569, Val F1macro: 0.1244,Test Accuracy: 0.153, Test F1-macro: 0.128\n",
      "03/058: Val Loss: 2.6677, Val F1macro: 0.1661,Test Accuracy: 0.167, Test F1-macro: 0.156\n",
      "03/059: Val Loss: 2.6586, Val F1macro: 0.1860,Test Accuracy: 0.174, Test F1-macro: 0.176\n",
      "03/060: Val Loss: 2.7961, Val F1macro: 0.1908,Test Accuracy: 0.178, Test F1-macro: 0.186\n",
      "03/061: Val Loss: 2.6654, Val F1macro: 0.1662,Test Accuracy: 0.166, Test F1-macro: 0.157\n",
      "03/062: Val Loss: 2.6493, Val F1macro: 0.1714,Test Accuracy: 0.171, Test F1-macro: 0.165\n",
      "03/063: Val Loss: 2.6833, Val F1macro: 0.1685,Test Accuracy: 0.168, Test F1-macro: 0.167\n",
      "03/064: Val Loss: 2.8453, Val F1macro: 0.0914,Test Accuracy: 0.107, Test F1-macro: 0.088\n",
      "03/065: Val Loss: 2.7620, Val F1macro: 0.1839,Test Accuracy: 0.172, Test F1-macro: 0.174\n",
      "03/066: Val Loss: 2.7386, Val F1macro: 0.1676,Test Accuracy: 0.167, Test F1-macro: 0.160\n",
      "03/067: Val Loss: 2.7043, Val F1macro: 0.1675,Test Accuracy: 0.163, Test F1-macro: 0.169\n",
      "03/068: Val Loss: 2.6862, Val F1macro: 0.1643,Test Accuracy: 0.161, Test F1-macro: 0.165\n",
      "03/069: Val Loss: 2.7803, Val F1macro: 0.1170,Test Accuracy: 0.129, Test F1-macro: 0.119\n",
      "03/070: Val Loss: 2.7248, Val F1macro: 0.1859,Test Accuracy: 0.175, Test F1-macro: 0.183\n",
      "03/071: Val Loss: 2.6512, Val F1macro: 0.1882,Test Accuracy: 0.176, Test F1-macro: 0.183\n",
      "03/072: Val Loss: 2.7994, Val F1macro: 0.0961,Test Accuracy: 0.115, Test F1-macro: 0.086\n",
      "03/073: Val Loss: 2.7072, Val F1macro: 0.1690,Test Accuracy: 0.150, Test F1-macro: 0.169\n",
      "03/074: Val Loss: 2.6524, Val F1macro: 0.1883,Test Accuracy: 0.175, Test F1-macro: 0.183\n",
      "03/075: Val Loss: 2.7564, Val F1macro: 0.1879,Test Accuracy: 0.175, Test F1-macro: 0.186\n",
      "03/076: Val Loss: 2.7154, Val F1macro: 0.1725,Test Accuracy: 0.171, Test F1-macro: 0.170\n",
      "03/077: Val Loss: 2.7432, Val F1macro: 0.0869,Test Accuracy: 0.110, Test F1-macro: 0.081\n",
      "03/078: Val Loss: 2.8609, Val F1macro: 0.1859,Test Accuracy: 0.172, Test F1-macro: 0.181\n",
      "03/079: Val Loss: 2.6812, Val F1macro: 0.1873,Test Accuracy: 0.167, Test F1-macro: 0.184\n",
      "03/080: Val Loss: 2.7884, Val F1macro: 0.1859,Test Accuracy: 0.169, Test F1-macro: 0.177\n",
      "03/081: Val Loss: 2.6561, Val F1macro: 0.1682,Test Accuracy: 0.164, Test F1-macro: 0.163\n",
      "03/082: Val Loss: 2.6438, Val F1macro: 0.1717,Test Accuracy: 0.173, Test F1-macro: 0.167\n",
      "03/083: Val Loss: 2.6659, Val F1macro: 0.1726,Test Accuracy: 0.171, Test F1-macro: 0.169\n",
      "03/084: Val Loss: 2.9538, Val F1macro: 0.0674,Test Accuracy: 0.100, Test F1-macro: 0.059\n",
      "03/085: Val Loss: 2.6941, Val F1macro: 0.1734,Test Accuracy: 0.155, Test F1-macro: 0.169\n",
      "03/086: Val Loss: 2.8679, Val F1macro: 0.0943,Test Accuracy: 0.105, Test F1-macro: 0.085\n",
      "03/087: Val Loss: 2.6606, Val F1macro: 0.1711,Test Accuracy: 0.169, Test F1-macro: 0.165\n",
      "03/088: Val Loss: 2.6990, Val F1macro: 0.1684,Test Accuracy: 0.170, Test F1-macro: 0.167\n",
      "03/089: Val Loss: 2.6943, Val F1macro: 0.1430,Test Accuracy: 0.152, Test F1-macro: 0.133\n",
      "03/090: Val Loss: 2.6947, Val F1macro: 0.1690,Test Accuracy: 0.167, Test F1-macro: 0.172\n",
      "03/091: Val Loss: 2.6549, Val F1macro: 0.1741,Test Accuracy: 0.171, Test F1-macro: 0.168\n",
      "03/092: Val Loss: 2.9420, Val F1macro: 0.1892,Test Accuracy: 0.173, Test F1-macro: 0.183\n",
      "03/093: Val Loss: 2.6471, Val F1macro: 0.1838,Test Accuracy: 0.175, Test F1-macro: 0.176\n",
      "03/094: Val Loss: 2.6999, Val F1macro: 0.1700,Test Accuracy: 0.171, Test F1-macro: 0.167\n",
      "03/095: Val Loss: 2.6708, Val F1macro: 0.1888,Test Accuracy: 0.176, Test F1-macro: 0.185\n",
      "03/096: Val Loss: 2.6892, Val F1macro: 0.1734,Test Accuracy: 0.157, Test F1-macro: 0.172\n",
      "03/097: Val Loss: 2.7279, Val F1macro: 0.1750,Test Accuracy: 0.174, Test F1-macro: 0.173\n",
      "03/098: Val Loss: 2.6784, Val F1macro: 0.1748,Test Accuracy: 0.174, Test F1-macro: 0.170\n",
      "03/099: Val Loss: 2.7309, Val F1macro: 0.1928,Test Accuracy: 0.177, Test F1-macro: 0.186\n",
      "03/100: Val Loss: 2.6291, Val F1macro: 0.1757,Test Accuracy: 0.173, Test F1-macro: 0.172\n",
      "03/101: Val Loss: 2.7395, Val F1macro: 0.1729,Test Accuracy: 0.172, Test F1-macro: 0.168\n",
      "03/102: Val Loss: 2.7776, Val F1macro: 0.1474,Test Accuracy: 0.128, Test F1-macro: 0.140\n",
      "03/103: Val Loss: 2.7763, Val F1macro: 0.1756,Test Accuracy: 0.149, Test F1-macro: 0.171\n",
      "03/104: Val Loss: 2.6542, Val F1macro: 0.1820,Test Accuracy: 0.172, Test F1-macro: 0.171\n",
      "03/105: Val Loss: 2.6452, Val F1macro: 0.1915,Test Accuracy: 0.177, Test F1-macro: 0.186\n",
      "03/106: Val Loss: 2.6740, Val F1macro: 0.1858,Test Accuracy: 0.173, Test F1-macro: 0.179\n",
      "03/107: Val Loss: 2.8737, Val F1macro: 0.1663,Test Accuracy: 0.159, Test F1-macro: 0.164\n",
      "03/108: Val Loss: 2.6464, Val F1macro: 0.1771,Test Accuracy: 0.169, Test F1-macro: 0.171\n",
      "03/109: Val Loss: 2.6609, Val F1macro: 0.1920,Test Accuracy: 0.179, Test F1-macro: 0.186\n",
      "03/110: Val Loss: 2.6859, Val F1macro: 0.1617,Test Accuracy: 0.155, Test F1-macro: 0.155\n",
      "03/111: Val Loss: 2.7689, Val F1macro: 0.1647,Test Accuracy: 0.151, Test F1-macro: 0.151\n",
      "03/112: Val Loss: 2.7205, Val F1macro: 0.1575,Test Accuracy: 0.151, Test F1-macro: 0.152\n",
      "03/113: Val Loss: 2.7122, Val F1macro: 0.1633,Test Accuracy: 0.154, Test F1-macro: 0.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/114: Val Loss: 2.6939, Val F1macro: 0.1594,Test Accuracy: 0.153, Test F1-macro: 0.150\n",
      "03/115: Val Loss: 2.7573, Val F1macro: 0.1076,Test Accuracy: 0.130, Test F1-macro: 0.106\n",
      "03/116: Val Loss: 2.6542, Val F1macro: 0.1607,Test Accuracy: 0.151, Test F1-macro: 0.147\n",
      "03/117: Val Loss: 2.7714, Val F1macro: 0.1620,Test Accuracy: 0.151, Test F1-macro: 0.157\n",
      "03/118: Val Loss: 2.7273, Val F1macro: 0.1761,Test Accuracy: 0.165, Test F1-macro: 0.169\n",
      "03/119: Val Loss: 2.8126, Val F1macro: 0.1019,Test Accuracy: 0.118, Test F1-macro: 0.102\n",
      "03/120: Val Loss: 2.7848, Val F1macro: 0.1584,Test Accuracy: 0.152, Test F1-macro: 0.149\n",
      "03/121: Val Loss: 2.6621, Val F1macro: 0.1608,Test Accuracy: 0.154, Test F1-macro: 0.150\n",
      "03/122: Val Loss: 2.6513, Val F1macro: 0.1796,Test Accuracy: 0.156, Test F1-macro: 0.166\n",
      "03/123: Val Loss: 2.7247, Val F1macro: 0.1573,Test Accuracy: 0.153, Test F1-macro: 0.147\n",
      "03/124: Val Loss: 2.8599, Val F1macro: 0.1100,Test Accuracy: 0.122, Test F1-macro: 0.105\n",
      "03/125: Val Loss: 2.7887, Val F1macro: 0.1109,Test Accuracy: 0.140, Test F1-macro: 0.109\n",
      "03/126: Val Loss: 2.6828, Val F1macro: 0.1584,Test Accuracy: 0.154, Test F1-macro: 0.148\n",
      "03/127: Val Loss: 2.7949, Val F1macro: 0.1610,Test Accuracy: 0.138, Test F1-macro: 0.155\n",
      "03/128: Val Loss: 2.7401, Val F1macro: 0.1622,Test Accuracy: 0.149, Test F1-macro: 0.158\n",
      "03/129: Val Loss: 3.0345, Val F1macro: 0.1097,Test Accuracy: 0.122, Test F1-macro: 0.098\n",
      "03/130: Val Loss: 2.7135, Val F1macro: 0.1621,Test Accuracy: 0.151, Test F1-macro: 0.157\n",
      "03/131: Val Loss: 2.7327, Val F1macro: 0.1594,Test Accuracy: 0.152, Test F1-macro: 0.153\n",
      "03/132: Val Loss: 2.7458, Val F1macro: 0.1743,Test Accuracy: 0.167, Test F1-macro: 0.171\n",
      "03/133: Val Loss: 2.8547, Val F1macro: 0.1600,Test Accuracy: 0.148, Test F1-macro: 0.155\n",
      "03/134: Val Loss: 2.8029, Val F1macro: 0.1625,Test Accuracy: 0.154, Test F1-macro: 0.153\n",
      "03/135: Val Loss: 2.7700, Val F1macro: 0.1764,Test Accuracy: 0.167, Test F1-macro: 0.180\n",
      "03/136: Val Loss: 2.7755, Val F1macro: 0.1744,Test Accuracy: 0.169, Test F1-macro: 0.173\n",
      "03/137: Val Loss: 2.8964, Val F1macro: 0.1588,Test Accuracy: 0.132, Test F1-macro: 0.152\n",
      "03/138: Val Loss: 3.0087, Val F1macro: 0.0791,Test Accuracy: 0.107, Test F1-macro: 0.071\n",
      "03/139: Val Loss: 2.7581, Val F1macro: 0.1592,Test Accuracy: 0.132, Test F1-macro: 0.151\n",
      "03/140: Val Loss: 2.8584, Val F1macro: 0.0990,Test Accuracy: 0.117, Test F1-macro: 0.090\n",
      "03/141: Val Loss: 2.7480, Val F1macro: 0.1652,Test Accuracy: 0.138, Test F1-macro: 0.158\n",
      "03/142: Val Loss: 2.7480, Val F1macro: 0.1890,Test Accuracy: 0.172, Test F1-macro: 0.184\n",
      "03/143: Val Loss: 2.7503, Val F1macro: 0.1551,Test Accuracy: 0.151, Test F1-macro: 0.154\n",
      "03/144: Val Loss: 3.3477, Val F1macro: 0.1051,Test Accuracy: 0.125, Test F1-macro: 0.097\n",
      "03/145: Val Loss: 2.8237, Val F1macro: 0.1724,Test Accuracy: 0.172, Test F1-macro: 0.173\n",
      "03/146: Val Loss: 2.7938, Val F1macro: 0.0997,Test Accuracy: 0.139, Test F1-macro: 0.096\n",
      "03/147: Val Loss: 2.7764, Val F1macro: 0.1717,Test Accuracy: 0.168, Test F1-macro: 0.176\n",
      "03/148: Val Loss: 2.9383, Val F1macro: 0.1551,Test Accuracy: 0.144, Test F1-macro: 0.150\n",
      "03/149: Val Loss: 2.8664, Val F1macro: 0.1493,Test Accuracy: 0.136, Test F1-macro: 0.139\n",
      "03/150: Val Loss: 2.9483, Val F1macro: 0.0969,Test Accuracy: 0.112, Test F1-macro: 0.089\n",
      "03/151: Val Loss: 2.7074, Val F1macro: 0.1577,Test Accuracy: 0.156, Test F1-macro: 0.151\n",
      "03/152: Val Loss: 2.8647, Val F1macro: 0.1013,Test Accuracy: 0.113, Test F1-macro: 0.089\n",
      "03/153: Val Loss: 2.7012, Val F1macro: 0.1471,Test Accuracy: 0.150, Test F1-macro: 0.149\n",
      "03/154: Val Loss: 2.9174, Val F1macro: 0.0995,Test Accuracy: 0.119, Test F1-macro: 0.095\n",
      "03/155: Val Loss: 2.7756, Val F1macro: 0.1048,Test Accuracy: 0.128, Test F1-macro: 0.096\n",
      "03/156: Val Loss: 2.7083, Val F1macro: 0.1521,Test Accuracy: 0.148, Test F1-macro: 0.148\n",
      "03/157: Val Loss: 2.8010, Val F1macro: 0.1565,Test Accuracy: 0.152, Test F1-macro: 0.152\n",
      "03/158: Val Loss: 2.7609, Val F1macro: 0.1544,Test Accuracy: 0.147, Test F1-macro: 0.150\n",
      "03/159: Val Loss: 2.7856, Val F1macro: 0.0972,Test Accuracy: 0.134, Test F1-macro: 0.099\n",
      "03/160: Val Loss: 2.7256, Val F1macro: 0.1477,Test Accuracy: 0.137, Test F1-macro: 0.140\n",
      "03/161: Val Loss: 2.7598, Val F1macro: 0.1681,Test Accuracy: 0.171, Test F1-macro: 0.172\n",
      "03/162: Val Loss: 2.7491, Val F1macro: 0.1557,Test Accuracy: 0.156, Test F1-macro: 0.150\n",
      "03/163: Val Loss: 2.8524, Val F1macro: 0.1578,Test Accuracy: 0.152, Test F1-macro: 0.153\n",
      "03/164: Val Loss: 2.9199, Val F1macro: 0.1553,Test Accuracy: 0.153, Test F1-macro: 0.150\n",
      "03/165: Val Loss: 2.7118, Val F1macro: 0.1590,Test Accuracy: 0.154, Test F1-macro: 0.158\n",
      "03/166: Val Loss: 2.8845, Val F1macro: 0.1430,Test Accuracy: 0.146, Test F1-macro: 0.142\n",
      "03/167: Val Loss: 2.6585, Val F1macro: 0.1699,Test Accuracy: 0.159, Test F1-macro: 0.160\n",
      "03/168: Val Loss: 2.7197, Val F1macro: 0.1644,Test Accuracy: 0.158, Test F1-macro: 0.155\n",
      "03/169: Val Loss: 2.7440, Val F1macro: 0.1608,Test Accuracy: 0.168, Test F1-macro: 0.166\n",
      "03/170: Val Loss: 2.6764, Val F1macro: 0.1944,Test Accuracy: 0.180, Test F1-macro: 0.190\n",
      "03/171: Val Loss: 2.6556, Val F1macro: 0.1731,Test Accuracy: 0.173, Test F1-macro: 0.171\n",
      "03/172: Val Loss: 2.7147, Val F1macro: 0.1749,Test Accuracy: 0.176, Test F1-macro: 0.174\n",
      "03/173: Val Loss: 2.7050, Val F1macro: 0.1649,Test Accuracy: 0.169, Test F1-macro: 0.163\n",
      "03/174: Val Loss: 2.6386, Val F1macro: 0.1862,Test Accuracy: 0.175, Test F1-macro: 0.179\n",
      "03/175: Val Loss: 2.7595, Val F1macro: 0.1961,Test Accuracy: 0.182, Test F1-macro: 0.192\n",
      "03/176: Val Loss: 2.6479, Val F1macro: 0.1825,Test Accuracy: 0.175, Test F1-macro: 0.175\n",
      "03/177: Val Loss: 2.7402, Val F1macro: 0.1511,Test Accuracy: 0.144, Test F1-macro: 0.144\n",
      "03/178: Val Loss: 2.7047, Val F1macro: 0.1795,Test Accuracy: 0.172, Test F1-macro: 0.172\n",
      "03/179: Val Loss: 2.6411, Val F1macro: 0.1766,Test Accuracy: 0.174, Test F1-macro: 0.170\n",
      "03/180: Val Loss: 2.6907, Val F1macro: 0.1769,Test Accuracy: 0.175, Test F1-macro: 0.174\n",
      "03/181: Val Loss: 2.6890, Val F1macro: 0.1767,Test Accuracy: 0.173, Test F1-macro: 0.172\n",
      "03/182: Val Loss: 2.6882, Val F1macro: 0.1921,Test Accuracy: 0.181, Test F1-macro: 0.190\n",
      "03/183: Val Loss: 2.8353, Val F1macro: 0.0864,Test Accuracy: 0.118, Test F1-macro: 0.084\n",
      "03/184: Val Loss: 2.7251, Val F1macro: 0.1518,Test Accuracy: 0.151, Test F1-macro: 0.148\n",
      "03/185: Val Loss: 2.6866, Val F1macro: 0.1651,Test Accuracy: 0.164, Test F1-macro: 0.167\n",
      "03/186: Val Loss: 2.6536, Val F1macro: 0.1847,Test Accuracy: 0.175, Test F1-macro: 0.181\n",
      "03/187: Val Loss: 2.6637, Val F1macro: 0.1797,Test Accuracy: 0.175, Test F1-macro: 0.172\n",
      "03/188: Val Loss: 2.9524, Val F1macro: 0.1488,Test Accuracy: 0.140, Test F1-macro: 0.144\n",
      "03/189: Val Loss: 2.6471, Val F1macro: 0.1766,Test Accuracy: 0.176, Test F1-macro: 0.173\n",
      "03/190: Val Loss: 2.7764, Val F1macro: 0.1758,Test Accuracy: 0.178, Test F1-macro: 0.174\n",
      "03/191: Val Loss: 2.6731, Val F1macro: 0.1760,Test Accuracy: 0.173, Test F1-macro: 0.173\n",
      "03/192: Val Loss: 2.8567, Val F1macro: 0.1530,Test Accuracy: 0.144, Test F1-macro: 0.151\n",
      "03/193: Val Loss: 2.6798, Val F1macro: 0.1785,Test Accuracy: 0.172, Test F1-macro: 0.173\n",
      "03/194: Val Loss: 2.7344, Val F1macro: 0.1825,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/195: Val Loss: 2.6790, Val F1macro: 0.1797,Test Accuracy: 0.179, Test F1-macro: 0.177\n",
      "03/196: Val Loss: 2.6965, Val F1macro: 0.1640,Test Accuracy: 0.167, Test F1-macro: 0.160\n",
      "03/197: Val Loss: 2.6807, Val F1macro: 0.1814,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/198: Val Loss: 2.7064, Val F1macro: 0.1532,Test Accuracy: 0.148, Test F1-macro: 0.140\n",
      "03/199: Val Loss: 2.6249, Val F1macro: 0.1757,Test Accuracy: 0.176, Test F1-macro: 0.173\n",
      "03/200: Val Loss: 2.6586, Val F1macro: 0.1845,Test Accuracy: 0.178, Test F1-macro: 0.183\n",
      "03/201: Val Loss: 2.7016, Val F1macro: 0.1938,Test Accuracy: 0.176, Test F1-macro: 0.185\n",
      "03/202: Val Loss: 2.7444, Val F1macro: 0.1503,Test Accuracy: 0.149, Test F1-macro: 0.152\n",
      "03/203: Val Loss: 2.6655, Val F1macro: 0.1870,Test Accuracy: 0.174, Test F1-macro: 0.176\n",
      "03/204: Val Loss: 2.8154, Val F1macro: 0.1531,Test Accuracy: 0.153, Test F1-macro: 0.147\n",
      "03/205: Val Loss: 2.6922, Val F1macro: 0.1812,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/206: Val Loss: 2.7473, Val F1macro: 0.1501,Test Accuracy: 0.137, Test F1-macro: 0.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/207: Val Loss: 2.9797, Val F1macro: 0.1753,Test Accuracy: 0.171, Test F1-macro: 0.172\n",
      "03/208: Val Loss: 2.6544, Val F1macro: 0.1776,Test Accuracy: 0.174, Test F1-macro: 0.172\n",
      "03/209: Val Loss: 2.7938, Val F1macro: 0.1791,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/210: Val Loss: 2.6751, Val F1macro: 0.1759,Test Accuracy: 0.173, Test F1-macro: 0.174\n",
      "03/211: Val Loss: 2.6773, Val F1macro: 0.1647,Test Accuracy: 0.168, Test F1-macro: 0.162\n",
      "03/212: Val Loss: 2.6389, Val F1macro: 0.1780,Test Accuracy: 0.179, Test F1-macro: 0.175\n",
      "03/213: Val Loss: 2.6759, Val F1macro: 0.1778,Test Accuracy: 0.179, Test F1-macro: 0.176\n",
      "03/214: Val Loss: 2.6790, Val F1macro: 0.1780,Test Accuracy: 0.179, Test F1-macro: 0.177\n",
      "03/215: Val Loss: 2.6477, Val F1macro: 0.1794,Test Accuracy: 0.181, Test F1-macro: 0.180\n",
      "03/216: Val Loss: 2.6798, Val F1macro: 0.1764,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/217: Val Loss: 2.9533, Val F1macro: 0.1779,Test Accuracy: 0.176, Test F1-macro: 0.176\n",
      "03/218: Val Loss: 2.6850, Val F1macro: 0.1758,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/219: Val Loss: 2.7045, Val F1macro: 0.1625,Test Accuracy: 0.167, Test F1-macro: 0.166\n",
      "03/220: Val Loss: 2.7033, Val F1macro: 0.1615,Test Accuracy: 0.163, Test F1-macro: 0.164\n",
      "03/221: Val Loss: 2.6396, Val F1macro: 0.1784,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/222: Val Loss: 2.7665, Val F1macro: 0.1586,Test Accuracy: 0.164, Test F1-macro: 0.159\n",
      "03/223: Val Loss: 2.6513, Val F1macro: 0.1777,Test Accuracy: 0.177, Test F1-macro: 0.175\n",
      "03/224: Val Loss: 2.6313, Val F1macro: 0.1745,Test Accuracy: 0.171, Test F1-macro: 0.171\n",
      "03/225: Val Loss: 2.6543, Val F1macro: 0.1767,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/226: Val Loss: 2.6482, Val F1macro: 0.1648,Test Accuracy: 0.166, Test F1-macro: 0.164\n",
      "03/227: Val Loss: 2.6298, Val F1macro: 0.1781,Test Accuracy: 0.178, Test F1-macro: 0.174\n",
      "03/228: Val Loss: 2.6343, Val F1macro: 0.1785,Test Accuracy: 0.178, Test F1-macro: 0.174\n",
      "03/229: Val Loss: 2.6636, Val F1macro: 0.1767,Test Accuracy: 0.178, Test F1-macro: 0.175\n",
      "03/230: Val Loss: 2.6307, Val F1macro: 0.1795,Test Accuracy: 0.175, Test F1-macro: 0.172\n",
      "03/231: Val Loss: 2.7473, Val F1macro: 0.1766,Test Accuracy: 0.171, Test F1-macro: 0.175\n",
      "03/232: Val Loss: 2.6437, Val F1macro: 0.1950,Test Accuracy: 0.183, Test F1-macro: 0.190\n",
      "03/233: Val Loss: 2.7180, Val F1macro: 0.1985,Test Accuracy: 0.183, Test F1-macro: 0.190\n",
      "03/234: Val Loss: 2.7741, Val F1macro: 0.1674,Test Accuracy: 0.162, Test F1-macro: 0.168\n",
      "03/235: Val Loss: 2.6955, Val F1macro: 0.1950,Test Accuracy: 0.183, Test F1-macro: 0.192\n",
      "03/236: Val Loss: 2.7784, Val F1macro: 0.1983,Test Accuracy: 0.182, Test F1-macro: 0.192\n",
      "03/237: Val Loss: 2.8027, Val F1macro: 0.1765,Test Accuracy: 0.178, Test F1-macro: 0.173\n",
      "03/238: Val Loss: 2.7336, Val F1macro: 0.1450,Test Accuracy: 0.141, Test F1-macro: 0.144\n",
      "03/239: Val Loss: 2.6726, Val F1macro: 0.1933,Test Accuracy: 0.180, Test F1-macro: 0.187\n",
      "03/240: Val Loss: 3.0518, Val F1macro: 0.1643,Test Accuracy: 0.175, Test F1-macro: 0.166\n",
      "03/241: Val Loss: 2.8946, Val F1macro: 0.1839,Test Accuracy: 0.178, Test F1-macro: 0.179\n",
      "03/242: Val Loss: 2.8018, Val F1macro: 0.1678,Test Accuracy: 0.177, Test F1-macro: 0.166\n",
      "03/243: Val Loss: 2.7462, Val F1macro: 0.1797,Test Accuracy: 0.178, Test F1-macro: 0.175\n",
      "03/244: Val Loss: 2.6387, Val F1macro: 0.1961,Test Accuracy: 0.181, Test F1-macro: 0.189\n",
      "03/245: Val Loss: 2.6269, Val F1macro: 0.1831,Test Accuracy: 0.178, Test F1-macro: 0.175\n",
      "03/246: Val Loss: 2.6329, Val F1macro: 0.1710,Test Accuracy: 0.176, Test F1-macro: 0.170\n",
      "03/247: Val Loss: 2.7004, Val F1macro: 0.1980,Test Accuracy: 0.181, Test F1-macro: 0.192\n",
      "03/248: Val Loss: 2.7911, Val F1macro: 0.1750,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/249: Val Loss: 2.6555, Val F1macro: 0.1918,Test Accuracy: 0.174, Test F1-macro: 0.181\n",
      "03/250: Val Loss: 2.6513, Val F1macro: 0.1986,Test Accuracy: 0.179, Test F1-macro: 0.192\n",
      "03/251: Val Loss: 2.6925, Val F1macro: 0.1968,Test Accuracy: 0.181, Test F1-macro: 0.193\n",
      "03/252: Val Loss: 2.7027, Val F1macro: 0.1621,Test Accuracy: 0.162, Test F1-macro: 0.163\n",
      "03/253: Val Loss: 2.8973, Val F1macro: 0.1434,Test Accuracy: 0.133, Test F1-macro: 0.141\n",
      "03/254: Val Loss: 2.7459, Val F1macro: 0.1994,Test Accuracy: 0.179, Test F1-macro: 0.190\n",
      "03/255: Val Loss: 2.7646, Val F1macro: 0.1523,Test Accuracy: 0.138, Test F1-macro: 0.148\n",
      "03/256: Val Loss: 2.6734, Val F1macro: 0.1991,Test Accuracy: 0.185, Test F1-macro: 0.193\n",
      "03/257: Val Loss: 2.6979, Val F1macro: 0.1978,Test Accuracy: 0.184, Test F1-macro: 0.193\n",
      "03/258: Val Loss: 2.7365, Val F1macro: 0.1985,Test Accuracy: 0.181, Test F1-macro: 0.190\n",
      "03/259: Val Loss: 2.6972, Val F1macro: 0.1985,Test Accuracy: 0.182, Test F1-macro: 0.185\n",
      "03/260: Val Loss: 2.7665, Val F1macro: 0.1959,Test Accuracy: 0.184, Test F1-macro: 0.193\n",
      "03/261: Val Loss: 2.7188, Val F1macro: 0.1987,Test Accuracy: 0.179, Test F1-macro: 0.191\n",
      "03/262: Val Loss: 2.8268, Val F1macro: 0.1851,Test Accuracy: 0.176, Test F1-macro: 0.181\n",
      "03/263: Val Loss: 2.9708, Val F1macro: 0.1647,Test Accuracy: 0.167, Test F1-macro: 0.162\n",
      "03/264: Val Loss: 2.6764, Val F1macro: 0.1946,Test Accuracy: 0.183, Test F1-macro: 0.191\n",
      "03/265: Val Loss: 2.8780, Val F1macro: 0.1474,Test Accuracy: 0.147, Test F1-macro: 0.144\n",
      "03/266: Val Loss: 2.7156, Val F1macro: 0.1437,Test Accuracy: 0.147, Test F1-macro: 0.141\n",
      "03/267: Val Loss: 2.6953, Val F1macro: 0.1963,Test Accuracy: 0.180, Test F1-macro: 0.192\n",
      "03/268: Val Loss: 2.6808, Val F1macro: 0.1937,Test Accuracy: 0.175, Test F1-macro: 0.185\n",
      "03/269: Val Loss: 2.7181, Val F1macro: 0.1926,Test Accuracy: 0.178, Test F1-macro: 0.181\n",
      "03/270: Val Loss: 2.7645, Val F1macro: 0.1837,Test Accuracy: 0.175, Test F1-macro: 0.177\n",
      "03/271: Val Loss: 2.7838, Val F1macro: 0.1643,Test Accuracy: 0.175, Test F1-macro: 0.166\n",
      "03/272: Val Loss: 2.6430, Val F1macro: 0.1937,Test Accuracy: 0.181, Test F1-macro: 0.188\n",
      "03/273: Val Loss: 2.7537, Val F1macro: 0.1586,Test Accuracy: 0.155, Test F1-macro: 0.156\n",
      "03/274: Val Loss: 2.6577, Val F1macro: 0.1809,Test Accuracy: 0.178, Test F1-macro: 0.173\n",
      "03/275: Val Loss: 2.6858, Val F1macro: 0.1970,Test Accuracy: 0.181, Test F1-macro: 0.190\n",
      "03/276: Val Loss: 2.6295, Val F1macro: 0.1858,Test Accuracy: 0.179, Test F1-macro: 0.181\n",
      "03/277: Val Loss: 2.6379, Val F1macro: 0.1961,Test Accuracy: 0.183, Test F1-macro: 0.190\n",
      "03/278: Val Loss: 2.6317, Val F1macro: 0.1964,Test Accuracy: 0.182, Test F1-macro: 0.189\n",
      "03/279: Val Loss: 2.8104, Val F1macro: 0.1642,Test Accuracy: 0.167, Test F1-macro: 0.162\n",
      "03/280: Val Loss: 2.6788, Val F1macro: 0.1635,Test Accuracy: 0.166, Test F1-macro: 0.164\n",
      "03/281: Val Loss: 2.6824, Val F1macro: 0.1696,Test Accuracy: 0.177, Test F1-macro: 0.167\n",
      "03/282: Val Loss: 2.7344, Val F1macro: 0.1670,Test Accuracy: 0.167, Test F1-macro: 0.159\n",
      "03/283: Val Loss: 2.6410, Val F1macro: 0.1940,Test Accuracy: 0.183, Test F1-macro: 0.189\n",
      "03/284: Val Loss: 2.8594, Val F1macro: 0.1901,Test Accuracy: 0.182, Test F1-macro: 0.179\n",
      "03/285: Val Loss: 2.7616, Val F1macro: 0.1953,Test Accuracy: 0.179, Test F1-macro: 0.184\n",
      "03/286: Val Loss: 2.7032, Val F1macro: 0.1972,Test Accuracy: 0.182, Test F1-macro: 0.190\n",
      "03/287: Val Loss: 2.6856, Val F1macro: 0.1829,Test Accuracy: 0.178, Test F1-macro: 0.174\n",
      "03/288: Val Loss: 2.8184, Val F1macro: 0.1429,Test Accuracy: 0.124, Test F1-macro: 0.138\n",
      "03/289: Val Loss: 2.6965, Val F1macro: 0.1850,Test Accuracy: 0.179, Test F1-macro: 0.181\n",
      "03/290: Val Loss: 2.6682, Val F1macro: 0.1824,Test Accuracy: 0.178, Test F1-macro: 0.175\n",
      "03/291: Val Loss: 2.6920, Val F1macro: 0.1912,Test Accuracy: 0.179, Test F1-macro: 0.177\n",
      "03/292: Val Loss: 2.7197, Val F1macro: 0.1944,Test Accuracy: 0.182, Test F1-macro: 0.189\n",
      "03/293: Val Loss: 2.7594, Val F1macro: 0.1791,Test Accuracy: 0.175, Test F1-macro: 0.170\n",
      "03/294: Val Loss: 2.9153, Val F1macro: 0.1609,Test Accuracy: 0.158, Test F1-macro: 0.157\n",
      "03/295: Val Loss: 2.9382, Val F1macro: 0.1508,Test Accuracy: 0.139, Test F1-macro: 0.143\n",
      "03/296: Val Loss: 2.6576, Val F1macro: 0.1773,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/297: Val Loss: 2.7106, Val F1macro: 0.1706,Test Accuracy: 0.169, Test F1-macro: 0.158\n",
      "03/298: Val Loss: 2.6365, Val F1macro: 0.1802,Test Accuracy: 0.178, Test F1-macro: 0.172\n",
      "03/299: Val Loss: 2.6627, Val F1macro: 0.1644,Test Accuracy: 0.159, Test F1-macro: 0.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/300: Val Loss: 3.2974, Val F1macro: 0.1803,Test Accuracy: 0.173, Test F1-macro: 0.179\n",
      "03/301: Val Loss: 2.7761, Val F1macro: 0.1815,Test Accuracy: 0.175, Test F1-macro: 0.172\n",
      "03/302: Val Loss: 2.7916, Val F1macro: 0.1768,Test Accuracy: 0.175, Test F1-macro: 0.172\n",
      "03/303: Val Loss: 2.7433, Val F1macro: 0.1811,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/304: Val Loss: 2.7363, Val F1macro: 0.1793,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/305: Val Loss: 2.6898, Val F1macro: 0.1808,Test Accuracy: 0.177, Test F1-macro: 0.172\n",
      "03/306: Val Loss: 2.7314, Val F1macro: 0.1827,Test Accuracy: 0.174, Test F1-macro: 0.178\n",
      "03/307: Val Loss: 2.8502, Val F1macro: 0.1440,Test Accuracy: 0.125, Test F1-macro: 0.139\n",
      "03/308: Val Loss: 2.7258, Val F1macro: 0.1855,Test Accuracy: 0.177, Test F1-macro: 0.177\n",
      "03/309: Val Loss: 2.8613, Val F1macro: 0.1779,Test Accuracy: 0.177, Test F1-macro: 0.173\n",
      "03/310: Val Loss: 2.7397, Val F1macro: 0.1608,Test Accuracy: 0.158, Test F1-macro: 0.162\n",
      "03/311: Val Loss: 2.6292, Val F1macro: 0.1808,Test Accuracy: 0.176, Test F1-macro: 0.172\n",
      "03/312: Val Loss: 2.8158, Val F1macro: 0.1476,Test Accuracy: 0.134, Test F1-macro: 0.142\n",
      "03/313: Val Loss: 2.7376, Val F1macro: 0.1703,Test Accuracy: 0.174, Test F1-macro: 0.163\n",
      "03/314: Val Loss: 2.7040, Val F1macro: 0.1809,Test Accuracy: 0.178, Test F1-macro: 0.173\n",
      "03/315: Val Loss: 2.6718, Val F1macro: 0.1888,Test Accuracy: 0.182, Test F1-macro: 0.187\n",
      "03/316: Val Loss: 2.6599, Val F1macro: 0.1748,Test Accuracy: 0.171, Test F1-macro: 0.163\n",
      "03/317: Val Loss: 2.6932, Val F1macro: 0.1708,Test Accuracy: 0.176, Test F1-macro: 0.162\n",
      "03/318: Val Loss: 2.6880, Val F1macro: 0.1671,Test Accuracy: 0.173, Test F1-macro: 0.162\n",
      "03/319: Val Loss: 2.6372, Val F1macro: 0.1670,Test Accuracy: 0.174, Test F1-macro: 0.162\n",
      "03/320: Val Loss: 2.7341, Val F1macro: 0.1637,Test Accuracy: 0.169, Test F1-macro: 0.161\n",
      "03/321: Val Loss: 2.7375, Val F1macro: 0.1477,Test Accuracy: 0.137, Test F1-macro: 0.145\n",
      "03/322: Val Loss: 2.7011, Val F1macro: 0.1790,Test Accuracy: 0.175, Test F1-macro: 0.170\n",
      "03/323: Val Loss: 2.7525, Val F1macro: 0.1802,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/324: Val Loss: 2.7308, Val F1macro: 0.1684,Test Accuracy: 0.175, Test F1-macro: 0.165\n",
      "03/325: Val Loss: 2.6347, Val F1macro: 0.1784,Test Accuracy: 0.177, Test F1-macro: 0.173\n",
      "03/326: Val Loss: 2.9592, Val F1macro: 0.0977,Test Accuracy: 0.117, Test F1-macro: 0.094\n",
      "03/327: Val Loss: 2.8947, Val F1macro: 0.1667,Test Accuracy: 0.161, Test F1-macro: 0.165\n",
      "03/328: Val Loss: 2.6490, Val F1macro: 0.1661,Test Accuracy: 0.172, Test F1-macro: 0.163\n",
      "03/329: Val Loss: 2.9543, Val F1macro: 0.1782,Test Accuracy: 0.177, Test F1-macro: 0.173\n",
      "03/330: Val Loss: 2.7055, Val F1macro: 0.1779,Test Accuracy: 0.178, Test F1-macro: 0.171\n",
      "03/331: Val Loss: 2.7479, Val F1macro: 0.1445,Test Accuracy: 0.136, Test F1-macro: 0.140\n",
      "03/332: Val Loss: 2.7341, Val F1macro: 0.1672,Test Accuracy: 0.162, Test F1-macro: 0.166\n",
      "03/333: Val Loss: 2.7394, Val F1macro: 0.1727,Test Accuracy: 0.175, Test F1-macro: 0.167\n",
      "03/334: Val Loss: 2.6531, Val F1macro: 0.1795,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/335: Val Loss: 2.8614, Val F1macro: 0.1617,Test Accuracy: 0.158, Test F1-macro: 0.159\n",
      "03/336: Val Loss: 2.6386, Val F1macro: 0.1780,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/337: Val Loss: 2.7109, Val F1macro: 0.1871,Test Accuracy: 0.180, Test F1-macro: 0.180\n",
      "03/338: Val Loss: 2.7697, Val F1macro: 0.1795,Test Accuracy: 0.175, Test F1-macro: 0.172\n",
      "03/339: Val Loss: 2.8594, Val F1macro: 0.1764,Test Accuracy: 0.173, Test F1-macro: 0.169\n",
      "03/340: Val Loss: 2.9447, Val F1macro: 0.1486,Test Accuracy: 0.132, Test F1-macro: 0.145\n",
      "03/341: Val Loss: 2.7382, Val F1macro: 0.1662,Test Accuracy: 0.162, Test F1-macro: 0.164\n",
      "03/342: Val Loss: 2.8685, Val F1macro: 0.1124,Test Accuracy: 0.130, Test F1-macro: 0.115\n",
      "03/343: Val Loss: 2.7408, Val F1macro: 0.1645,Test Accuracy: 0.165, Test F1-macro: 0.160\n",
      "03/344: Val Loss: 2.6526, Val F1macro: 0.1665,Test Accuracy: 0.172, Test F1-macro: 0.161\n",
      "03/345: Val Loss: 2.7038, Val F1macro: 0.1627,Test Accuracy: 0.171, Test F1-macro: 0.161\n",
      "03/346: Val Loss: 2.6572, Val F1macro: 0.1682,Test Accuracy: 0.174, Test F1-macro: 0.166\n",
      "03/347: Val Loss: 2.7179, Val F1macro: 0.1667,Test Accuracy: 0.174, Test F1-macro: 0.163\n",
      "03/348: Val Loss: 2.6812, Val F1macro: 0.1661,Test Accuracy: 0.176, Test F1-macro: 0.162\n",
      "03/349: Val Loss: 2.7177, Val F1macro: 0.1668,Test Accuracy: 0.171, Test F1-macro: 0.160\n",
      "03/350: Val Loss: 2.6877, Val F1macro: 0.1661,Test Accuracy: 0.175, Test F1-macro: 0.161\n",
      "03/351: Val Loss: 2.7019, Val F1macro: 0.1613,Test Accuracy: 0.159, Test F1-macro: 0.158\n",
      "03/352: Val Loss: 2.7384, Val F1macro: 0.1651,Test Accuracy: 0.175, Test F1-macro: 0.163\n",
      "03/353: Val Loss: 2.6301, Val F1macro: 0.1676,Test Accuracy: 0.174, Test F1-macro: 0.160\n",
      "03/354: Val Loss: 2.6622, Val F1macro: 0.1802,Test Accuracy: 0.180, Test F1-macro: 0.174\n",
      "03/355: Val Loss: 2.6784, Val F1macro: 0.1639,Test Accuracy: 0.173, Test F1-macro: 0.163\n",
      "03/356: Val Loss: 2.7902, Val F1macro: 0.1772,Test Accuracy: 0.177, Test F1-macro: 0.173\n",
      "03/357: Val Loss: 2.6250, Val F1macro: 0.1769,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/358: Val Loss: 2.7005, Val F1macro: 0.1803,Test Accuracy: 0.175, Test F1-macro: 0.171\n",
      "03/359: Val Loss: 2.6723, Val F1macro: 0.1629,Test Accuracy: 0.164, Test F1-macro: 0.161\n",
      "03/360: Val Loss: 2.6990, Val F1macro: 0.1715,Test Accuracy: 0.175, Test F1-macro: 0.168\n",
      "03/361: Val Loss: 2.6549, Val F1macro: 0.1673,Test Accuracy: 0.171, Test F1-macro: 0.159\n",
      "03/362: Val Loss: 2.6452, Val F1macro: 0.1698,Test Accuracy: 0.177, Test F1-macro: 0.168\n",
      "03/363: Val Loss: 2.7131, Val F1macro: 0.1679,Test Accuracy: 0.176, Test F1-macro: 0.163\n",
      "03/364: Val Loss: 2.9895, Val F1macro: 0.1641,Test Accuracy: 0.166, Test F1-macro: 0.158\n",
      "03/365: Val Loss: 2.6718, Val F1macro: 0.1641,Test Accuracy: 0.166, Test F1-macro: 0.159\n",
      "03/366: Val Loss: 2.7825, Val F1macro: 0.1779,Test Accuracy: 0.175, Test F1-macro: 0.171\n",
      "03/367: Val Loss: 2.6624, Val F1macro: 0.1681,Test Accuracy: 0.173, Test F1-macro: 0.161\n",
      "03/368: Val Loss: 2.7217, Val F1macro: 0.1840,Test Accuracy: 0.179, Test F1-macro: 0.177\n",
      "03/369: Val Loss: 2.7587, Val F1macro: 0.1666,Test Accuracy: 0.159, Test F1-macro: 0.166\n",
      "03/370: Val Loss: 2.7341, Val F1macro: 0.1765,Test Accuracy: 0.167, Test F1-macro: 0.164\n",
      "03/371: Val Loss: 2.7402, Val F1macro: 0.1638,Test Accuracy: 0.168, Test F1-macro: 0.166\n",
      "03/372: Val Loss: 2.6707, Val F1macro: 0.1774,Test Accuracy: 0.180, Test F1-macro: 0.172\n",
      "03/373: Val Loss: 2.6970, Val F1macro: 0.1620,Test Accuracy: 0.171, Test F1-macro: 0.161\n",
      "03/374: Val Loss: 2.7684, Val F1macro: 0.1779,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/375: Val Loss: 2.9431, Val F1macro: 0.1367,Test Accuracy: 0.129, Test F1-macro: 0.132\n",
      "03/376: Val Loss: 2.9222, Val F1macro: 0.1421,Test Accuracy: 0.133, Test F1-macro: 0.141\n",
      "03/377: Val Loss: 2.7078, Val F1macro: 0.1637,Test Accuracy: 0.161, Test F1-macro: 0.164\n",
      "03/378: Val Loss: 2.6367, Val F1macro: 0.1716,Test Accuracy: 0.174, Test F1-macro: 0.165\n",
      "03/379: Val Loss: 2.9193, Val F1macro: 0.1595,Test Accuracy: 0.160, Test F1-macro: 0.156\n",
      "03/380: Val Loss: 2.7179, Val F1macro: 0.1635,Test Accuracy: 0.171, Test F1-macro: 0.161\n",
      "03/381: Val Loss: 3.0611, Val F1macro: 0.1646,Test Accuracy: 0.168, Test F1-macro: 0.157\n",
      "03/382: Val Loss: 2.7797, Val F1macro: 0.1634,Test Accuracy: 0.166, Test F1-macro: 0.167\n",
      "03/383: Val Loss: 2.8168, Val F1macro: 0.1642,Test Accuracy: 0.166, Test F1-macro: 0.166\n",
      "03/384: Val Loss: 2.6875, Val F1macro: 0.1618,Test Accuracy: 0.172, Test F1-macro: 0.163\n",
      "03/385: Val Loss: 2.8088, Val F1macro: 0.1786,Test Accuracy: 0.180, Test F1-macro: 0.173\n",
      "03/386: Val Loss: 2.6928, Val F1macro: 0.1759,Test Accuracy: 0.176, Test F1-macro: 0.173\n",
      "03/387: Val Loss: 2.7731, Val F1macro: 0.1680,Test Accuracy: 0.172, Test F1-macro: 0.163\n",
      "03/388: Val Loss: 2.6716, Val F1macro: 0.1638,Test Accuracy: 0.168, Test F1-macro: 0.163\n",
      "03/389: Val Loss: 4.3223, Val F1macro: 0.1739,Test Accuracy: 0.172, Test F1-macro: 0.168\n",
      "03/390: Val Loss: 2.8933, Val F1macro: 0.1794,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/391: Val Loss: 2.7089, Val F1macro: 0.1676,Test Accuracy: 0.161, Test F1-macro: 0.164\n",
      "03/392: Val Loss: 2.7346, Val F1macro: 0.1782,Test Accuracy: 0.179, Test F1-macro: 0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/393: Val Loss: 2.6677, Val F1macro: 0.1770,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/394: Val Loss: 2.9369, Val F1macro: 0.1634,Test Accuracy: 0.160, Test F1-macro: 0.162\n",
      "03/395: Val Loss: 2.6246, Val F1macro: 0.1797,Test Accuracy: 0.177, Test F1-macro: 0.172\n",
      "03/396: Val Loss: 2.6454, Val F1macro: 0.1731,Test Accuracy: 0.167, Test F1-macro: 0.162\n",
      "03/397: Val Loss: 2.6603, Val F1macro: 0.1687,Test Accuracy: 0.174, Test F1-macro: 0.164\n",
      "03/398: Val Loss: 2.6794, Val F1macro: 0.1633,Test Accuracy: 0.165, Test F1-macro: 0.161\n",
      "03/399: Val Loss: 2.6432, Val F1macro: 0.1697,Test Accuracy: 0.171, Test F1-macro: 0.161\n",
      "03/400: Val Loss: 2.7815, Val F1macro: 0.1640,Test Accuracy: 0.161, Test F1-macro: 0.165\n",
      "03/401: Val Loss: 2.7497, Val F1macro: 0.1638,Test Accuracy: 0.162, Test F1-macro: 0.158\n",
      "03/402: Val Loss: 2.7186, Val F1macro: 0.1675,Test Accuracy: 0.172, Test F1-macro: 0.161\n",
      "03/403: Val Loss: 2.8329, Val F1macro: 0.1662,Test Accuracy: 0.159, Test F1-macro: 0.166\n",
      "03/404: Val Loss: 2.6295, Val F1macro: 0.1721,Test Accuracy: 0.171, Test F1-macro: 0.162\n",
      "03/405: Val Loss: 2.7649, Val F1macro: 0.1791,Test Accuracy: 0.175, Test F1-macro: 0.170\n",
      "03/406: Val Loss: 2.8737, Val F1macro: 0.1650,Test Accuracy: 0.165, Test F1-macro: 0.160\n",
      "03/407: Val Loss: 2.7767, Val F1macro: 0.1618,Test Accuracy: 0.158, Test F1-macro: 0.159\n",
      "03/408: Val Loss: 2.7362, Val F1macro: 0.1653,Test Accuracy: 0.170, Test F1-macro: 0.164\n",
      "03/409: Val Loss: 2.7583, Val F1macro: 0.1540,Test Accuracy: 0.149, Test F1-macro: 0.150\n",
      "03/410: Val Loss: 2.8190, Val F1macro: 0.1657,Test Accuracy: 0.176, Test F1-macro: 0.163\n",
      "03/411: Val Loss: 2.6270, Val F1macro: 0.1804,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/412: Val Loss: 2.6358, Val F1macro: 0.1806,Test Accuracy: 0.173, Test F1-macro: 0.168\n",
      "03/413: Val Loss: 2.6296, Val F1macro: 0.1775,Test Accuracy: 0.176, Test F1-macro: 0.172\n",
      "03/414: Val Loss: 2.7037, Val F1macro: 0.1799,Test Accuracy: 0.178, Test F1-macro: 0.172\n",
      "03/415: Val Loss: 2.7478, Val F1macro: 0.1637,Test Accuracy: 0.171, Test F1-macro: 0.166\n",
      "03/416: Val Loss: 2.7613, Val F1macro: 0.1776,Test Accuracy: 0.178, Test F1-macro: 0.172\n",
      "03/417: Val Loss: 2.7167, Val F1macro: 0.1742,Test Accuracy: 0.175, Test F1-macro: 0.168\n",
      "03/418: Val Loss: 2.6628, Val F1macro: 0.1810,Test Accuracy: 0.177, Test F1-macro: 0.173\n",
      "03/419: Val Loss: 2.7403, Val F1macro: 0.1782,Test Accuracy: 0.177, Test F1-macro: 0.172\n",
      "03/420: Val Loss: 2.7251, Val F1macro: 0.1723,Test Accuracy: 0.167, Test F1-macro: 0.160\n",
      "03/421: Val Loss: 2.7660, Val F1macro: 0.1650,Test Accuracy: 0.174, Test F1-macro: 0.165\n",
      "03/422: Val Loss: 3.4734, Val F1macro: 0.1646,Test Accuracy: 0.163, Test F1-macro: 0.165\n",
      "03/423: Val Loss: 2.9261, Val F1macro: 0.1672,Test Accuracy: 0.172, Test F1-macro: 0.163\n",
      "03/424: Val Loss: 2.8010, Val F1macro: 0.1658,Test Accuracy: 0.176, Test F1-macro: 0.164\n",
      "03/425: Val Loss: 2.9790, Val F1macro: 0.1755,Test Accuracy: 0.176, Test F1-macro: 0.172\n",
      "03/426: Val Loss: 2.6988, Val F1macro: 0.1633,Test Accuracy: 0.164, Test F1-macro: 0.161\n",
      "03/427: Val Loss: 2.6501, Val F1macro: 0.1795,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/428: Val Loss: 2.6780, Val F1macro: 0.1767,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/429: Val Loss: 2.6942, Val F1macro: 0.1706,Test Accuracy: 0.163, Test F1-macro: 0.168\n",
      "03/430: Val Loss: 2.7351, Val F1macro: 0.1613,Test Accuracy: 0.158, Test F1-macro: 0.157\n",
      "03/431: Val Loss: 2.6599, Val F1macro: 0.1643,Test Accuracy: 0.166, Test F1-macro: 0.157\n",
      "03/432: Val Loss: 2.7564, Val F1macro: 0.1793,Test Accuracy: 0.180, Test F1-macro: 0.173\n",
      "03/433: Val Loss: 2.7798, Val F1macro: 0.1776,Test Accuracy: 0.180, Test F1-macro: 0.173\n",
      "03/434: Val Loss: 2.7127, Val F1macro: 0.1805,Test Accuracy: 0.173, Test F1-macro: 0.169\n",
      "03/435: Val Loss: 2.6405, Val F1macro: 0.1746,Test Accuracy: 0.175, Test F1-macro: 0.168\n",
      "03/436: Val Loss: 2.6690, Val F1macro: 0.1627,Test Accuracy: 0.166, Test F1-macro: 0.159\n",
      "03/437: Val Loss: 2.6441, Val F1macro: 0.1676,Test Accuracy: 0.176, Test F1-macro: 0.162\n",
      "03/438: Val Loss: 2.6479, Val F1macro: 0.1810,Test Accuracy: 0.180, Test F1-macro: 0.175\n",
      "03/439: Val Loss: 2.7017, Val F1macro: 0.1701,Test Accuracy: 0.173, Test F1-macro: 0.166\n",
      "03/440: Val Loss: 2.6222, Val F1macro: 0.1792,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/441: Val Loss: 2.7606, Val F1macro: 0.1448,Test Accuracy: 0.131, Test F1-macro: 0.143\n",
      "03/442: Val Loss: 2.7427, Val F1macro: 0.1475,Test Accuracy: 0.128, Test F1-macro: 0.141\n",
      "03/443: Val Loss: 2.7433, Val F1macro: 0.1700,Test Accuracy: 0.175, Test F1-macro: 0.166\n",
      "03/444: Val Loss: 2.7992, Val F1macro: 0.1794,Test Accuracy: 0.173, Test F1-macro: 0.170\n",
      "03/445: Val Loss: 2.6753, Val F1macro: 0.1652,Test Accuracy: 0.162, Test F1-macro: 0.163\n",
      "03/446: Val Loss: 2.6540, Val F1macro: 0.1686,Test Accuracy: 0.176, Test F1-macro: 0.163\n",
      "03/447: Val Loss: 2.6847, Val F1macro: 0.1775,Test Accuracy: 0.175, Test F1-macro: 0.171\n",
      "03/448: Val Loss: 2.6288, Val F1macro: 0.1783,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/449: Val Loss: 2.7593, Val F1macro: 0.1795,Test Accuracy: 0.178, Test F1-macro: 0.172\n",
      "03/450: Val Loss: 2.6850, Val F1macro: 0.1651,Test Accuracy: 0.165, Test F1-macro: 0.167\n",
      "03/451: Val Loss: 2.8005, Val F1macro: 0.1658,Test Accuracy: 0.166, Test F1-macro: 0.166\n",
      "03/452: Val Loss: 2.7126, Val F1macro: 0.1790,Test Accuracy: 0.180, Test F1-macro: 0.174\n",
      "03/453: Val Loss: 2.6309, Val F1macro: 0.1797,Test Accuracy: 0.178, Test F1-macro: 0.172\n",
      "03/454: Val Loss: 2.7320, Val F1macro: 0.1782,Test Accuracy: 0.179, Test F1-macro: 0.174\n",
      "03/455: Val Loss: 2.6308, Val F1macro: 0.1799,Test Accuracy: 0.177, Test F1-macro: 0.171\n",
      "03/456: Val Loss: 3.0894, Val F1macro: 0.1687,Test Accuracy: 0.167, Test F1-macro: 0.161\n",
      "03/457: Val Loss: 2.9304, Val F1macro: 0.1807,Test Accuracy: 0.177, Test F1-macro: 0.170\n",
      "03/458: Val Loss: 2.8246, Val F1macro: 0.1691,Test Accuracy: 0.176, Test F1-macro: 0.163\n",
      "03/459: Val Loss: 2.7124, Val F1macro: 0.1783,Test Accuracy: 0.176, Test F1-macro: 0.174\n",
      "03/460: Val Loss: 2.6790, Val F1macro: 0.1781,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/461: Val Loss: 2.7414, Val F1macro: 0.1786,Test Accuracy: 0.180, Test F1-macro: 0.176\n",
      "03/462: Val Loss: 2.6244, Val F1macro: 0.1799,Test Accuracy: 0.177, Test F1-macro: 0.171\n",
      "03/463: Val Loss: 2.7274, Val F1macro: 0.1790,Test Accuracy: 0.182, Test F1-macro: 0.174\n",
      "03/464: Val Loss: 2.6433, Val F1macro: 0.1790,Test Accuracy: 0.175, Test F1-macro: 0.171\n",
      "03/465: Val Loss: 2.8369, Val F1macro: 0.1789,Test Accuracy: 0.174, Test F1-macro: 0.169\n",
      "03/466: Val Loss: 2.6236, Val F1macro: 0.1807,Test Accuracy: 0.176, Test F1-macro: 0.171\n",
      "03/467: Val Loss: 2.6808, Val F1macro: 0.1627,Test Accuracy: 0.167, Test F1-macro: 0.160\n",
      "03/468: Val Loss: 2.6733, Val F1macro: 0.1685,Test Accuracy: 0.169, Test F1-macro: 0.162\n",
      "03/469: Val Loss: 2.6635, Val F1macro: 0.1770,Test Accuracy: 0.178, Test F1-macro: 0.171\n",
      "03/470: Val Loss: 2.7340, Val F1macro: 0.1490,Test Accuracy: 0.132, Test F1-macro: 0.139\n",
      "03/471: Val Loss: 2.6631, Val F1macro: 0.1672,Test Accuracy: 0.170, Test F1-macro: 0.162\n",
      "03/472: Val Loss: 2.8463, Val F1macro: 0.1664,Test Accuracy: 0.170, Test F1-macro: 0.161\n",
      "03/473: Val Loss: 2.7923, Val F1macro: 0.1670,Test Accuracy: 0.174, Test F1-macro: 0.163\n",
      "03/474: Val Loss: 2.7548, Val F1macro: 0.1655,Test Accuracy: 0.175, Test F1-macro: 0.166\n",
      "03/475: Val Loss: 2.7933, Val F1macro: 0.1671,Test Accuracy: 0.177, Test F1-macro: 0.163\n",
      "03/476: Val Loss: 2.6852, Val F1macro: 0.1671,Test Accuracy: 0.178, Test F1-macro: 0.165\n",
      "03/477: Val Loss: 2.7780, Val F1macro: 0.1664,Test Accuracy: 0.175, Test F1-macro: 0.163\n",
      "03/478: Val Loss: 2.8133, Val F1macro: 0.1733,Test Accuracy: 0.175, Test F1-macro: 0.167\n",
      "03/479: Val Loss: 2.8389, Val F1macro: 0.1784,Test Accuracy: 0.179, Test F1-macro: 0.171\n",
      "03/480: Val Loss: 2.9055, Val F1macro: 0.1629,Test Accuracy: 0.156, Test F1-macro: 0.164\n",
      "03/481: Val Loss: 2.7848, Val F1macro: 0.1835,Test Accuracy: 0.176, Test F1-macro: 0.174\n",
      "03/482: Val Loss: 2.6919, Val F1macro: 0.1767,Test Accuracy: 0.179, Test F1-macro: 0.173\n",
      "03/483: Val Loss: 2.6267, Val F1macro: 0.1773,Test Accuracy: 0.177, Test F1-macro: 0.172\n",
      "03/484: Val Loss: 2.6921, Val F1macro: 0.1777,Test Accuracy: 0.175, Test F1-macro: 0.171\n",
      "03/485: Val Loss: 2.7726, Val F1macro: 0.1680,Test Accuracy: 0.162, Test F1-macro: 0.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/486: Val Loss: 2.7938, Val F1macro: 0.1723,Test Accuracy: 0.176, Test F1-macro: 0.167\n",
      "03/487: Val Loss: 2.7331, Val F1macro: 0.1769,Test Accuracy: 0.174, Test F1-macro: 0.170\n",
      "03/488: Val Loss: 2.6669, Val F1macro: 0.1780,Test Accuracy: 0.180, Test F1-macro: 0.173\n",
      "03/489: Val Loss: 2.7224, Val F1macro: 0.1784,Test Accuracy: 0.179, Test F1-macro: 0.172\n",
      "03/490: Val Loss: 2.8491, Val F1macro: 0.1720,Test Accuracy: 0.177, Test F1-macro: 0.168\n",
      "03/491: Val Loss: 2.8740, Val F1macro: 0.1627,Test Accuracy: 0.162, Test F1-macro: 0.161\n",
      "03/492: Val Loss: 2.6568, Val F1macro: 0.1648,Test Accuracy: 0.169, Test F1-macro: 0.162\n",
      "03/493: Val Loss: 2.7469, Val F1macro: 0.1661,Test Accuracy: 0.165, Test F1-macro: 0.171\n",
      "03/494: Val Loss: 2.7789, Val F1macro: 0.1775,Test Accuracy: 0.178, Test F1-macro: 0.171\n",
      "03/495: Val Loss: 2.7130, Val F1macro: 0.1812,Test Accuracy: 0.177, Test F1-macro: 0.174\n",
      "03/496: Val Loss: 2.9719, Val F1macro: 0.1635,Test Accuracy: 0.160, Test F1-macro: 0.161\n",
      "03/497: Val Loss: 2.7244, Val F1macro: 0.1667,Test Accuracy: 0.163, Test F1-macro: 0.164\n",
      "03/498: Val Loss: 149.4263, Val F1macro: 0.1766,Test Accuracy: 0.179, Test F1-macro: 0.171\n",
      "03/499: Val Loss: 2.6961, Val F1macro: 0.1689,Test Accuracy: 0.175, Test F1-macro: 0.164\n",
      "03/500: Val Loss: 2.6442, Val F1macro: 0.1686,Test Accuracy: 0.166, Test F1-macro: 0.157\n",
      "Val Loss: 2.6311, Val f1macro: 0.1658, Test Accuracy: 0.177 ± 0.008,Test F1-macro: 0.169 ± 0.033, Duration: 3941.636\n",
      "Best result - 0.177 ± 0.008\n",
      "-----\n",
      "assemblerv1 - GIN: 0.177 ± 0.008\n",
      "Best result - 0.177 ± 0.008\n",
      "-----\n",
      "assemblerv1 - GIN: 0.177 ± 0.008\n",
      "assemblerv1 - GIN: 0.177 ± 0.008\n"
     ]
    }
   ],
   "source": [
    "#V3 training\n",
    "# previous winner is 2layer 64 hidden units\n",
    "# now testing 2l 64h + GCN/GraphSAGE/GIN + 30,35,40 epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = []\n",
    "\n",
    "batch_size=[128,64,32]\n",
    "lr=[0.001,0.005,0.01]\n",
    "lr_decay_factor=0.5\n",
    "lr_decay_step_size=50\n",
    "layers = [ 2, ]\n",
    "hiddens = [64]\n",
    "nets = [   \n",
    "    ##GCNWithJK, \n",
    "    ##GraphSAGEWithJK,    \n",
    "    #GIN0WithJK,    \n",
    "    #GINWithJK,    \n",
    "    #GCN,    \n",
    "    #GraphSAGE,    \n",
    "    #GIN0,    \n",
    "    GIN,    \n",
    "    #GlobalAttentionNet,    \n",
    "    #Set2SetNet, \n",
    "    \n",
    "]\n",
    "\n",
    "best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "\n",
    "dataset = FunctionsDataset(root='./tmp/symbols_dataset_3_precomp_split_undersample_max2/training_set')\n",
    "dataset_name='assemblerv1'\n",
    "dataset.gnn_mode_on()\n",
    "num_classes=19\n",
    "\n",
    "\n",
    "\n",
    "for Net in nets:\n",
    "    for lr in [0.001]:\n",
    "        for batch_size in [32]:\n",
    "            for wd in [ 5e-4]:\n",
    "                for epochs in [500]:\n",
    "                    for num_layers, hidden in product(layers, hiddens):\n",
    "                        print(\"\\n\\n model:\", Net.__name__,\" epochs:\",epochs,\" num_layers:\",num_layers,\" hidden:\",hidden)\n",
    "                        # rewrite here & definition to pass num_classes and num_features manually\n",
    "                        model = Net(dataset, num_layers, hidden, num_classes, bow_dim=86)\n",
    "\n",
    "                        # verify how validation set is obtained\n",
    "                        loss, acc, std = cross_validation_with_val_set(\n",
    "                            dataset,\n",
    "                            model,\n",
    "                            folds=3,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            lr=lr,\n",
    "                            lr_decay_factor=lr_decay_factor,\n",
    "                            lr_decay_step_size=lr_decay_step_size,\n",
    "                            weight_decay=wd,\n",
    "                            logger=logger)\n",
    "\n",
    "                        if loss < best_result[0]:\n",
    "                            best_result = (loss, acc, std)\n",
    "\n",
    "                    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "                    print('Best result - {}'.format(desc))\n",
    "                    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "                    print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n",
    "desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "print('Best result - {}'.format(desc))\n",
    "results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
