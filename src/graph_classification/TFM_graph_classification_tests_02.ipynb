{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- train & compare differnt GNN models for graph classificaiton in common benchmarks (PPI, Proteins, ENZYMES,..)\n",
    "- compare results to publication results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a hack to use this kind of gnn layer with the stable version of\n",
    "PyTorch Geometric as of 2019-03-25\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter as Param\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "\n",
    "class GatedGraphConv(MessagePassing):\n",
    "    r\"\"\"The gated graph convolution operator from the `\"Gated Graph Sequence\n",
    "    Neural Networks\" <https://arxiv.org/abs/1511.05493>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{h}_i^{(0)} &= \\mathbf{x}_i \\, \\Vert \\, \\mathbf{0}\n",
    "\n",
    "        \\mathbf{m}_i^{(l+1)} &= \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{\\Theta}\n",
    "        \\cdot \\mathbf{h}_j^{(l)}\n",
    "\n",
    "        \\mathbf{h}_i^{(l+1)} &= \\textrm{GRU} (\\mathbf{m}_i^{(l+1)},\n",
    "        \\mathbf{h}_i^{(l)})\n",
    "\n",
    "    up to representation :math:`\\mathbf{h}_i^{(L)}`.\n",
    "    The number of input channels of :math:`\\mathbf{x}_i` needs to be less or\n",
    "    equal than :obj:`out_channels`.\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): Size of each input sample.\n",
    "        num_layers (int): The sequence length :math:`L`.\n",
    "        aggr (string): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels, num_layers, aggr='add', bias=True):\n",
    "        super(GatedGraphConv, self).__init__(aggr)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.weight = Param(Tensor(num_layers, out_channels, out_channels))\n",
    "        self.rnn = torch.nn.GRUCell(out_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.out_channels\n",
    "        uniform(size, self.weight)\n",
    "        self.rnn.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\"\"\"\n",
    "        h = x if x.dim() == 2 else x.unsqueeze(-1)\n",
    "        assert h.size(1) <= self.out_channels\n",
    "\n",
    "        if h.size(1) < self.out_channels:\n",
    "            zero = h.new_zeros(h.size(0), self.out_channels - h.size(1))\n",
    "            h = torch.cat([h, zero], dim=1)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            m = torch.matmul(h, self.weight[i])\n",
    "            # original master 1.0.3 (new version with problems when using rnn)\n",
    "            #m = self.propagate(edge_index, x=m)\n",
    "            # hacky version to use with the pip installation of pytorch-geometric 20190325\n",
    "            m = self.propagate('add',edge_index, x=m)\n",
    "            h = self.rnn(m, h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, num_layers={})'.format(\n",
    "            self.__class__.__name__, self.out_channels, self.num_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net1, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net2, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Net3(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20, d3=10,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net3, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, d3)\n",
    "        self.dense2_bn = nn.BatchNorm1d(d3)\n",
    "        self.fc3 = nn.Linear(d3, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.dense2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class Net4(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net4, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models for hyperparameter search\n",
    "model_list =[\n",
    "    {'epochs': 20,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 10,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    \n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "]\n",
    "\n",
    "model_list2 = []\n",
    "for modelclass in [Net1, Net2, Net3,Net4]:\n",
    "    for d1 in [25,50,100,200]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [100,200,300]:\n",
    "                    model_list2.append(\n",
    "                        {\n",
    "                        'model': modelclass,\n",
    "                        'epochs': epochs,\n",
    "                        'kwargs':{'d1': d1,'d2': d2,'num_layers':2, \n",
    "                                  'aggr_type':aggr_type}, \n",
    "                        'learning_rate': 0.01, 'weight_decay':5e-4, \n",
    "                        'batch_size': 32},\n",
    "                    )\n",
    "                    \n",
    "#model_list = model_list2\n",
    "model_list = model_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PRE, REC and F1\n",
    "def PRE(measuresdict):\n",
    "    m = measuresdict\n",
    "    measuresdict['PRE'] = float(m['TP'])/float(m['TP']+m['FP'])\n",
    "    \n",
    "def REC(m):\n",
    "    m['REC'] = float(m['TP'])/float(m['FN']+m['TP'])\n",
    "\n",
    "def F1(m):\n",
    "    m['F1']=2.0*(m['PRE']*m['REC'])/(m['PRE']+m['REC'])\n",
    "    \n",
    "def macroAndMicroScores(m):\n",
    "    # average all precisions\n",
    "    # average all recalls\n",
    "    # compute macroF1\n",
    "    macroPRE = 0.0\n",
    "    macroREC = 0.0\n",
    "    num_classes = 0\n",
    "    microPREnumerator = 0.0\n",
    "    microPREdenominator = 0.0\n",
    "    microRECnumerator = 0.0\n",
    "    microRECdenominator = 0.0\n",
    "    for k,v in m.items():\n",
    "        try:\n",
    "            a = int(k)\n",
    "            macroPRE+=m[k]['PRE']\n",
    "            macroREC+=m[k]['REC']\n",
    "            \n",
    "            microPREnumerator+=m[k]['TP']\n",
    "            microPREdenominator+=m[k]['TP']\n",
    "            microPREdenominator+=m[k]['FP']\n",
    "            \n",
    "            microRECnumerator+=m[k]['TP']\n",
    "            microRECdenominator+=m[k]['TP']\n",
    "            microRECdenominator+=m[k]['FN']\n",
    "            \n",
    "            num_classes+=1\n",
    "        except:\n",
    "            # only keys related to classes\n",
    "            # avoid macro and micro keys\n",
    "            pass\n",
    "        \n",
    "    macroPRE = macroPRE/float(num_classes)\n",
    "    macroREC = macroREC/float(num_classes)\n",
    "    macroF1 = 2.0*(macroPRE*macroREC)/(macroPRE+macroREC)\n",
    "    m['macroPRE'] = macroPRE\n",
    "    m['macroREC'] = macroREC\n",
    "    m['macroF1'] = macroF1\n",
    "    \n",
    "    microPRE = microPREnumerator/microPREdenominator\n",
    "    microREC = microRECnumerator/microRECdenominator\n",
    "    microF1 = 2.0*(microPRE*microREC)/(microPRE+microREC)\n",
    "    m['microPRE'] = microPRE\n",
    "    m['microREC'] = microREC\n",
    "    m['microF1'] = microF1\n",
    "    \n",
    "\n",
    "def F1Score(pred, target):\n",
    "    predset = set(pred)\n",
    "    targetset = set(target)\n",
    "    #print(predset)\n",
    "    #print(targetset)\n",
    "    num_classes = max(len(predset),len(targetset))\n",
    "\n",
    "    # for each class save pred_indices, and target_indices\n",
    "    preddict = { i:[] for i in range(num_classes) }\n",
    "    targetdict = { i:[] for i in range(num_classes) }\n",
    "    #print(preddict)\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        preddict[pred[i]].append(i)\n",
    "        targetdict[target[i]].append(i)\n",
    "\n",
    "    #print(\"preddict\", preddict)\n",
    "    #print(\"targetdict\", targetdict)\n",
    "\n",
    "    measures = { \n",
    "        i:{'TP':0, 'TN':0, 'FP':0, \n",
    "           'FN':0, 'PRE':0.0, 'REC': 0.0, 'F1':0.0} \n",
    "        for i in range(num_classes)}\n",
    "    for i in range(num_classes):\n",
    "        for j in range(len(preddict[i])):\n",
    "            if preddict[i][j] in targetdict[i]:\n",
    "                measures[i]['TP']+=1\n",
    "            else:\n",
    "                measures[i]['FP']+=1\n",
    "\n",
    "        for j in range(len(targetdict[i])):\n",
    "            if targetdict[i][j] not in preddict[i]:\n",
    "                measures[i]['FN']+=1\n",
    "\n",
    "        for j in range(len(pred)):\n",
    "            if pred[j] not in preddict[i] and pred[j] not in targetdict[i]:\n",
    "                measures[i]['TN']+=1\n",
    "\n",
    "    #print(\" single measures\",measures)\n",
    "    for k,mdict in measures.items():\n",
    "        try:\n",
    "            PRE(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute PRE on class \",k)\n",
    "            pass\n",
    "        try:\n",
    "            REC(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute REC on class \",k)\n",
    "            pass\n",
    "        try:\n",
    "            F1(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute F1 on class \",k)\n",
    "            pass\n",
    "\n",
    "    macroAndMicroScores(measures)\n",
    "\n",
    "    return measures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "# count how many graphs of each class in the dataset\n",
    "def printDatasetBalance(dataset):\n",
    "    num_classes = dataset.num_classes\n",
    "    class_counts = { i:0 for i in range(num_classes)}\n",
    "    #print(class_counts)\n",
    "    for graph in dataset:\n",
    "        class_counts[int(graph.y.item())]+=1\n",
    "    print(class_counts)\n",
    "    \n",
    "    \n",
    "def balancedDatasetSplit_list(dataset, prop):\n",
    "    \n",
    "    dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for graph in dataset:\n",
    "        datasets_byclass[int(graph.y.item())].append(graph)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_dataset.extend(datasets_byclass[c][:limit])\n",
    "        test_dataset.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "def balancedDatasetSplit_slice(dataset, prop):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    x=torch.Tensor([True,False,True])==True\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    train_dataset_slice = [False]*n\n",
    "    test_dataset_slice = [False]*n\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "    #print(\"datasets_byclass\", datasets_byclass)\n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(\"datasets_byclass\",datasets_byclass)\n",
    "        \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_list.extend(datasets_byclass[c][:limit])\n",
    "        test_list.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    #print(\"train_list\", train_list)\n",
    "    #print(\"test_list\", test_list)\n",
    "\n",
    "        \n",
    "    # now from list of integers(indices) to boolean mask tensor\n",
    "    #for i in range(len(train_list)):\n",
    "    #    real_index = train_list[i]\n",
    "    #    train_dataset_slice[real_index] = True\n",
    "        \n",
    "    #for i in range(len(test_list)):\n",
    "    #    real_index = test_list[i]\n",
    "    #    test_dataset_slice[real_index] = True\n",
    "        \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "        \n",
    "    train_dataset = dataset[torch.LongTensor(train_list)]\n",
    "    test_dataset = dataset[torch.LongTensor(test_list)]\n",
    "        \n",
    "    #print(\"train_dataset\", train_dataset)\n",
    "    #print(\"test_dataset\", test_dataset)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def balancedDatasetKfoldSplit_slice(dataset,k):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    \n",
    "    foldsize = int(n/k)\n",
    "    num_classes = dataset.num_classes\n",
    "    num_items_x_class = int(foldsize/num_classes)\n",
    "    \n",
    "    # list of items for each class\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(datasets_byclass)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append([])\n",
    "        for c in range(num_classes):\n",
    "            for j in range(num_items_x_class):\n",
    "                index = datasets_byclass[c].pop()\n",
    "                folds[i].append(index)\n",
    "        \n",
    "    # returns a list of list of indices\n",
    "    return folds\n",
    "\n",
    "def kFolding(train_dataset, k):\n",
    "    n = len(train_dataset)\n",
    "    fold_size = int(n/k)\n",
    "    \n",
    "    # build folds\n",
    "    #folds = []\n",
    "    #for i in range(k):\n",
    "    #    i1 = i*fold_size\n",
    "    #    i2 = i1+fold_size\n",
    "    #    folds.append((i1,i2))\n",
    "    #print(folds)\n",
    "    \n",
    "    # build train-val sets\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        preval_index = (0,i*fold_size)\n",
    "        val_index = (i*fold_size,i*fold_size+fold_size)\n",
    "        postval_index = (i*fold_size+fold_size,n)\n",
    "        train_sets.append((preval_index, val_index, postval_index))\n",
    "        \n",
    "    #print(train_sets)\n",
    "    return train_sets\n",
    "\n",
    "def kFolding2(train_dataset, k):\n",
    "\n",
    "    #print(\" train_dataset len:\", len(train_dataset))\n",
    "    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        # each train_set must have a torch.LongTensor for train indices\n",
    "        # and a torch.LongTensor for val indices\n",
    "        val_merge = folds[i]\n",
    "        train_merge = [] \n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train_merge.extend(folds[j])\n",
    "        train_sets.append((torch.LongTensor(train_merge), torch.LongTensor(val_merge)))\n",
    "    \n",
    "    return train_sets\n",
    "    \n",
    "\n",
    "\n",
    "def accuracy(pred, batch):\n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, train_loss_history):\n",
    "    global device \n",
    "    \n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    total_num_graphs = 0\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(out, target)\n",
    "        loss_train +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_train = loss_train /total_num_graphs\n",
    "    train_loss_history.append(loss_train.item()) \n",
    "    \n",
    "def val_loss_model(model, loader, optimizer, val_history):\n",
    "    global device \n",
    "    \n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    total_num_graphs = 0\n",
    "    total_pred = []\n",
    "    total_acc = []\n",
    "    total_gt = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        total_pred.extend(pred.flatten().tolist())\n",
    "        total_gt.extend(batch.y.flatten().tolist())\n",
    "        \n",
    "        _, predacc = pred.max(dim=1)\n",
    "        total_acc.extend(predacc.flatten().tolist())\n",
    "        \n",
    "        target = data.y\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        loss_val += loss\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_val = loss_val / total_num_graphs\n",
    "    val_history['loss'].append(loss_val.item())\n",
    "    \n",
    "    # accuracy needs correction\n",
    "    val_history['accuracy'].append(accuracy(predacc, batch))\n",
    "    \n",
    "    # compute F1 scores\n",
    "    #pred2 = pred.to('cpu')\n",
    "    #pred2 = pred2.flatten().tolist()\n",
    "    #target = batch.y.to('cpu')\n",
    "    #target = target.flatten().tolist()\n",
    "    \n",
    "    #print(\"total_acc\",total_acc)\n",
    "    #print(\"total_gt\",total_gt)\n",
    "    measures = F1Score(total_acc, total_gt)\n",
    "    val_history['microF1'].append(measures['microF1'])\n",
    "    val_history['macroF1'].append(measures['macroF1'])\n",
    "\n",
    "# Retrain the best model\n",
    "def final_model_train(modeldict, train_dataset):\n",
    "    global device \n",
    "    \n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history=[]\n",
    "    \n",
    "    lr = modeldict['learning_rate']\n",
    "    wd = modeldict['weight_decay']\n",
    "    bs = modeldict['batch_size']\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saving\n",
    "\n",
    "def modelSaveName(modeldict):\n",
    "    classname = modeldict['model'].__class__.__name__\n",
    "    architecture = \"\"\n",
    "    for k,v in modeldict['kwargs'].items():\n",
    "        architecture = architecture+'_'+str(k)+'-'+str(v)\n",
    "    \n",
    "    epochs = modeldict['epochs']\n",
    "    lr = modeldict['learning_rate']\n",
    "    wd = modeldict['weight_decay']\n",
    "    bs = modeldict['batch_size']\n",
    "    \n",
    "    finalname = classname + \"_\" + architecture + \"_\" + \\\n",
    "                str(epochs) + \"_\" + str(lr) + \\\n",
    "                str(wd) + \"_\" + str(bs)\n",
    "    return finalname\n",
    "\n",
    "def saveModel(modeldict):\n",
    "    import traceback\n",
    "    \n",
    "    \"\"\"\n",
    "        based on :\n",
    "        https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if not os.path.exists('./models'):\n",
    "            os.mkdir('./models')\n",
    "        \n",
    "        # model naming convention\n",
    "        model = modeldict['model_instance']\n",
    "        path = './models/'+modelSaveName(modeldict)\n",
    "            \n",
    "        # save operation\n",
    "        torch.save(model.state_dict(),path)\n",
    "            \n",
    "        return path\n",
    "    except Exception as err:\n",
    "        print(\"ERROR SAVING MODEL \"+model.__name__)\n",
    "        print(err)\n",
    "        \n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "        \n",
    "def loadModel(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "def testSavingLoadingModel(train_dataset, test_dataset):\n",
    "\n",
    "    global device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # create model\n",
    "    m1 = {'epochs': 20,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 10}\n",
    "\n",
    "    # train model\n",
    "    model = final_model_train(m1, train_dataset)\n",
    "    \n",
    "\n",
    "    # test model and print accuracy\n",
    "    testresult = testModel(model, test_dataset)\n",
    "\n",
    "    # save model\n",
    "    m1['model_instance']=model\n",
    "    path = saveModel(m1)\n",
    "\n",
    "    # create new similar model\n",
    "    m2 = {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32}\n",
    "        \n",
    "    epochs = m2['epochs']\n",
    "    modelclass = m2['model']\n",
    "    kwargs = m2['kwargs']\n",
    "    model2 = modelclass(**kwargs)\n",
    "    model2 = model2.to(device)\n",
    "    m2['model_instance']=model2\n",
    "    \n",
    "\n",
    "    # load state_dict\n",
    "    print(\"path\", path)\n",
    "    if path is not None:\n",
    "        loadModel(model2, path)\n",
    "\n",
    "        # test new model and print accuracy\n",
    "        testresult = testModel(model2, test_dataset)\n",
    "\n",
    "\n",
    "    \n",
    "def saveModels(modelsdict):\n",
    "    for k,model in modelsdict['best_models'].items():\n",
    "        saveModel(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTrainedModel(modeldict):\n",
    "    print(\" trained model: \",modeldict['model'].__class__.__name__,\n",
    "              modeldict['kwargs'], \" epochs:\",modeldict['epochs'],\n",
    "             ' val loss=',modeldict['cv_val_loss'],\n",
    "          ' val accuracy=',modeldict['cv_val_accuracy'],\n",
    "         ' val microF1=',modeldict['cv_val_microF1'],\n",
    "          ' val macroF1=',modeldict['cv_val_macroF1'])\n",
    "    \n",
    "def selectBestModel(model_list):\n",
    "    # select the best model (lower validation loss)\n",
    "    losses = np.array([ modeldict['cv_val_loss'] for modeldict in model_list])\n",
    "    accuracies = np.array([ modeldict['cv_val_accuracy'] for modeldict in model_list])\n",
    "    microF1 = np.array([ modeldict['cv_val_microF1'] for modeldict in model_list])\n",
    "    macroF1 = np.array([ modeldict['cv_val_macroF1'] for modeldict in model_list])\n",
    "    best_model_loss = model_list[np.argmin(losses)]\n",
    "    best_model_acc = model_list[np.argmax(accuracies)]\n",
    "    best_model_microF1 = model_list[np.argmax(microF1)]\n",
    "    best_model_macroF1 = model_list[np.argmax(macroF1)]\n",
    "    \n",
    "    # save selections to model_list\n",
    "    modelsdict = {}\n",
    "    modelsdict['models'] = model_list\n",
    "    modelsdict['best_models']={}\n",
    "    modelsdict['best_models']['loss'] = best_model_loss\n",
    "    modelsdict['best_models']['accuracy'] = best_model_acc\n",
    "    modelsdict['best_models']['microF1'] = best_model_microF1\n",
    "    modelsdict['best_models']['macroF1'] = best_model_macroF1\n",
    "\n",
    "    modelsdict['testing']={}\n",
    "    \n",
    "    return modelsdict\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def modelSelection(model_list,k, train_dataset ):    \n",
    "\n",
    "    global device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    kfolds = kFolding2(train_dataset,k)\n",
    "\n",
    "    for modeldict in model_list:\n",
    "\n",
    "        epochs = modeldict['epochs']\n",
    "        modelclass = modeldict['model']\n",
    "        kwargs = modeldict['kwargs']\n",
    "        model = modelclass(**kwargs)\n",
    "        model = model.to(device)\n",
    "        modeldict['model_instance'] = model\n",
    "        \n",
    "        lr = modeldict['learning_rate']\n",
    "        wd = modeldict['weight_decay']\n",
    "        bs = modeldict['batch_size']\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_history = {'loss':[], 'accuracy':[], 'microF1':[],'macroF1':[]}\n",
    "        modeldict['cv_val_loss']=0.0\n",
    "        modeldict['cv_val_accuracy']=0.0\n",
    "        modeldict['cv_val_microF1'] =0.0\n",
    "        modeldict['cv_val_macroF1'] =0.0\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        for kfold in kfolds:\n",
    "\n",
    "            train = train_dataset[kfold[0]]\n",
    "            val = train_dataset[kfold[1]]\n",
    "            loader = DataLoader(train, batch_size=bs, shuffle=True)\n",
    "            loader_val = DataLoader(val, batch_size=bs, shuffle=True)\n",
    "            for epoch in range(epochs):\n",
    "                train_model(model, loader, optimizer, train_loss_history)\n",
    "                val_loss_model(model, loader_val, optimizer, val_history)\n",
    "\n",
    "            # save results\n",
    "            modeldict['train_loss_history']=train_loss_history\n",
    "            modeldict['val_loss_history']=val_history['loss']\n",
    "            modeldict['val_accuracy_history']=val_history['accuracy']\n",
    "            modeldict['val_loss']=val_history['loss'][-1]\n",
    "            modeldict['accuracy']=val_history['accuracy'][-1]\n",
    "            modeldict['microF1']=val_history['microF1'][-1]\n",
    "            modeldict['macroF1']=val_history['macroF1'][-1]\n",
    "\n",
    "            modeldict['cv_val_loss']+=modeldict['val_loss']\n",
    "            modeldict['cv_val_accuracy']+=modeldict['accuracy']\n",
    "            modeldict['cv_val_microF1']+=modeldict['microF1']\n",
    "            modeldict['cv_val_macroF1']+=modeldict['macroF1']\n",
    "\n",
    "        modeldict['cv_val_loss']=modeldict['cv_val_loss']/len(kfolds)\n",
    "        modeldict['cv_val_accuracy']=modeldict['cv_val_accuracy']/len(kfolds)\n",
    "        modeldict['cv_val_microF1']=modeldict['cv_val_microF1']/len(kfolds)\n",
    "        modeldict['cv_val_macroF1']=modeldict['cv_val_macroF1']/len(kfolds)\n",
    "        \n",
    "        # report model results\n",
    "        reportTrainedModel(modeldict)\n",
    "        \n",
    "\n",
    "        \n",
    "    # select best model\n",
    "    modelsdict = selectBestModel(model_list)\n",
    "    \n",
    "    # save model to disk + save file path    \n",
    "    # or save model in the dict.. (could take too much memory)\n",
    "    saveModels(modelsdict)\n",
    "    \n",
    "    return modelsdict\n",
    "        \n",
    "\n",
    "    \n",
    "def reportModelSelectionResult(modeldict):\n",
    "    best_model_loss = modeldict['best_models']['loss']\n",
    "    best_model_acc = modeldict['best_models']['accuracy']\n",
    "    best_model_microF1 = modeldict['best_models']['microF1']\n",
    "    best_model_macroF1 = modeldict['best_models']['macroF1']\n",
    "    \n",
    "    print(\"\\n selected model from loss: \",best_model_loss['model'].__name__,\n",
    "      best_model_loss['kwargs'],\" epochs:\", best_model_loss['epochs'], \n",
    "      best_model_loss['cv_val_loss'], best_model_loss['cv_val_accuracy'], \n",
    "      best_model_loss['cv_val_microF1'], best_model_loss['cv_val_macroF1'])\n",
    "    print(\" selected model from accuracy: \",best_model_acc['model'].__name__,\n",
    "          best_model_acc['kwargs'],\" epochs:\",best_model_acc['epochs'],  \n",
    "          best_model_acc['cv_val_loss'], best_model_acc['cv_val_accuracy'], \n",
    "          best_model_loss['cv_val_microF1'], best_model_loss['cv_val_macroF1'])\n",
    "    print(\" selected model from microF1: \",best_model_microF1['model'].__name__,best_model_microF1['kwargs'],\n",
    "          \" epochs:\", best_model_microF1['epochs'],  \n",
    "          best_model_microF1['cv_val_loss'], best_model_microF1['cv_val_accuracy'], \n",
    "          best_model_microF1['cv_val_microF1'], best_model_microF1['cv_val_macroF1'])\n",
    "\n",
    "    print(\" selected model from macroF1: \",best_model_macroF1['model'].__name__,best_model_macroF1['kwargs'],\n",
    "          \" epochs:\", best_model_macroF1['epochs'],  \n",
    "          best_model_macroF1['cv_val_loss'], best_model_macroF1['cv_val_accuracy'], \n",
    "          best_model_macroF1['cv_val_microF1'], best_model_macroF1['cv_val_macroF1'])\n",
    "    \n",
    "    # report with Pandas table\n",
    "    res = pd.DataFrame({\n",
    "        'best_model_loss': best_model_loss, \n",
    "        'best_model_acc' : best_model_acc, \n",
    "        'best_model_microF1' : best_model_microF1, \n",
    "        'best_model_macroF1': best_model_macroF1})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reportTest(batch, pred, measures, test_dataset):\n",
    "    print(\"len(test_dataset): \", len(test_dataset))\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    print('Accuracy: {:.4f}'.format(measures['accuracy']),\" macroF1:\",measures['macroF1'], \" microF1:\", measures['microF1'])\n",
    "\n",
    "def testModel(model, test_dataset):\n",
    "    global device\n",
    "    \n",
    "    model.eval()\n",
    "    loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        #_, pred = model(test_dataset).max(dim=1)\n",
    "        _, pred = model(batch).max(dim=1)\n",
    "        acc = accuracy(pred, batch)    \n",
    "        pred2 = pred.to('cpu')\n",
    "        pred2 = pred2.flatten().tolist()\n",
    "        target = batch.y.to('cpu')\n",
    "        target = target.flatten().tolist()\n",
    "        measures = F1Score(pred2, target)\n",
    "        measures['accuracy']=acc\n",
    "        \n",
    "        reportTest(batch, pred, measures, test_dataset)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return measures\n",
    "\n",
    "def reportAllTest(modelsdict):\n",
    "    reportDict = [{'name':k, \n",
    "                   'accuracy': v['accuracy'],\n",
    "                  'macroF1': v['macroF1'],\n",
    "                  'microF1': v['microF1']} for k,v in modelsdict['testing'].items()]\n",
    "    #print(reportDict)\n",
    "    res = pd.DataFrame(reportDict)\n",
    "    display(res)\n",
    "        \n",
    "def saveResults(modelsdict):\n",
    "    import json\n",
    "    import datetime\n",
    "    import os\n",
    "    import copy\n",
    "    \n",
    "    savedict = {}\n",
    "    for model in modelsdict['models']:\n",
    "        savedict['models']=[]\n",
    "        #v2['train_loss_history']=[]\n",
    "        #v2['val_loss_history']=[]\n",
    "        #v2['val_accuracy_history']=[]\n",
    "        mod = copy.deepcopy(model)\n",
    "        mod['model']=model['model'].__name__\n",
    "        mod['model_instance']=model['model_instance'].__class__.__name__\n",
    "        savedict['models'].append(mod)\n",
    "        \n",
    "    savedict['best_models']={}\n",
    "    for k,v2 in modelsdict['best_models'].items():\n",
    "        #v2['train_loss_history']=[]\n",
    "        #v2['val_loss_history']=[]\n",
    "        #v2['val_accuracy_history']=[]\n",
    "        savedict['best_models'][k]=copy.deepcopy(v2)\n",
    "        savedict['best_models'][k]['model'] =v2['model'].__name__\n",
    "        savedict['best_models'][k]['model_instance'] =v2['model_instance'].__class__.__name__\n",
    "            \n",
    "    savedict['tests'] = modelsdict['testing']\n",
    "            \n",
    "    \n",
    "    \n",
    "    d = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S') \n",
    "    if not os.path.exists('./results'):\n",
    "        os.mkdir('./results')\n",
    "    results_file = './results/experiment_'+d\n",
    "    \n",
    "    with open(results_file, 'w') as outfile:\n",
    "        json.dump(savedict, outfile)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 3, 4, 1, 4, 3, 3, 4, 3, 2, 2, 1, 3, 1, 4, 2, 2, 4, 1, 3, 5, 1, 4, 4,\n",
      "        2, 5, 5, 4, 4, 3, 2, 1, 4, 3, 4, 1, 4, 3, 1, 2, 3, 5, 5, 4, 2, 2, 2, 4,\n",
      "        4, 2, 2, 3, 4, 4, 4, 2, 2, 3, 2, 4, 1, 4, 4, 2, 2, 3, 4, 3, 1, 3, 3, 4,\n",
      "        3, 4, 4, 3, 1, 5, 1, 2, 3, 4, 3, 4, 3, 1, 1, 1, 3, 3, 2, 1, 4, 1, 1, 5,\n",
      "        1, 2, 5, 4, 2, 2, 4, 1, 3, 3, 5, 4, 4, 4, 1, 3, 5, 1, 1, 3, 3, 3, 2, 5],\n",
      "       device='cuda:0')\n",
      "tensor([3, 3, 1, 1, 5, 3, 4, 5, 1, 1, 3, 5, 5, 0, 0, 1, 2, 3, 3, 3, 5, 1, 0, 5,\n",
      "        3, 5, 5, 1, 5, 3, 1, 1, 0, 3, 3, 4, 2, 2, 0, 1, 3, 2, 5, 0, 2, 4, 2, 5,\n",
      "        0, 1, 2, 3, 4, 3, 4, 4, 3, 5, 3, 5, 0, 0, 3, 4, 2, 5, 2, 0, 2, 1, 2, 4,\n",
      "        2, 1, 4, 5, 1, 2, 1, 0, 0, 1, 4, 3, 1, 0, 5, 0, 0, 4, 2, 2, 2, 5, 1, 2,\n",
      "        4, 2, 5, 0, 5, 1, 4, 0, 4, 0, 2, 4, 1, 4, 3, 4, 5, 4, 0, 3, 2, 0, 4, 4],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2833  macroF1: 0.2705480344811855  microF1: 0.2833333333333333\n",
      "path ./models/Net1__d1-50_d2-20_num_layers-2_aggr_type-mean_200_0.010.0005_32\n",
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([4, 2, 3, 1, 4, 5, 4, 2, 2, 5, 4, 3, 4, 5, 3, 1, 3, 4, 1, 3, 2, 2, 4, 2,\n",
      "        2, 4, 1, 1, 3, 1, 2, 4, 3, 5, 3, 3, 3, 2, 1, 5, 2, 2, 3, 2, 4, 1, 3, 2,\n",
      "        3, 4, 4, 1, 1, 3, 3, 2, 3, 1, 3, 2, 4, 1, 3, 5, 1, 1, 4, 4, 4, 4, 1, 1,\n",
      "        3, 2, 2, 4, 2, 1, 4, 5, 1, 2, 4, 5, 2, 4, 3, 4, 4, 4, 2, 3, 4, 3, 4, 2,\n",
      "        4, 5, 3, 3, 4, 1, 3, 5, 5, 4, 3, 3, 3, 1, 5, 2, 1, 4, 3, 4, 1, 1, 4, 4],\n",
      "       device='cuda:0')\n",
      "tensor([2, 2, 2, 0, 5, 5, 5, 2, 1, 5, 0, 1, 5, 5, 5, 1, 1, 3, 5, 3, 4, 3, 0, 4,\n",
      "        2, 5, 1, 4, 0, 3, 2, 1, 4, 2, 3, 4, 2, 3, 2, 5, 2, 1, 3, 5, 0, 0, 0, 1,\n",
      "        1, 3, 1, 0, 1, 5, 4, 1, 0, 1, 2, 3, 1, 1, 4, 5, 0, 0, 4, 4, 0, 0, 1, 5,\n",
      "        3, 4, 2, 4, 3, 3, 4, 3, 4, 1, 1, 2, 4, 3, 4, 4, 4, 3, 2, 3, 2, 3, 0, 1,\n",
      "        2, 4, 3, 0, 0, 0, 5, 2, 2, 5, 2, 5, 3, 2, 5, 0, 4, 3, 0, 4, 5, 0, 1, 5],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2833  macroF1: 0.2705480344811855  microF1: 0.2833333333333333\n"
     ]
    }
   ],
   "source": [
    "# testing saving Model\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "testSavingLoadingModel(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 600  k folds= 3\n",
      "Datasets balancing: \n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "{0: 80, 1: 80, 2: 80, 3: 80, 4: 80, 5: 80}\n",
      "{0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20}\n",
      "\n",
      " trained model:  type {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 20  val loss= 0.05577197546760241  val accuracy= 0.2261904761904762  val microF1= 0.24358974358974358  val macroF1= 0.24785810387830406\n",
      " trained model:  type {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 10  val loss= 0.05495597794651985  val accuracy= 0.36904761904761907  val microF1= 0.2884615384615385  val macroF1= 0.28021755324532654\n",
      "\n",
      " selected model from loss:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 10 0.05495597794651985 0.36904761904761907 0.2884615384615385 0.28021755324532654\n",
      " selected model from accuracy:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 10 0.05495597794651985 0.36904761904761907 0.2884615384615385 0.28021755324532654\n",
      " selected model from microF1:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 10 0.05495597794651985 0.36904761904761907 0.2884615384615385 0.28021755324532654\n",
      " selected model from macroF1:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 10 0.05495597794651985 0.36904761904761907 0.2884615384615385 0.28021755324532654\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 5, 2, 2, 2, 5, 5, 2, 5, 2, 2, 5, 5, 5, 5, 2, 5, 2, 2, 5, 5, 5, 5, 5,\n",
      "        5, 2, 2, 5, 5, 2, 2, 5, 4, 5, 5, 5, 4, 2, 2, 2, 5, 5, 5, 2, 5, 2, 5, 5,\n",
      "        5, 2, 5, 5, 2, 5, 5, 5, 2, 4, 4, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2,\n",
      "        2, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 2,\n",
      "        2, 5, 5, 4, 5, 2, 5, 2, 2, 2, 2, 5, 2, 2, 4, 2, 2, 2, 5, 5, 2, 2, 2, 5],\n",
      "       device='cuda:0')\n",
      "tensor([1, 5, 4, 1, 3, 0, 3, 3, 4, 4, 3, 5, 3, 1, 4, 4, 5, 2, 5, 1, 3, 2, 3, 0,\n",
      "        4, 4, 1, 3, 3, 1, 2, 3, 5, 3, 1, 2, 1, 0, 4, 4, 5, 1, 0, 4, 0, 4, 0, 0,\n",
      "        5, 2, 0, 5, 0, 3, 3, 5, 2, 2, 5, 4, 2, 1, 1, 4, 5, 0, 5, 1, 5, 3, 2, 0,\n",
      "        4, 5, 0, 1, 4, 4, 2, 4, 1, 5, 3, 0, 4, 5, 2, 1, 1, 2, 4, 1, 0, 0, 5, 2,\n",
      "        0, 0, 5, 3, 0, 0, 2, 3, 0, 5, 3, 5, 2, 2, 3, 4, 2, 2, 1, 1, 1, 2, 2, 3],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2500  macroF1: 0.13175333281359633  microF1: 0.25\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([1, 5, 3, 5, 2, 1, 4, 5, 2, 5, 5, 5, 5, 3, 1, 4, 5, 5, 2, 2, 5, 1, 5, 5,\n",
      "        2, 2, 2, 5, 2, 3, 5, 2, 2, 1, 2, 3, 1, 5, 3, 5, 5, 5, 5, 3, 2, 1, 1, 5,\n",
      "        1, 2, 2, 5, 4, 4, 4, 2, 3, 2, 4, 5, 5, 4, 5, 2, 5, 5, 2, 5, 5, 5, 5, 5,\n",
      "        1, 1, 1, 5, 5, 5, 5, 1, 5, 2, 2, 2, 5, 0, 2, 4, 5, 5, 1, 1, 1, 5, 5, 2,\n",
      "        3, 1, 5, 5, 5, 5, 5, 2, 5, 1, 1, 2, 2, 2, 2, 5, 5, 3, 2, 2, 5, 2, 2, 2],\n",
      "       device='cuda:0')\n",
      "tensor([1, 4, 2, 2, 4, 1, 5, 4, 5, 5, 4, 1, 0, 2, 2, 3, 1, 4, 3, 4, 5, 5, 3, 1,\n",
      "        3, 3, 4, 3, 2, 1, 3, 0, 4, 5, 2, 4, 3, 3, 1, 5, 5, 3, 1, 5, 4, 0, 1, 3,\n",
      "        4, 3, 0, 5, 1, 4, 1, 3, 5, 0, 2, 2, 4, 4, 1, 1, 5, 4, 2, 1, 0, 2, 4, 0,\n",
      "        5, 0, 5, 4, 3, 5, 5, 1, 3, 0, 2, 2, 5, 2, 2, 0, 5, 0, 3, 0, 1, 0, 3, 2,\n",
      "        1, 3, 2, 1, 4, 1, 0, 4, 3, 0, 0, 5, 0, 0, 2, 5, 0, 3, 2, 0, 1, 2, 2, 4],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2500  macroF1: 0.2198986611920414  microF1: 0.25\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([2, 0, 2, 4, 4, 2, 1, 2, 2, 0, 2, 5, 5, 2, 2, 4, 4, 2, 2, 2, 5, 2, 1, 1,\n",
      "        3, 3, 3, 0, 0, 4, 3, 4, 2, 2, 2, 0, 4, 0, 1, 4, 4, 0, 2, 4, 1, 2, 0, 0,\n",
      "        2, 4, 2, 1, 2, 2, 4, 2, 4, 5, 4, 3, 4, 1, 0, 2, 1, 2, 2, 2, 0, 2, 2, 2,\n",
      "        0, 3, 0, 2, 2, 2, 5, 3, 2, 4, 2, 2, 0, 4, 3, 4, 2, 4, 2, 5, 1, 2, 1, 0,\n",
      "        0, 4, 2, 2, 2, 0, 4, 2, 3, 2, 2, 3, 4, 0, 4, 0, 4, 0, 0, 4, 2, 1, 2, 4],\n",
      "       device='cuda:0')\n",
      "tensor([2, 5, 0, 5, 1, 4, 5, 2, 0, 3, 4, 1, 4, 5, 5, 4, 4, 0, 1, 4, 5, 2, 1, 4,\n",
      "        1, 3, 1, 5, 0, 0, 0, 5, 4, 4, 1, 0, 5, 1, 2, 5, 4, 0, 4, 1, 1, 5, 0, 1,\n",
      "        4, 0, 2, 3, 3, 2, 5, 2, 3, 5, 2, 4, 3, 3, 0, 2, 1, 0, 4, 2, 5, 2, 1, 0,\n",
      "        1, 3, 5, 1, 2, 3, 5, 3, 2, 4, 2, 2, 3, 2, 3, 4, 0, 1, 5, 4, 3, 1, 0, 2,\n",
      "        0, 5, 5, 2, 4, 1, 0, 3, 4, 2, 3, 2, 3, 5, 3, 0, 4, 0, 3, 3, 1, 0, 1, 3],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.3250  macroF1: 0.33784907961875815  microF1: 0.325\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 2, 5, 2, 5, 5, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 1, 5, 5, 2, 5, 5,\n",
      "        5, 2, 5, 2, 1, 2, 2, 2, 5, 1, 2, 1, 5, 2, 5, 2, 1, 5, 5, 5, 5, 2, 1, 5,\n",
      "        5, 2, 5, 2, 5, 1, 1, 5, 5, 5, 5, 5, 2, 2, 5, 5, 2, 5, 2, 2, 5, 5, 2, 2,\n",
      "        5, 5, 5, 2, 2, 5, 5, 1, 2, 5, 5, 5, 5, 2, 2, 1, 5, 5, 5, 2, 5, 5, 2, 5,\n",
      "        2, 5, 5, 5, 1, 2, 5, 1, 5, 2, 5, 5, 5, 2, 5, 2, 5, 2, 2, 2, 5, 5, 5, 5],\n",
      "       device='cuda:0')\n",
      "tensor([1, 2, 5, 0, 2, 3, 4, 4, 3, 0, 5, 1, 2, 1, 0, 4, 1, 4, 2, 5, 5, 2, 5, 4,\n",
      "        3, 5, 5, 2, 2, 4, 3, 1, 3, 0, 2, 4, 0, 1, 1, 4, 2, 0, 0, 4, 3, 3, 3, 1,\n",
      "        5, 2, 3, 4, 4, 1, 1, 1, 5, 4, 1, 4, 4, 2, 5, 5, 0, 3, 4, 3, 0, 0, 1, 4,\n",
      "        1, 5, 1, 3, 0, 3, 3, 5, 2, 1, 0, 5, 3, 4, 0, 2, 5, 5, 0, 0, 0, 1, 2, 4,\n",
      "        3, 2, 0, 1, 4, 2, 2, 5, 5, 2, 0, 5, 2, 3, 1, 2, 3, 0, 4, 0, 1, 3, 5, 3],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2500  macroF1: 0.15832910106653122  microF1: 0.25\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macroF1</th>\n",
       "      <th>microF1</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.131753</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Net1loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.219899</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Net1accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.337849</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Net1microF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.158329</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Net1macroF1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   macroF1  microF1          name\n",
       "0     0.250  0.131753    0.250      Net1loss\n",
       "1     0.250  0.219899    0.250  Net1accuracy\n",
       "2     0.325  0.337849    0.325   Net1microF1\n",
       "3     0.250  0.158329    0.250   Net1macroF1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. encapsulate all training, model selection,.. everything\n",
    "2. present results with Pandas tables, and histograms\n",
    "3. save models and results to disk, and load them later for testing\n",
    "\n",
    "### Pending:\n",
    "- transform into a python module or package\n",
    "- prepare another notebook using the python module (prepare local and on collab)\n",
    "- test other GNN layers: GAT, GCN, GraphSAGE, Metalayer\n",
    "- do a good HP search\n",
    "- look for published architectures?\n",
    "- compare with published benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
