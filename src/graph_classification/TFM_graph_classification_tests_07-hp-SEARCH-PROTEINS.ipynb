{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- train & compare differnt GNN models for graph classificaiton in common benchmarks (PPI, Proteins, ENZYMES,..)\n",
    "- compare results to publication results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "#from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "from torch_geometric.datasets import TUDataset, QM9, QM7b, PPI, Planetoid, KarateClub\n",
    "\n",
    "\n",
    "from TFM_graph_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GGNN1(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN1, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GGNN2(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN2, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class GGNN3(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20, d3=10,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN3, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, d3)\n",
    "        self.dense2_bn = nn.BatchNorm1d(d3)\n",
    "        self.fc3 = nn.Linear(d3, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.dense2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class GGNN4(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN4, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class META1(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META1, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "        # version using only u\n",
    "        x = F.relu(self.dense1_bn(self.fc1(u2)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    \n",
    "class META2(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META2, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "        \n",
    "        # version using only x \n",
    "        x = self.global_pool(x2,batch) # separate by graph level\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class META3(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META3, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "\n",
    "        # version using x and  u\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        ubatch = list(set([ elem.item() for elem in batch]))\n",
    "        #print(ubatch)\n",
    "        x = self.global_pool(x, torch.cat([batch,torch.LongTensor( ubatch).to(device) ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class META4(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META4, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "    \n",
    "        # version using x and  u and edge_attr\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        #x = x2\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, torch.cat([batch, ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class META5(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Not using edge attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, d1=3, d2=50, d3=15):\n",
    "        super(META5, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*2, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(2, d2), ReLU(), Lin(d2, d3))\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            out = torch.cat([source, target], dim=1)\n",
    "            #print(\"edge_model\")\n",
    "            #print(out.size())\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            out = torch.cat([x[col]], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        #print(\"Forward: \")\n",
    "        #print(x.size())\n",
    "        return self.op(x, edge_index, edge_attr, u, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_list21 = []\n",
    "for modelclass in [GGNN1]:\n",
    "    for d1 in [25,50]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [300,600]:\n",
    "                    for num_layers in [2,4,8]:\n",
    "                        for lr in [0.01]:\n",
    "                            for wd in [5e-4]:\n",
    "                                model_list21.append(\n",
    "                                    {\n",
    "                                    'model': modelclass,\n",
    "                                    'epochs': epochs,\n",
    "                                    'kwargs':{'d1': d1,'d2': d2,'num_layers':num_layers, \n",
    "                                              'aggr_type':aggr_type}, \n",
    "                                    'learning_rate': lr, 'weight_decay':wd, \n",
    "                                    'batch_size': 32},\n",
    "                                )\n",
    "model_list22 = []\n",
    "for modelclass in [GGNN2]:\n",
    "    for d1 in [25,50]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [300,600]:\n",
    "                    for num_layers in [2,4,8]:\n",
    "                        for lr in [0.01]:\n",
    "                            for wd in [5e-4]:\n",
    "                                model_list22.append(\n",
    "                                    {\n",
    "                                    'model': modelclass,\n",
    "                                    'epochs': epochs,\n",
    "                                    'kwargs':{'d1': d1,'d2': d2,'num_layers':num_layers, \n",
    "                                              'aggr_type':aggr_type}, \n",
    "                                    'learning_rate': lr, 'weight_decay':wd, \n",
    "                                    'batch_size': 32},\n",
    "                                )\n",
    "                                \n",
    "model_list23 = []\n",
    "for modelclass in [GGNN3]:\n",
    "    for d1 in [25,50]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [300,600]:\n",
    "                    for num_layers in [2,4,8]:\n",
    "                        for lr in [0.01]:\n",
    "                            for wd in [5e-4]:\n",
    "                                model_list23.append(\n",
    "                                    {\n",
    "                                    'model': modelclass,\n",
    "                                    'epochs': epochs,\n",
    "                                    'kwargs':{'d1': d1,'d2': d2,'num_layers':num_layers, \n",
    "                                              'aggr_type':aggr_type}, \n",
    "                                    'learning_rate': lr, 'weight_decay':wd, \n",
    "                                    'batch_size': 32},\n",
    "                                )\n",
    "                                \n",
    "model_list24 = []\n",
    "for modelclass in [GGNN4]:\n",
    "    for d1 in [25,50]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [300,600]:\n",
    "                    for num_layers in [2,4,8]:\n",
    "                        for lr in [0.01]:\n",
    "                            for wd in [5e-4]:\n",
    "                                model_list24.append(\n",
    "                                    {\n",
    "                                    'model': modelclass,\n",
    "                                    'epochs': epochs,\n",
    "                                    'kwargs':{'d1': d1,'d2': d2,'num_layers':num_layers, \n",
    "                                              'aggr_type':aggr_type}, \n",
    "                                    'learning_rate': lr, 'weight_decay':wd, \n",
    "                                    'batch_size': 32},\n",
    "                                )\n",
    "                                \n",
    "model_list25 = []\n",
    "for modelclass in [META3,]:\n",
    "    for d1 in [3]:\n",
    "        for d2 in [10,20,50]:\n",
    "            for d3 in [15]:\n",
    "                for d4 in [15]:\n",
    "                    for d5 in [10,20]:\n",
    "                        for epochs in [300,600]:\n",
    "                            model_list25.append(\n",
    "                                {'epochs': epochs,\n",
    "                                'model': modelclass,\n",
    "                                'kwargs':{'d1': d1,'d2': d2, 'd3': d3, 'd4': d4, 'd5':d5}, \n",
    "                                'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 64},\n",
    "                            )\n",
    "                                \n",
    "model_list26 = []\n",
    "for modelclass in [META2,]:\n",
    "    for d1 in [3]:\n",
    "        for d2 in [10,20,50]:\n",
    "            for d3 in [15]:\n",
    "                for d4 in [15]:\n",
    "                    for d5 in [10,20]:\n",
    "                        for epochs in [300,600]:\n",
    "                            model_list26.append(\n",
    "                                {'epochs': epochs,\n",
    "                                'model': modelclass,\n",
    "                                'kwargs':{'d1': d1,'d2': d2, 'd3': d3, 'd4': d4, 'd5':d5}, \n",
    "                                'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 64},\n",
    "                            )\n",
    "                              \n",
    "                               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 1113  k folds= 3\n",
      "Datasets balancing: \n",
      "{0: 663, 1: 450}\n",
      "{0: 530, 1: 360}\n",
      "{0: 133, 1: 90}\n",
      "\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021165318166216213  val accuracy= 0.6666666666666666  val microF1= 0.7117117117117117  val macroF1= 0.7016542824156032\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02079047014315923  val accuracy= 0.6666666666666666  val microF1= 0.6936936936936937  val macroF1= 0.6771271184683938\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022811940560738247  val accuracy= 0.5833333333333334  val microF1= 0.596846846846847  val macroF1= 0.45573775631242713\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021463000526030857  val accuracy= 0.875  val microF1= 0.7353603603603603  val macroF1= 0.720512132252419\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019587913528084755  val accuracy= 0.625  val microF1= 0.7297297297297297  val macroF1= 0.7207652644707351\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022774339963992436  val accuracy= 0.6666666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020814649139841396  val accuracy= 0.5416666666666666  val microF1= 0.7252252252252253  val macroF1= 0.7099124120544561\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.019902820388476055  val accuracy= 0.75  val microF1= 0.7060810810810811  val macroF1= 0.6911390793440598\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.0214417465031147  val accuracy= 0.6666666666666666  val microF1= 0.6373873873873874  val macroF1= 0.567979148690044\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02140093284348647  val accuracy= 0.7083333333333334  val microF1= 0.7274774774774775  val macroF1= 0.7135551138306223\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020053898294766743  val accuracy= 0.75  val microF1= 0.7117117117117117  val macroF1= 0.6985576462342267\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02262938457230727  val accuracy= 0.5416666666666666  val microF1= 0.5957207207207208  val macroF1= 0.5003810126785418\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02097596600651741  val accuracy= 0.4583333333333333  val microF1= 0.7207207207207208  val macroF1= 0.7101380016032922\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.01941346563398838  val accuracy= 0.7916666666666666  val microF1= 0.7027027027027026  val macroF1= 0.6878946954270746\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022188765307267506  val accuracy= 0.5416666666666666  val microF1= 0.6317567567567567  val macroF1= 0.559278693121497\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021142279108365376  val accuracy= 0.6666666666666666  val microF1= 0.7387387387387387  val macroF1= 0.7257124680157011\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022035617381334305  val accuracy= 0.7083333333333334  val microF1= 0.7184684684684685  val macroF1= 0.7042660447436799\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022145049646496773  val accuracy= 0.7083333333333334  val microF1= 0.6171171171171171  val macroF1= 0.4714361358173979\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02063825912773609  val accuracy= 0.7083333333333334  val microF1= 0.7274774774774775  val macroF1= 0.7175126032300331\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020383760333061218  val accuracy= 0.6666666666666666  val microF1= 0.6948198198198198  val macroF1= 0.6764857019674652\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021985722705721855  val accuracy= 0.6666666666666666  val microF1= 0.6193693693693695  val macroF1= 0.46469670719434825\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.023008559520045917  val accuracy= 0.6666666666666666  val microF1= 0.7195945945945946  val macroF1= 0.7046514919600647\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020890480528275173  val accuracy= 0.75  val microF1= 0.7173423423423424  val macroF1= 0.7061208818487695\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02248549337188403  val accuracy= 0.625  val microF1= 0.6216216216216216  val macroF1= 0.5508803646814465\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.01909890150030454  val accuracy= 0.7916666666666666  val microF1= 0.7286036036036037  val macroF1= 0.7147700254848428\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020907110224167507  val accuracy= 0.75  val microF1= 0.6756756756756755  val macroF1= 0.6550971065560659\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022862017154693604  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019201592231790226  val accuracy= 0.875  val microF1= 0.7364864864864865  val macroF1= 0.7214387359736681\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02156040631234646  val accuracy= 0.6666666666666666  val microF1= 0.668918918918919  val macroF1= 0.6477120192988192\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02288026176393032  val accuracy= 0.5416666666666666  val microF1= 0.5968468468468469  val macroF1= 0.434616661502222\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020069106792410214  val accuracy= 0.7083333333333334  val microF1= 0.7195945945945947  val macroF1= 0.7027715855438267\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020605583985646565  val accuracy= 0.7916666666666666  val microF1= 0.6869369369369368  val macroF1= 0.6799664351769009\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022804778690139454  val accuracy= 0.625  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020364409933487575  val accuracy= 0.7083333333333334  val microF1= 0.7162162162162162  val macroF1= 0.7016556320849907\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020413169637322426  val accuracy= 0.6666666666666666  val microF1= 0.6858108108108109  val macroF1= 0.6646415680709765\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.023007813841104507  val accuracy= 0.5416666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02105260267853737  val accuracy= 0.5  val microF1= 0.7083333333333331  val macroF1= 0.700512343934293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021363036707043648  val accuracy= 0.625  val microF1= 0.6531531531531533  val macroF1= 0.5735633628278833\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022740958879391353  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019932031631469727  val accuracy= 0.5416666666666666  val microF1= 0.7218468468468467  val macroF1= 0.7065767776543134\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019462216024597485  val accuracy= 0.7916666666666666  val microF1= 0.7229729729729729  val macroF1= 0.7068958342007553\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022922328983743984  val accuracy= 0.4583333333333333  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.01998957432806492  val accuracy= 0.7083333333333334  val microF1= 0.7263513513513513  val macroF1= 0.7140469155682619\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.01951916515827179  val accuracy= 0.875  val microF1= 0.7004504504504504  val macroF1= 0.6850148331368885\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022743200262387592  val accuracy= 0.6666666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020608992005387943  val accuracy= 0.6666666666666666  val microF1= 0.7263513513513513  val macroF1= 0.7129197924071349\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0208879051109155  val accuracy= 0.75  val microF1= 0.6711711711711712  val macroF1= 0.5855061864887222\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02281498412291209  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      "\n",
      " selected model from loss:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300 0.01909890150030454 0.7916666666666666 0.7286036036036037 0.7147700254848428\n",
      " selected model from accuracy:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600 0.021463000526030857 0.875 0.7286036036036037 0.7147700254848428\n",
      " selected model from microF1:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600 0.021142279108365376 0.6666666666666666 0.7387387387387387 0.7257124680157011\n",
      " selected model from macroF1:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600 0.021142279108365376 0.6666666666666666 0.7387387387387387 0.7257124680157011\n"
     ]
    }
   ],
   "source": [
    "thename='PROTEINS'\n",
    "dataset = TUDataset(root='/tmp/'+thename, name=thename)\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.019131358712911606  val accuracy= 0.7916666666666666  val microF1= 0.7432432432432433  val macroF1= 0.732588097620924\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020590096712112427  val accuracy= 0.6666666666666666  val microF1= 0.6914414414414414  val macroF1= 0.6816959011782355\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022912309194604557  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02257305011153221  val accuracy= 0.6666666666666666  val microF1= 0.7286036036036037  val macroF1= 0.7160370694372511\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019650490954518318  val accuracy= 0.6666666666666666  val microF1= 0.7263513513513513  val macroF1= 0.7196650399704773\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022845184430480003  val accuracy= 0.625  val microF1= 0.6081081081081082  val macroF1= 0.45829062195898684\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02013295330107212  val accuracy= 0.6666666666666666  val microF1= 0.7398648648648649  val macroF1= 0.7253893069199027\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.019291774680217106  val accuracy= 0.7916666666666666  val microF1= 0.7274774774774775  val macroF1= 0.7112770368744036\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02292256864408652  val accuracy= 0.625  val microF1= 0.5945945945945946  val macroF1= 0.42307119298870965\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02002250092724959  val accuracy= 0.75  val microF1= 0.7263513513513513  val macroF1= 0.71311696497566\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.019270915538072586  val accuracy= 0.7916666666666666  val microF1= 0.7229729729729729  val macroF1= 0.7115698389039561\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022492118179798126  val accuracy= 0.4166666666666667  val microF1= 0.6385135135135136  val macroF1= 0.4861691223894873\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02008085325360298  val accuracy= 0.75  val microF1= 0.7117117117117117  val macroF1= 0.695033994283524\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.01922996963063876  val accuracy= 0.75  val microF1= 0.7286036036036037  val macroF1= 0.716873179277477\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.0218071099370718  val accuracy= 0.8333333333333334  val microF1= 0.6407657657657658  val macroF1= 0.4876939690263991\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021353767563899357  val accuracy= 0.5416666666666666  val microF1= 0.7364864864864865  val macroF1= 0.7248354744788094\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020007049664855003  val accuracy= 0.6666666666666666  val microF1= 0.7195945945945946  val macroF1= 0.706839039121864\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022508445506294567  val accuracy= 0.5833333333333334  val microF1= 0.6103603603603603  val macroF1= 0.4634406224321134\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02064669815202554  val accuracy= 0.7083333333333334  val microF1= 0.6835585585585585  val macroF1= 0.6734403108382336\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.01946672797203064  val accuracy= 0.7083333333333334  val microF1= 0.7139639639639639  val macroF1= 0.7023321167661635\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022475071251392365  val accuracy= 0.6666666666666666  val microF1= 0.6396396396396397  val macroF1= 0.5992804083363009\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021472452208399773  val accuracy= 0.625  val microF1= 0.7297297297297297  val macroF1= 0.7160099800259302\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.01950526051223278  val accuracy= 0.5416666666666666  val microF1= 0.7342342342342342  val macroF1= 0.721994868762834\n",
      " trained model:  GGNN1 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022865333904822666  val accuracy= 0.7083333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020084142064054806  val accuracy= 0.7083333333333334  val microF1= 0.7004504504504504  val macroF1= 0.6866833447109082\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021348475168148678  val accuracy= 0.625  val microF1= 0.6452702702702703  val macroF1= 0.4943214615598765\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02278249462445577  val accuracy= 0.6666666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020979207009077072  val accuracy= 0.75  val microF1= 0.7297297297297297  val macroF1= 0.7188528843753169\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019421722119053204  val accuracy= 0.9166666666666666  val microF1= 0.7128378378378378  val macroF1= 0.6999827408562385\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02291586995124817  val accuracy= 0.5  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.01913126806418101  val accuracy= 0.7083333333333334  val microF1= 0.7173423423423424  val macroF1= 0.7025095526399873\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02075003335873286  val accuracy= 0.625  val microF1= 0.6734234234234234  val macroF1= 0.6694264245191688\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02292563517888387  val accuracy= 0.5416666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020538195967674255  val accuracy= 0.7083333333333334  val microF1= 0.7015765765765766  val macroF1= 0.6921153824249685\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020602212597926457  val accuracy= 0.5833333333333334  val microF1= 0.6869369369369368  val macroF1= 0.6750166846551712\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022927821924289066  val accuracy= 0.5  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.0194156418244044  val accuracy= 0.75  val microF1= 0.7139639639639639  val macroF1= 0.704439393484846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021439513191580772  val accuracy= 0.5833333333333334  val microF1= 0.6677927927927927  val macroF1= 0.6536367795328625\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022908736641208332  val accuracy= 0.625  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.0213920921087265  val accuracy= 0.625  val microF1= 0.713963963963964  val macroF1= 0.7015546165356698\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02169240452349186  val accuracy= 0.75  val microF1= 0.6385135135135135  val macroF1= 0.5440757897245487\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02283031182984511  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.019258484865228336  val accuracy= 0.7083333333333334  val microF1= 0.7342342342342342  val macroF1= 0.7207587995423214\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020813335354129475  val accuracy= 0.625  val microF1= 0.6677927927927928  val macroF1= 0.5882235418627584\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022859446704387665  val accuracy= 0.5833333333333334  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021000181635220844  val accuracy= 0.6666666666666666  val microF1= 0.7038288288288288  val macroF1= 0.6943141436745419\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0217945147305727  val accuracy= 0.5  val microF1= 0.6137387387387387  val macroF1= 0.5690520935087758\n",
      " trained model:  GGNN1 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022782025237878162  val accuracy= 0.6666666666666666  val microF1= 0.5945945945945946  val macroF1= 0.37288135593220345\n",
      "\n",
      " selected model from loss:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.01913126806418101 0.7083333333333334 0.7173423423423424 0.7025095526399873\n",
      " selected model from accuracy:  GGNN1 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600 0.019421722119053204 0.9166666666666666 0.7173423423423424 0.7025095526399873\n",
      " selected model from microF1:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300 0.019131358712911606 0.7916666666666666 0.7432432432432433 0.732588097620924\n",
      " selected model from macroF1:  GGNN1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300 0.019131358712911606 0.7916666666666666 0.7432432432432433 0.732588097620924\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsdict1 = modelSelection(model_list21,k, train_dataset, balanced=False)\n",
    "reportModelSelectionResult(modelsdict1)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022138381376862526  val accuracy= 0.6333333333333333  val microF1= 0.6181157572667005  val macroF1= 0.6233140209962138\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021998091290394466  val accuracy= 0.6083333333333333  val microF1= 0.6660717321094679  val macroF1= 0.6715405623687148\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02313585641483466  val accuracy= 0.7000000000000001  val microF1= 0.49815145334013256  val macroF1= 0.4910184736913468\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02165457730491956  val accuracy= 0.6833333333333332  val microF1= 0.6957547169811321  val macroF1= 0.6880135096360268\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.023006457835435867  val accuracy= 0.5833333333333334  val microF1= 0.6381098079211287  val macroF1= 0.6415279305859897\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02401723898947239  val accuracy= 0.5750000000000001  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021345579375823338  val accuracy= 0.5083333333333333  val microF1= 0.6917176610572837  val macroF1= 0.6927836091848055\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022628047193090122  val accuracy= 0.6583333333333333  val microF1= 0.6217703552609213  val macroF1= 0.656414031964829\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.023247622574369114  val accuracy= 0.45  val microF1= 0.5613207547169812  val macroF1= 0.4178330858187955\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02108497793475787  val accuracy= 0.6749999999999999  val microF1= 0.6730409654937958  val macroF1= 0.671912488773747\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.023735095436374348  val accuracy= 0.8166666666666668  val microF1= 0.6928437871834099  val macroF1= 0.6826496083726742\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.024749467770258587  val accuracy= 0.30833333333333335  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020993237073222797  val accuracy= 0.5333333333333333  val microF1= 0.667410334863165  val macroF1= 0.6683548075509954\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02155184435347716  val accuracy= 0.4916666666666667  val microF1= 0.6764193438721741  val macroF1= 0.6708219684749769\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.023738598451018333  val accuracy= 0.375  val microF1= 0.5440251572327044  val macroF1= 0.4020545658365906\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.023092002297441166  val accuracy= 0.65  val microF1= 0.6320967193608703  val macroF1= 0.6362683324173823\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021737884109218914  val accuracy= 0.6166666666666667  val microF1= 0.6424443311235765  val macroF1= 0.6505048829626249\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.023958535864949226  val accuracy= 0.4916666666666667  val microF1= 0.4553586605473397  val macroF1= 0.38195287818631907\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021267301092545193  val accuracy= 0.525  val microF1= 0.6827086520482748  val macroF1= 0.6792897908156066\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021522545566161472  val accuracy= 0.5499999999999999  val microF1= 0.6777579466258712  val macroF1= 0.665796703024446\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02307533596952756  val accuracy= 0.5499999999999999  val microF1= 0.565676525582186  val macroF1= 0.5207353056310858\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02201051140824954  val accuracy= 0.5416666666666666  val microF1= 0.6358575556688764  val macroF1= 0.6300750278743593\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022157913073897362  val accuracy= 0.6666666666666666  val microF1= 0.6277621961584225  val macroF1= 0.6232970582691351\n",
      " trained model:  GGNN2 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02379675644139449  val accuracy= 0.39999999999999997  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021083119014898937  val accuracy= 0.7416666666666667  val microF1= 0.6516658167601563  val macroF1= 0.6512254791422996\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02323811501264572  val accuracy= 0.5  val microF1= 0.5672063573006969  val macroF1= 0.5201941203232492\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02549191067616145  val accuracy= 0.4666666666666666  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021601277713974316  val accuracy= 0.6749999999999999  val microF1= 0.6503272140064593  val macroF1= 0.6589471034547283\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021827730039755504  val accuracy= 0.6749999999999999  val microF1= 0.6563615502294747  val macroF1= 0.6467811954834569\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.024567902709047  val accuracy= 0.39999999999999997  val microF1= 0.4823856875743668  val macroF1= 0.48748508904418886\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021113147338231403  val accuracy= 0.6583333333333333  val microF1= 0.6354113547509773  val macroF1= 0.631287235900729\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022548002501328785  val accuracy= 0.6666666666666666  val microF1= 0.6006926738058814  val macroF1= 0.5413628987565382\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02264445771773656  val accuracy= 0.5750000000000001  val microF1= 0.5651453340132585  val macroF1= 0.482591161262756\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0218064288298289  val accuracy= 0.5833333333333334  val microF1= 0.684089750127486  val macroF1= 0.6881875572802215\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021802810331185658  val accuracy= 0.65  val microF1= 0.6671978582355941  val macroF1= 0.6677628072566436\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02421298126379649  val accuracy= 0.525  val microF1= 0.5727944926058134  val macroF1= 0.45351148891026866\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020855191474159557  val accuracy= 0.6583333333333333  val microF1= 0.6723610402855685  val macroF1= 0.6737043519425047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02265792340040207  val accuracy= 0.47500000000000003  val microF1= 0.6005864354920959  val macroF1= 0.6171070683188044\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.023780447741349537  val accuracy= 0.525  val microF1= 0.5467661057283699  val macroF1= 0.5061740105479332\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02050648753841718  val accuracy= 0.4916666666666667  val microF1= 0.6802226755056943  val macroF1= 0.6799192349644736\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02158302813768387  val accuracy= 0.8416666666666667  val microF1= 0.6263810980792113  val macroF1= 0.5550372103023963\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02413550578057766  val accuracy= 0.5666666666666667  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02117647851506869  val accuracy= 0.7583333333333333  val microF1= 0.6599949005609383  val macroF1= 0.6566036554269238\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.021003680924574535  val accuracy= 0.7083333333333334  val microF1= 0.6689826619071901  val macroF1= 0.6560773495860984\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022611305738488834  val accuracy= 0.4583333333333333  val microF1= 0.5738993710691824  val macroF1= 0.43446073594518203\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02160094492137432  val accuracy= 0.775  val microF1= 0.6977732449430563  val macroF1= 0.6894645584701902\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.023596512774626415  val accuracy= 0.5666666666666667  val microF1= 0.5375658677545471  val macroF1= 0.5898530446673689\n",
      " trained model:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02326839230954647  val accuracy= 0.6416666666666667  val microF1= 0.45986316505184427  val macroF1= 0.39752016320863276\n",
      "\n",
      " selected model from loss:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600 0.02050648753841718 0.4916666666666667 0.6802226755056943 0.6799192349644736\n",
      " selected model from accuracy:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600 0.02158302813768387 0.8416666666666667 0.6802226755056943 0.6799192349644736\n",
      " selected model from microF1:  GGNN2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600 0.02160094492137432 0.775 0.6977732449430563 0.6894645584701902\n",
      " selected model from macroF1:  GGNN2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.021345579375823338 0.5083333333333333 0.6917176610572837 0.6927836091848055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsdict2 = modelSelection(model_list22,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict2)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.0198644424478213  val accuracy= 0.6583333333333333  val microF1= 0.7092469828318885  val macroF1= 0.6924892286615698\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02572790967921416  val accuracy= 0.525  val microF1= 0.5649116097229304  val macroF1= 0.39902788327984506\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02749832657476266  val accuracy= 0.48333333333333334  val microF1= 0.5743668196498385  val macroF1= 0.5989459729451432\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020576929052670796  val accuracy= 0.7666666666666666  val microF1= 0.6818162502124766  val macroF1= 0.6529564832346332\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.022406398008267086  val accuracy= 0.5916666666666667  val microF1= 0.6096379398266191  val macroF1= 0.649371032098457\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.025624563296635944  val accuracy= 0.7166666666666667  val microF1= 0.5919386367499575  val macroF1= 0.44147984119668787\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.019793668761849403  val accuracy= 0.6666666666666666  val microF1= 0.7259051504334523  val macroF1= 0.7047171072809638\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.025454264134168625  val accuracy= 0.65  val microF1= 0.6040710521842597  val macroF1= 0.5448735380850579\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.030691043784221012  val accuracy= 0.6166666666666667  val microF1= 0.5628930817610063  val macroF1= 0.39615375553203797\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021032923211654026  val accuracy= 0.5750000000000001  val microF1= 0.7114779874213837  val macroF1= 0.7025086645724636\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022290997828046482  val accuracy= 0.6  val microF1= 0.6527281998980112  val macroF1= 0.6498199343366533\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.027284797901908558  val accuracy= 0.425  val microF1= 0.5795512493625701  val macroF1= 0.4362985709991632\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.020273896555105846  val accuracy= 0.7166666666666667  val microF1= 0.7144526602073773  val macroF1= 0.7016871711141893\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.021723680819074314  val accuracy= 0.75  val microF1= 0.632032976372599  val macroF1= 0.5906968902821933\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.024371222282449406  val accuracy= 0.6583333333333333  val microF1= 0.5727944926058134  val macroF1= 0.4414843219414217\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021314896643161774  val accuracy= 0.6833333333333332  val microF1= 0.6725310215876253  val macroF1= 0.6724933019378735\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021613089367747307  val accuracy= 0.625  val microF1= 0.6304606493285738  val macroF1= 0.5326794583302824\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.0283290712783734  val accuracy= 0.5416666666666666  val microF1= 0.5964431412544621  val macroF1= 0.457636840413943\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020618912453452747  val accuracy= 0.8083333333333332  val microF1= 0.6993455719870814  val macroF1= 0.6902738928694075\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.023650314037998516  val accuracy= 0.6083333333333333  val microF1= 0.5838645249022608  val macroF1= 0.6361677856380035\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.026989207913478214  val accuracy= 0.5833333333333334  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.025554324810703594  val accuracy= 0.6083333333333333  val microF1= 0.5291730409654939  val macroF1= 0.5362211518113295\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021264190475145977  val accuracy= 0.6166666666666667  val microF1= 0.6410419853816081  val macroF1= 0.6009092227819841\n",
      " trained model:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0291867529352506  val accuracy= 0.27499999999999997  val microF1= 0.48801631820499747  val macroF1= 0.41839920258653396\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02055114010969798  val accuracy= 0.6083333333333333  val microF1= 0.6977519972802991  val macroF1= 0.6841345450169914\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.027868409330646198  val accuracy= 0.6166666666666667  val microF1= 0.5786163522012578  val macroF1= 0.4386126215638695\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.024996522814035416  val accuracy= 0.6166666666666667  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020328286414345104  val accuracy= 0.7416666666666667  val microF1= 0.7029364269930308  val macroF1= 0.690448263948168\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.024296793465813  val accuracy= 0.5916666666666667  val microF1= 0.5477010028896822  val macroF1= 0.5341866656254701\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.028880042334397633  val accuracy= 0.6833333333333332  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02624715305864811  val accuracy= 0.47500000000000003  val microF1= 0.4992775794662587  val macroF1= 0.49546422158082737\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.023586169506112736  val accuracy= 0.4666666666666666  val microF1= 0.6500509943906171  val macroF1= 0.6344265684481581\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.030468926454583805  val accuracy= 0.6166666666666667  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020865661402543385  val accuracy= 0.6749999999999999  val microF1= 0.6899116097229304  val macroF1= 0.6519570151565331\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02359161215523879  val accuracy= 0.6666666666666666  val microF1= 0.5966556178820329  val macroF1= 0.5271739823148975\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.028475483879446983  val accuracy= 0.48333333333333334  val microF1= 0.586987931327554  val macroF1= 0.5002697876901485\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022687692195177078  val accuracy= 0.75  val microF1= 0.6378548359680435  val macroF1= 0.6429156282810867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.023100221529603004  val accuracy= 0.6749999999999999  val microF1= 0.6606323304436512  val macroF1= 0.5756081921265678\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02776806304852168  val accuracy= 0.525  val microF1= 0.46931837497875234  val macroF1= 0.5059990575344765\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02217839037378629  val accuracy= 0.7000000000000001  val microF1= 0.6635645079041306  val macroF1= 0.6599952036204638\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.026496889690558117  val accuracy= 0.39166666666666666  val microF1= 0.43553459119496857  val macroF1= 0.37924683467699055\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.024535025159517925  val accuracy= 0.5833333333333334  val microF1= 0.5849056603773585  val macroF1= 0.45019681130019834\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020547229175766308  val accuracy= 0.6  val microF1= 0.7123916369199388  val macroF1= 0.6898230422525568\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.026360225553313892  val accuracy= 0.5666666666666667  val microF1= 0.6005014448410675  val macroF1= 0.595612843723138\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02823420117298762  val accuracy= 0.48333333333333334  val microF1= 0.5806773754886962  val macroF1= 0.44600656946421796\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021525558084249496  val accuracy= 0.6749999999999999  val microF1= 0.675314465408805  val macroF1= 0.6908325160182941\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021549584344029427  val accuracy= 0.6583333333333333  val microF1= 0.640595784463709  val macroF1= 0.6321511652782604\n",
      " trained model:  GGNN3 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022617068762580555  val accuracy= 0.525  val microF1= 0.5707547169811321  val macroF1= 0.441278478511761\n",
      "\n",
      " selected model from loss:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.019793668761849403 0.6666666666666666 0.7259051504334523 0.7047171072809638\n",
      " selected model from accuracy:  GGNN3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.020618912453452747 0.8083333333333332 0.7259051504334523 0.7047171072809638\n",
      " selected model from microF1:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.019793668761849403 0.6666666666666666 0.7259051504334523 0.7047171072809638\n",
      " selected model from macroF1:  GGNN3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300 0.019793668761849403 0.6666666666666666 0.7259051504334523 0.7047171072809638\n"
     ]
    }
   ],
   "source": [
    "modelsdict3 = modelSelection(model_list23,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict3)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.019712908814350765  val accuracy= 0.6916666666666668  val microF1= 0.6950535441101479  val macroF1= 0.6916366924776857\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.024213384216030438  val accuracy= 0.5499999999999999  val microF1= 0.6038373278939316  val macroF1= 0.6136665825498611\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.029032019898295403  val accuracy= 0.39999999999999997  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020998502150177956  val accuracy= 0.7333333333333334  val microF1= 0.7188934217236104  val macroF1= 0.7182806385900807\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020884640514850616  val accuracy= 0.7000000000000001  val microF1= 0.6806476287608364  val macroF1= 0.6663713006287115\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.025714386875430744  val accuracy= 0.6916666666666668  val microF1= 0.5908125106238313  val macroF1= 0.45017664791265827\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020332032193740208  val accuracy= 0.75  val microF1= 0.7069947305796362  val macroF1= 0.6896239989014997\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.022691528002421062  val accuracy= 0.6833333333333332  val microF1= 0.6419343872174061  val macroF1= 0.6276447800922357\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02552475283543269  val accuracy= 0.7416666666666667  val microF1= 0.6189656637769846  val macroF1= 0.475378746473795\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022573657954732578  val accuracy= 0.625  val microF1= 0.7225267720550739  val macroF1= 0.7113669858317183\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02506699226796627  val accuracy= 0.6583333333333333  val microF1= 0.6002464728879823  val macroF1= 0.6003734449695383\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02671177126467228  val accuracy= 0.5750000000000001  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02166800945997238  val accuracy= 0.6666666666666666  val microF1= 0.6397246302906681  val macroF1= 0.6513175958450447\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02367475690941016  val accuracy= 0.5  val microF1= 0.6189019207887132  val macroF1= 0.5667849752409818\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022799044847488403  val accuracy= 0.5833333333333334  val microF1= 0.5896863844977053  val macroF1= 0.5406832724104742\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.01961277239024639  val accuracy= 0.7583333333333333  val microF1= 0.7135177630460648  val macroF1= 0.6909002851701606\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.021849488839507103  val accuracy= 0.625  val microF1= 0.6211966683664797  val macroF1= 0.6154297999345696\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.03144761547446251  val accuracy= 0.4083333333333334  val microF1= 0.43396226415094336  val macroF1= 0.2995169082125604\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.020757773891091347  val accuracy= 0.5750000000000001  val microF1= 0.6581038585755566  val macroF1= 0.6700253791446763\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.025269464900096256  val accuracy= 0.5833333333333334  val microF1= 0.6018187999320075  val macroF1= 0.6207803220523403\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.026948582381010056  val accuracy= 0.7999999999999999  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020375876997907955  val accuracy= 0.725  val microF1= 0.7031913989461159  val macroF1= 0.6989019433080004\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.021518147860964138  val accuracy= 0.425  val microF1= 0.6747832738398776  val macroF1= 0.6640629341549844\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.023329798753062885  val accuracy= 0.5666666666666667  val microF1= 0.5727519972802991  val macroF1= 0.478067544171712\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02047234463195006  val accuracy= 0.8166666666666668  val microF1= 0.6835798062213158  val macroF1= 0.6798934178929557\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.022611692547798157  val accuracy= 0.6083333333333333  val microF1= 0.6020737718850926  val macroF1= 0.5492449854960891\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02418718735376994  val accuracy= 0.65  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.019469423219561577  val accuracy= 0.8166666666666668  val microF1= 0.7137514873363929  val macroF1= 0.7094880734748509\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02127974232037862  val accuracy= 0.6666666666666666  val microF1= 0.6583588305286417  val macroF1= 0.6387092950663594\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.027445713058114052  val accuracy= 0.6666666666666666  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.01990072677532832  val accuracy= 0.725  val microF1= 0.7072072072072073  val macroF1= 0.6953591529508566\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.026857045168677967  val accuracy= 0.6083333333333333  val microF1= 0.5610445351011388  val macroF1= 0.5515295279792118\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.026210771252711613  val accuracy= 0.5666666666666667  val microF1= 0.5682899881013088  val macroF1= 0.4499451458471146\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0196455226590236  val accuracy= 0.725  val microF1= 0.7252039775624682  val macroF1= 0.71484035002358\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.022783252721031506  val accuracy= 0.525  val microF1= 0.6117627061023287  val macroF1= 0.5924635040213083\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.02521841290096442  val accuracy= 0.6166666666666667  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.019113438824812572  val accuracy= 0.7666666666666666  val microF1= 0.7182559918408975  val macroF1= 0.6926257482644343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02611234039068222  val accuracy= 0.5583333333333333  val microF1= 0.6133350331463538  val macroF1= 0.5297177149009978\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.02923895666996638  val accuracy= 0.47500000000000003  val microF1= 0.43396226415094336  val macroF1= 0.36899599395037846\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.020430763562520344  val accuracy= 0.7416666666666667  val microF1= 0.7076534081251062  val macroF1= 0.6840628155466053\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.025545764714479446  val accuracy= 0.5750000000000001  val microF1= 0.5964218935917049  val macroF1= 0.5242147965936481\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'mean'}  epochs: 600  val loss= 0.02821509726345539  val accuracy= 0.4583333333333333  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.019800587246815365  val accuracy= 0.75  val microF1= 0.7074196838347783  val macroF1= 0.698351964122247\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02356422444184621  val accuracy= 0.5833333333333334  val microF1= 0.6209629440761516  val macroF1= 0.6241725694245551\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 300  val loss= 0.02473470630745093  val accuracy= 0.4916666666666667  val microF1= 0.5660377358490566  val macroF1= 0.3592592592592592\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600  val loss= 0.020921187475323677  val accuracy= 0.6  val microF1= 0.6376211116777154  val macroF1= 0.5614335044048381\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 4, 'aggr_type': 'add'}  epochs: 600  val loss= 0.0251438170671463  val accuracy= 0.6749999999999999  val microF1= 0.581761006289308  val macroF1= 0.44760767176784144\n",
      " trained model:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 8, 'aggr_type': 'add'}  epochs: 600  val loss= 0.026210006947318714  val accuracy= 0.6  val microF1= 0.5626593574706783  val macroF1= 0.40591480688984866\n",
      "\n",
      " selected model from loss:  GGNN4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300 0.019113438824812572 0.7666666666666666 0.7182559918408975 0.6926257482644343\n",
      " selected model from accuracy:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300 0.02047234463195006 0.8166666666666668 0.7182559918408975 0.6926257482644343\n",
      " selected model from microF1:  GGNN4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 600 0.0196455226590236 0.725 0.7252039775624682 0.71484035002358\n",
      " selected model from macroF1:  GGNN4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 600 0.020998502150177956 0.7333333333333334 0.7188934217236104 0.7182806385900807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsdict4 = modelSelection(model_list24,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict4)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.009408975951373577  val accuracy= 0.725  val microF1= 0.74275454699983  val macroF1= 0.7338612667380917\n",
      "Problem training model META3\n",
      " trained model:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.0  val accuracy= 0.0  val microF1= 0.0  val macroF1= 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 917, in modelSelection\n",
      "    val_loss_model(model, loader_val, optimizer, val_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 699, in val_loss_model\n",
      "    return val_loss_model_META(model, loader, optimizer, val_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 692, in val_loss_model_META\n",
      "    measures = F1Score(total_acc, total_gt)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 171, in F1Score\n",
      "    preddict[pred[i]].append(i)\n",
      "KeyError: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.009422309075792631  val accuracy= 0.8250000000000001  val microF1= 0.7314932857385688  val macroF1= 0.7272838415919992\n",
      " trained model:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.009759203530848026  val accuracy= 0.75  val microF1= 0.7200195478497365  val macroF1= 0.7172332220506608\n",
      " trained model:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.009310266934335232  val accuracy= 0.7583333333333333  val microF1= 0.7472590515043347  val macroF1= 0.7371908787805023\n",
      " trained model:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.009358911464611689  val accuracy= 0.7416666666666666  val microF1= 0.73419173890872  val macroF1= 0.7275254721826167\n",
      " trained model:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.009664067377646765  val accuracy= 0.7083333333333334  val microF1= 0.7305796362400135  val macroF1= 0.730318560013245\n",
      " trained model:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.009602510059873262  val accuracy= 0.7333333333333334  val microF1= 0.7155150433452321  val macroF1= 0.7138603609841518\n",
      " trained model:  META3 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.00944589264690876  val accuracy= 0.7333333333333334  val microF1= 0.7288160802311746  val macroF1= 0.7207795954748071\n",
      " trained model:  META3 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.009743262392779192  val accuracy= 0.6749999999999999  val microF1= 0.7137302396736359  val macroF1= 0.7105319218030713\n",
      " trained model:  META3 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.009375095056990782  val accuracy= 0.6666666666666666  val microF1= 0.7321732109467959  val macroF1= 0.7248254031629422\n",
      " trained model:  META3 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.00991631206125021  val accuracy= 0.7166666666666667  val microF1= 0.6959246982831888  val macroF1= 0.6931320418166411\n",
      "\n",
      " selected model from loss:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600 0.0 0.0 0.0 0.0\n",
      " selected model from accuracy:  META3 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300 0.009422309075792631 0.8250000000000001 0.0 0.0\n",
      " selected model from microF1:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300 0.009310266934335232 0.7583333333333333 0.7472590515043347 0.7371908787805023\n",
      " selected model from macroF1:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300 0.009310266934335232 0.7583333333333333 0.7472590515043347 0.7371908787805023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsdict5 = modelSelection(model_list25,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict5)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  META2 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.010608522221446037  val accuracy= 0.6666666666666666  val microF1= 0.6880843107258201  val macroF1= 0.6748070697624392\n",
      " trained model:  META2 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.011150006204843521  val accuracy= 0.6416666666666667  val microF1= 0.6513895971443141  val macroF1= 0.654121828433381\n",
      " trained model:  META2 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.010687325149774551  val accuracy= 0.6333333333333333  val microF1= 0.6644356620771715  val macroF1= 0.6501722470779756\n",
      " trained model:  META2 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.014482046787937483  val accuracy= 0.6166666666666666  val microF1= 0.566781404045555  val macroF1= 0.5480852286292531\n",
      " trained model:  META2 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.011060274516542753  val accuracy= 0.7166666666666668  val microF1= 0.6626508584055754  val macroF1= 0.6654487782464362\n",
      "Problem training model META2\n",
      " trained model:  META2 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.0  val accuracy= 0.0  val microF1= 0.0  val macroF1= 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 917, in modelSelection\n",
      "    val_loss_model(model, loader_val, optimizer, val_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 699, in val_loss_model\n",
      "    return val_loss_model_META(model, loader, optimizer, val_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 692, in val_loss_model_META\n",
      "    measures = F1Score(total_acc, total_gt)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 171, in F1Score\n",
      "    preddict[pred[i]].append(i)\n",
      "KeyError: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  META2 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.012194599645833174  val accuracy= 0.6666666666666666  val microF1= 0.5873066462689104  val macroF1= 0.608856547588013\n",
      " trained model:  META2 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.011239480848113695  val accuracy= 0.6749999999999999  val microF1= 0.6644569097399285  val macroF1= 0.6572826730337141\n",
      " trained model:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300  val loss= 0.011374074034392834  val accuracy= 0.6666666666666666  val microF1= 0.6545554988951215  val macroF1= 0.6547204077052945\n",
      " trained model:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600  val loss= 0.01329769721875588  val accuracy= 0.6166666666666667  val microF1= 0.640595784463709  val macroF1= 0.6542349664716814\n",
      " trained model:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 300  val loss= 0.01067391379425923  val accuracy= 0.6583333333333333  val microF1= 0.6727859935407107  val macroF1= 0.6691372613799064\n",
      " trained model:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600  val loss= 0.011038317965964476  val accuracy= 0.725  val microF1= 0.6849609043005268  val macroF1= 0.6784310443745719\n",
      "\n",
      " selected model from loss:  META2 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 600 0.0 0.0 0.0 0.0\n",
      " selected model from accuracy:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600 0.011038317965964476 0.725 0.0 0.0\n",
      " selected model from microF1:  META2 {'d1': 3, 'd2': 10, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 300 0.010608522221446037 0.6666666666666666 0.6880843107258201 0.6748070697624392\n",
      " selected model from macroF1:  META2 {'d1': 3, 'd2': 50, 'd3': 15, 'd4': 15, 'd5': 20}  epochs: 600 0.011038317965964476 0.725 0.6849609043005268 0.6784310443745719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelsdict6 = modelSelection(model_list26,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict6)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultsdict3={}\n",
    "resultsdict3['best_models_list']=[]\n",
    "resultsdict3['best_models_list'].append(modelsdict1['best_models'])\n",
    "resultsdict3['best_models_list'].append(modelsdict2['best_models'])\n",
    "resultsdict3['best_models_list'].append(modelsdict4['best_models'])\n",
    "resultsdict3['best_models_list'].append(modelsdict5['best_models'])\n",
    "resultsdict3['best_models_list'].append(modelsdict6['best_models'])\n",
    "#resultsdict3['best_models_list'].append(modelsdict3['best_models'])\n",
    "modelsdict['best_models_list']=resultsdict3['best_models_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelsdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b79b49e0a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# gather top 5 models by accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m selection = [ (round(float(mod['cv_val_accuracy']),2), mod) for m in modelsdict['best_models_list']\n\u001b[0m\u001b[1;32m      6\u001b[0m       for measure,mod in m.items()]\n\u001b[1;32m      7\u001b[0m \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelsdict' is not defined"
     ]
    }
   ],
   "source": [
    "#print([(mod['model'].__name__, round(float(mod['cv_val_accuracy']),2) ) for m in modelsdict['best_models_list']\n",
    "#      for measure,mod in m.items()])\n",
    "\n",
    "# gather top 5 models by accuracy\n",
    "selection = [ (round(float(mod['cv_val_accuracy']),2), mod) for m in modelsdict['best_models_list']\n",
    "      for measure,mod in m.items()]\n",
    "selection = sorted(selection, key=lambda x: float(x[0]), reverse=True )\n",
    "selection = selection[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 1113  k folds= 3\n",
      "Datasets balancing: \n",
      "{0: 663, 1: 450}\n",
      "{0: 530, 1: 360}\n",
      "{0: 133, 1: 90}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "#from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "from torch_geometric.datasets import TUDataset, QM9, QM7b, PPI, Planetoid, KarateClub\n",
    "\n",
    "\n",
    "from TFM_graph_classification import *\n",
    "\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# load dataset\n",
    "thename='PROTEINS'\n",
    "dataset = TUDataset(root='/tmp/'+thename, name=thename)\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "\n",
    "# load selection from saved models\n",
    "modelsdict={}\n",
    "modelsdict['models']=[]\n",
    "modelsdict['best_models_list']=[]\n",
    "\n",
    "filenames =[\n",
    "    './models/proteins/GGNN1__d1-25_d2-20_num_layers-2_aggr_type-mean_300_0.01_0.0005_32_date2019-04-02_15-39-45',\n",
    "    './models/proteins/GGNN1__d1-25_d2-20_num_layers-2_aggr_type-mean_300_0.01_0.0005_32_date2019-04-02_15-39-45',\n",
    "    './models/proteins/GGNN1__d1-25_d2-20_num_layers-2_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_04-01-15',\n",
    "    './models/proteins/GGNN1__d1-25_d2-50_num_layers-2_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_04-01-15',\n",
    "    './models/proteins/GGNN1__d1-50_d2-20_num_layers-2_aggr_type-add_300_0.01_0.0005_32_date2019-04-02_15-39-45',\n",
    "    './models/proteins/GGNN1__d1-50_d2-20_num_layers-2_aggr_type-mean_300_0.01_0.0005_32_date2019-04-02_04-01-15',\n",
    "    './models/proteins/GGNN1__d1-50_d2-20_num_layers-4_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_15-39-45',\n",
    "    './models/proteins/GGNN2__d1-25_d2-20_num_layers-2_aggr_type-add_300_0.01_0.0005_32_date2019-04-02_06-02-40',\n",
    "    './models/proteins/GGNN2__d1-50_d2-50_num_layers-2_aggr_type-add_600_0.01_0.0005_32_date2019-04-02_06-02-40',\n",
    "    './models/proteins/GGNN2__d1-50_d2-50_num_layers-2_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_06-02-40',\n",
    "    './models/proteins/GGNN2__d1-50_d2-50_num_layers-4_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_06-02-40',\n",
    "    './models/proteins/GGNN3__d1-25_d2-20_num_layers-2_aggr_type-add_300_0.01_0.0005_32_date2019-04-02_08-27-20',\n",
    "    './models/proteins/GGNN3__d1-25_d2-50_num_layers-2_aggr_type-add_300_0.01_0.0005_32_date2019-04-02_08-27-20',\n",
    "    './models/proteins/GGNN4__d1-25_d2-20_num_layers-2_aggr_type-mean_600_0.01_0.0005_32_date2019-04-02_10-43-22',\n",
    "    './models/proteins/GGNN4__d1-50_d2-20_num_layers-2_aggr_type-add_600_0.01_0.0005_32_date2019-04-02_10-43-22',\n",
    "    './models/proteins/GGNN4__d1-50_d2-20_num_layers-2_aggr_type-mean_300_0.01_0.0005_32_date2019-04-02_10-43-22',\n",
    "    './models/proteins/GGNN4__d1-50_d2-50_num_layers-2_aggr_type-mean_300_0.01_0.0005_32_date2019-04-02_10-43-22',\n",
    "    './models/proteins/META2__d1-3_d2-10_d3-15_d4-15_d5-10_300_0.01_0.0005_64_date2019-04-02_12-52-47',\n",
    "    './models/proteins/META2__d1-3_d2-20_d3-15_d4-15_d5-10_600_0.01_0.0005_64_date2019-04-02_12-52-47',\n",
    "    './models/proteins/META2__d1-3_d2-50_d3-15_d4-15_d5-20_600_0.01_0.0005_64_date2019-04-02_12-52-47',\n",
    "    './models/proteins/META3__d1-3_d2-10_d3-15_d4-15_d5-10_600_0.01_0.0005_64_date2019-04-02_12-33-04',\n",
    "    './models/proteins/META3__d1-3_d2-10_d3-15_d4-15_d5-20_300_0.01_0.0005_64_date2019-04-02_12-33-04',\n",
    "    './models/proteins/META3__d1-3_d2-20_d3-15_d4-15_d5-10_300_0.01_0.0005_64_date2019-04-02_12-33-04',\n",
    "]\n",
    "for path in filenames:\n",
    "    modeldict = loadModelFromFile(path)\n",
    "    modelsdict['models'].append(modeldict)\n",
    "    modelsdict['best_models_list'].append(modeldict)\n",
    "    modelsdict['testing']={}\n",
    "    modelsdict['best_models']={}\n",
    "\n",
    "selection = [ (round(float(0),2), m) for m in modelsdict['best_models_list'] ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7399  macroF1: 0.7254255914881702  microF1: 0.7399103139013452\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7220  macroF1: 0.7102327155226935  microF1: 0.7219730941704036\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7534  macroF1: 0.7425481335444628  microF1: 0.7533632286995515\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.7444  macroF1: 0.7293254612471254  microF1: 0.7443946188340808\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7399  macroF1: 0.7246046304943495  microF1: 0.7399103139013452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7265  macroF1: 0.7120138628134246  microF1: 0.726457399103139\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.5964  macroF1: 0.3735955056179775  microF1: 0.5964125560538116\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7623  macroF1: 0.7495723626101735  microF1: 0.7623318385650225\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.7399  macroF1: 0.7249326254941588  microF1: 0.7399103139013452\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.7399  macroF1: 0.7254719430180016  microF1: 0.7399103139013452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7130  macroF1: 0.7041893437563562  microF1: 0.7130044843049327\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.6726  macroF1: 0.6478061644232332  microF1: 0.672645739910314\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.7489  macroF1: 0.7347307873770716  microF1: 0.7488789237668162\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.7444  macroF1: 0.7303628154709341  microF1: 0.7443946188340808\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.7130  macroF1: 0.6950127282687252  microF1: 0.7130044843049327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7399  macroF1: 0.724081247885053  microF1: 0.7399103139013452\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7713  macroF1: 0.7588271058969916  microF1: 0.7713004484304933\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7265  macroF1: 0.7090909192539618  microF1: 0.726457399103139\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7534  macroF1: 0.739063906390639  microF1: 0.7533632286995515\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7354  macroF1: 0.7191467583389776  microF1: 0.7354260089686099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.7892  macroF1: 0.7784700647980328  microF1: 0.7892376681614348\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.7982  macroF1: 0.7877528097122307  microF1: 0.7982062780269058\n",
      "len(test_dataset):  223\n",
      "num graphs:  223\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.7713  macroF1: 0.7605824613179353  microF1: 0.7713004484304933\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macroF1</th>\n",
       "      <th>microF1</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>GGNN1_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>GGNN1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7534</td>\n",
       "      <td>0.7425</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>GGNN1_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7444</td>\n",
       "      <td>0.7293</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>GGNN1_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>GGNN1_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7265</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>0.7265</td>\n",
       "      <td>GGNN1_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5964</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>0.5964</td>\n",
       "      <td>GGNN1_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7623</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>GGNN2_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>GGNN2_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7255</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>GGNN2_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>GGNN2_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.6726</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.6726</td>\n",
       "      <td>GGNN3_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.7489</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>0.7489</td>\n",
       "      <td>GGNN3_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.7444</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>GGNN4_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.7130</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>GGNN4_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.7399</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>GGNN4_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.7713</td>\n",
       "      <td>0.7588</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>GGNN4_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.7265</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>0.7265</td>\n",
       "      <td>META2_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.7534</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>META2_18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.7354</td>\n",
       "      <td>0.7191</td>\n",
       "      <td>0.7354</td>\n",
       "      <td>META2_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.7785</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>META3_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.7982</td>\n",
       "      <td>0.7878</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>META3_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.7713</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>META3_22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  macroF1  microF1      name\n",
       "0     0.7399   0.7254   0.7399   GGNN1_0\n",
       "1     0.7220   0.7102   0.7220   GGNN1_1\n",
       "2     0.7534   0.7425   0.7534   GGNN1_2\n",
       "3     0.7444   0.7293   0.7444   GGNN1_3\n",
       "4     0.7399   0.7246   0.7399   GGNN1_4\n",
       "5     0.7265   0.7120   0.7265   GGNN1_5\n",
       "6     0.5964   0.3736   0.5964   GGNN1_6\n",
       "7     0.7623   0.7496   0.7623   GGNN2_7\n",
       "8     0.7399   0.7249   0.7399   GGNN2_8\n",
       "9     0.7399   0.7255   0.7399   GGNN2_9\n",
       "10    0.7130   0.7042   0.7130  GGNN2_10\n",
       "11    0.6726   0.6478   0.6726  GGNN3_11\n",
       "12    0.7489   0.7347   0.7489  GGNN3_12\n",
       "13    0.7444   0.7304   0.7444  GGNN4_13\n",
       "14    0.7130   0.6950   0.7130  GGNN4_14\n",
       "15    0.7399   0.7241   0.7399  GGNN4_15\n",
       "16    0.7713   0.7588   0.7713  GGNN4_16\n",
       "17    0.7265   0.7091   0.7265  META2_17\n",
       "18    0.7534   0.7391   0.7534  META2_18\n",
       "19    0.7354   0.7191   0.7354  META2_19\n",
       "20    0.7892   0.7785   0.7892  META3_20\n",
       "21    0.7982   0.7878   0.7982  META3_21\n",
       "22    0.7713   0.7606   0.7713  META3_22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i in range(len(selection)):    \n",
    "    bmodel = final_model_train(selection[i][1], train_dataset)\n",
    "    testresult = testModel(bmodel, test_dataset)\n",
    "    modelsdict['testing'][bmodel.__class__.__name__+'_'+str(i)]=testresult\n",
    "\n",
    "\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteins dataset benchmark\n",
    "\n",
    "Paper \"Distinguishing Enzyme Structures from Non-enzymes Without Alignments\", develops 2 algorithms that predict enzyme from non-enzyme with **77%** and **80%** accuracy.\n",
    "\n",
    "\n",
    "Paper  \"Protein function prediction via graph kernels\" , **72.33** to **84.08%** accuracy.\n",
    "\n",
    "### References\n",
    "- [4] K. M. Borgwardt, C. S. Ong, S. Schoenauer, S. V. N. Vishwanathan, A. J. Smola, and H. P. Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(Suppl 1):i47i56, Jun 2005.\n",
    "- [6] P. D. Dobson and A. J. Doig. Distinguishing enzyme structures from non-enzymes without alignments. J. Mol. Biol., 330(4):771783, Jul 2003.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. encapsulate all training, model selection,.. everything\n",
    "2. present results with Pandas tables, and histograms\n",
    "3. save models and results to disk, and load them later for testing\n",
    "4. transform into a python module or package\n",
    "5. prepare another notebook using the python module (prepare local and on collab)\n",
    "6. test other GNN layers: GAT, GCN, GraphSAGE, Metalayer\n",
    "7. do a good HP search\n",
    "8. repeat HP for , PROTEINS, IMDB\n",
    "\n",
    "### Pending:\n",
    "\n",
    "- ko-repeat for PPI, REDDIT \n",
    "- report results (top 5 best accruacy/F1 models)\n",
    "- compare with published benchmarks\n",
    "- baselines (random and MLP with node features)\n",
    "- look for published architectures?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
