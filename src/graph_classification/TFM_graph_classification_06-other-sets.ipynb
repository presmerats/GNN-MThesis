{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- train & compare differnt GNN models for graph classificaiton in common benchmarks (PPI, Proteins, ENZYMES,..)\n",
    "- compare results to publication results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "#from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "from torch_geometric.datasets import TUDataset, QM9, QM7b, PPI, Planetoid, KarateClub\n",
    "\n",
    "from TFM_graph_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GGNN1(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN1, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GGNN2(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN2, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class GGNN3(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20, d3=10,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN3, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, d3)\n",
    "        self.dense2_bn = nn.BatchNorm1d(d3)\n",
    "        self.fc3 = nn.Linear(d3, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.dense2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class GGNN4(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(GGNN4, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class META1(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META1, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "        # version using only u\n",
    "        x = F.relu(self.dense1_bn(self.fc1(u2)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    \n",
    "class META2(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META2, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "        \n",
    "        # version using only x \n",
    "        x = self.global_pool(x2,batch) # separate by graph level\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class META3(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META3, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(d3+1, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "\n",
    "        # version using x and  u\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        ubatch = list(set([ elem.item() for elem in batch]))\n",
    "        #print(ubatch)\n",
    "        x = self.global_pool(x, torch.cat([batch,torch.LongTensor( ubatch).to(device) ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class META4(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META4, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "    \n",
    "        # version using x and  u and edge_attr\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        #x = x2\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, torch.cat([batch, ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class META5(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Not using edge attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, d1=3, d2=50, d3=15):\n",
    "        super(META5, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*2, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(2, d2), ReLU(), Lin(d2, d3))\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            out = torch.cat([source, target], dim=1)\n",
    "            #print(\"edge_model\")\n",
    "            #print(out.size())\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            out = torch.cat([x[col]], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        #print(\"Forward: \")\n",
    "        #print(x.size())\n",
    "        return self.op(x, edge_index, edge_attr, u, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list =[\n",
    "    {'epochs': 300,\n",
    "    'model': GGNN4,\n",
    "    'kwargs':{'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 300,\n",
    "    'model': GGNN1,\n",
    "    'kwargs':{'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 300 ,\n",
    "    'model': GGNN3,\n",
    "    'kwargs': {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'},\n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 800,\n",
    "    'model': GGNN4,\n",
    "    'kwargs':{'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 800,\n",
    "    'model': GGNN1,\n",
    "    'kwargs':{'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 800 ,\n",
    "    'model': GGNN3,\n",
    "    'kwargs': {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'},\n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': META3,\n",
    "    'kwargs':{'d1': 3,'d2': 20, 'd3': 15, 'd4': 15, 'd5':10}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "]\n",
    " \n",
    "model_list =[\n",
    "    {'epochs': 100,\n",
    "    'model': GGNN4,\n",
    "    'kwargs':{'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones(1)\n",
    "y[0]\n",
    "print(y.size())\n",
    "print(torch.Size([1]))\n",
    "print(y.size()==torch.Size([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      " n: 133246  k folds= 3\n",
      "Problem training model GGNN4\n",
      " trained model:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.0  val accuracy= 0.0  val microF1= 0.0  val macroF1= 0.0\n",
      "\n",
      " selected model from loss:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100 0.0 0.0 0.0 0.0\n",
      " selected model from accuracy:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100 0.0 0.0 0.0 0.0\n",
      " selected model from microF1:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100 0.0 0.0 0.0 0.0\n",
      " selected model from macroF1:  GGNN4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100 0.0 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 916, in modelSelection\n",
      "    train_model(model, loader, optimizer, train_loss_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 582, in train_model\n",
      "    return train_model_GGNN(model, loader, optimizer, train_loss_history)\n",
      "  File \"/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\", line 517, in train_model_GGNN\n",
      "    loss = F.nll_loss(out, target)\n",
      "  File \"/home/pau/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py\", line 1790, in nll_loss\n",
      "    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'target'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6a29e100dadb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelsdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtestresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodelsdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\u001b[0m in \u001b[0;36mfinal_model_train\u001b[0;34m(modeldict, train_dataset)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, optimizer, train_loss_history)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_model_META\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_model_GGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py\u001b[0m in \u001b[0;36mtrain_model_GGNN\u001b[0;34m(model, loader, optimizer, train_loss_history)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "dataset = QM9(root='/tmp/QM9')\n",
    "print(dataset.num_classes)\n",
    "#print(list(set([graph.y.item() for graph in dataset])))\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = randomDatasetSplit_slice(dataset, prop=0.8)\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset, balanced=False, unbalanced_split=True)\n",
    "reportModelSelectionResult(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thename='REDDIT-BINARY'\n",
    "dataset = TUDataset(root='/tmp/'+thename, name=thename)\n",
    "print(dataset.num_classes)\n",
    "print(list(set([graph.y.item() for graph in dataset])))\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset, balanced=False, force_numclasses=2)\n",
    "reportModelSelectionResult(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thename='PROTEINS'\n",
    "dataset = TUDataset(root='/tmp/'+thename, name=thename)\n",
    "print(dataset.num_classes)\n",
    "print(list(set([graph.y.item() for graph in dataset])))\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset, balanced=False)\n",
    "reportModelSelectionResult(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "print(dataset.num_classes)\n",
    "print(list(set([graph.y.item() for graph in dataset])))\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thename='IMDB-BINARY'\n",
    "dataset = TUDataset(root='/tmp/'+thename, name=thename)\n",
    "print(dataset.num_classes)\n",
    "print(list(set([graph.y.item() for graph in dataset])))\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult\n",
    "\n",
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult\n",
    "\n",
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "#!cp -r models /content/drive/My\\ Drive/TFM/graph_classification/\n",
    "#!cp -r results /content/drive/My\\ Drive/TFM/graph_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. encapsulate all training, model selection,.. everything\n",
    "2. present results with Pandas tables, and histograms\n",
    "3. save models and results to disk, and load them later for testing\n",
    "4. transform into a python module or package\n",
    "5. prepare another notebook using the python module (prepare local and on collab)\n",
    "6. test other GNN layers: GAT, GCN, GraphSAGE, Metalayer\n",
    "7. do a good HP search\n",
    "\n",
    "### Pending:\n",
    "\n",
    "- repeat for PPI, PROTEINS, IMDB and REDDIT\n",
    "- look for published architectures?\n",
    "- compare with published benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
