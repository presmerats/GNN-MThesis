{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- loading of graph classification datasets such as PPI, PROTEINS or ENZYMES\n",
    "- using GNN for graph classification such as MPNN, GGNN and Graph Nets\n",
    "Goals:\n",
    "- how to load and feed the graphs to the models\n",
    "- how to use GNN models\n",
    "- compare GNN models\n",
    "- compare to published results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load a dataset and use GGNN to classify it (train-test setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "\n",
    "# shuffling dataset\n",
    "dataset = dataset.shuffle()\n",
    "#equivalent\n",
    "#perm = torch.randperm(len(dataset))\n",
    "#dataset = dataset[perm]\n",
    "\n",
    "# train-test setting\n",
    "train_dataset = dataset[:540]\n",
    "test_dataset = dataset[540:]\n",
    "\n",
    "print(\"num_classes:\",dataset.num_classes)\n",
    "\n",
    "# batch feeding\n",
    "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for batch in loader:\n",
    "    break\n",
    "    # 32 graphs in each batch\n",
    "    #batch\n",
    "    #>>> Batch(x=[1082, 21], edge_index=[2, 4066], y=[32], batch=[1082])\n",
    "    #print(batch.num_graphs)\n",
    "    #>>> 32\n",
    "    \n",
    "    #print(dir(batch))\n",
    "    \n",
    "    # the batch contains batch, a list of where does each node belong \n",
    "    #(to whhich graph of the batch does each node belong)\n",
    "    print(batch.batch)\n",
    "    #print(batch.test_mask)\n",
    "    \n",
    "    #print(dir(batch[0])) # this does not work\n",
    "    \n",
    "    # compute avereage node features in the node fimension for each graph individually\n",
    "    #x = scatter_mean(data.x, data.batch, dim=0)\n",
    "    #x.size()\n",
    "    #>>> torch.Size([32, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=50, num_layers=2,aggr='mean', bias=True)\n",
    "        self.fc1 = nn.Linear(50, 20)\n",
    "        self.fc2 = nn.Linear(20, 6)\n",
    "        self.global_pool = global_add_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=50, num_layers=2,aggr='add', bias=True)\n",
    "        self.ggnn2 = GatedGraphConv(out_channels=100, num_layers=2,aggr='mean', bias=True)\n",
    "        self.fc1 = nn.Linear(100, 20)\n",
    "        self.fc2 = nn.Linear(20, 6)\n",
    "        self.global_pool = global_add_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        x = self.ggnn2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "tensor(1.8054, grad_fn=<NllLossBackward>)\n",
      "epoch  1\n",
      "tensor(3.7293, grad_fn=<NllLossBackward>)\n",
      "epoch  2\n",
      "tensor(2.3745, grad_fn=<NllLossBackward>)\n",
      "epoch  3\n",
      "tensor(1.8006, grad_fn=<NllLossBackward>)\n",
      "epoch  4\n",
      "tensor(1.8407, grad_fn=<NllLossBackward>)\n",
      "epoch  5\n",
      "tensor(1.9914, grad_fn=<NllLossBackward>)\n",
      "epoch  6\n",
      "tensor(1.7778, grad_fn=<NllLossBackward>)\n",
      "epoch  7\n",
      "tensor(1.7347, grad_fn=<NllLossBackward>)\n",
      "epoch  8\n",
      "tensor(1.6641, grad_fn=<NllLossBackward>)\n",
      "epoch  9\n",
      "tensor(1.6718, grad_fn=<NllLossBackward>)\n",
      "epoch  10\n",
      "tensor(1.8714, grad_fn=<NllLossBackward>)\n",
      "epoch  11\n",
      "tensor(1.7405, grad_fn=<NllLossBackward>)\n",
      "epoch  12\n",
      "tensor(1.7233, grad_fn=<NllLossBackward>)\n",
      "epoch  13\n",
      "tensor(1.7195, grad_fn=<NllLossBackward>)\n",
      "epoch  14\n",
      "tensor(1.7201, grad_fn=<NllLossBackward>)\n",
      "epoch  15\n",
      "tensor(1.7054, grad_fn=<NllLossBackward>)\n",
      "epoch  16\n",
      "tensor(1.6878, grad_fn=<NllLossBackward>)\n",
      "epoch  17\n",
      "tensor(1.6725, grad_fn=<NllLossBackward>)\n",
      "epoch  18\n",
      "tensor(1.6757, grad_fn=<NllLossBackward>)\n",
      "epoch  19\n",
      "tensor(1.6550, grad_fn=<NllLossBackward>)\n",
      "epoch  20\n",
      "tensor(1.6478, grad_fn=<NllLossBackward>)\n",
      "epoch  21\n",
      "tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "epoch  22\n",
      "tensor(1.6417, grad_fn=<NllLossBackward>)\n",
      "epoch  23\n",
      "tensor(1.6423, grad_fn=<NllLossBackward>)\n",
      "epoch  24\n",
      "tensor(1.6308, grad_fn=<NllLossBackward>)\n",
      "epoch  25\n",
      "tensor(1.6233, grad_fn=<NllLossBackward>)\n",
      "epoch  26\n",
      "tensor(1.6416, grad_fn=<NllLossBackward>)\n",
      "epoch  27\n",
      "tensor(1.5911, grad_fn=<NllLossBackward>)\n",
      "epoch  28\n",
      "tensor(1.6361, grad_fn=<NllLossBackward>)\n",
      "epoch  29\n",
      "tensor(1.6308, grad_fn=<NllLossBackward>)\n",
      "epoch  30\n",
      "tensor(1.5650, grad_fn=<NllLossBackward>)\n",
      "epoch  31\n",
      "tensor(1.5531, grad_fn=<NllLossBackward>)\n",
      "epoch  32\n",
      "tensor(1.8489, grad_fn=<NllLossBackward>)\n",
      "epoch  33\n",
      "tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "epoch  34\n",
      "tensor(1.6252, grad_fn=<NllLossBackward>)\n",
      "epoch  35\n",
      "tensor(1.6248, grad_fn=<NllLossBackward>)\n",
      "epoch  36\n",
      "tensor(1.6016, grad_fn=<NllLossBackward>)\n",
      "epoch  37\n",
      "tensor(1.6156, grad_fn=<NllLossBackward>)\n",
      "epoch  38\n",
      "tensor(1.5886, grad_fn=<NllLossBackward>)\n",
      "epoch  39\n",
      "tensor(1.5683, grad_fn=<NllLossBackward>)\n",
      "epoch  40\n",
      "tensor(1.5859, grad_fn=<NllLossBackward>)\n",
      "epoch  41\n",
      "tensor(1.5471, grad_fn=<NllLossBackward>)\n",
      "epoch  42\n",
      "tensor(1.5427, grad_fn=<NllLossBackward>)\n",
      "epoch  43\n",
      "tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "epoch  44\n",
      "tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "epoch  45\n",
      "tensor(1.5779, grad_fn=<NllLossBackward>)\n",
      "epoch  46\n",
      "tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "epoch  47\n",
      "tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "epoch  48\n",
      "tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "epoch  49\n",
      "tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "epoch  50\n",
      "tensor(1.4684, grad_fn=<NllLossBackward>)\n",
      "epoch  51\n",
      "tensor(1.4375, grad_fn=<NllLossBackward>)\n",
      "epoch  52\n",
      "tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "epoch  53\n",
      "tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch  54\n",
      "tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch  55\n",
      "tensor(1.4206, grad_fn=<NllLossBackward>)\n",
      "epoch  56\n",
      "tensor(1.3872, grad_fn=<NllLossBackward>)\n",
      "epoch  57\n",
      "tensor(1.3930, grad_fn=<NllLossBackward>)\n",
      "epoch  58\n",
      "tensor(1.3815, grad_fn=<NllLossBackward>)\n",
      "epoch  59\n",
      "tensor(1.4013, grad_fn=<NllLossBackward>)\n",
      "epoch  60\n",
      "tensor(1.4052, grad_fn=<NllLossBackward>)\n",
      "epoch  61\n",
      "tensor(1.3350, grad_fn=<NllLossBackward>)\n",
      "epoch  62\n",
      "tensor(1.3450, grad_fn=<NllLossBackward>)\n",
      "epoch  63\n",
      "tensor(1.3595, grad_fn=<NllLossBackward>)\n",
      "epoch  64\n",
      "tensor(1.3517, grad_fn=<NllLossBackward>)\n",
      "epoch  65\n",
      "tensor(1.3574, grad_fn=<NllLossBackward>)\n",
      "epoch  66\n",
      "tensor(1.3473, grad_fn=<NllLossBackward>)\n",
      "epoch  67\n",
      "tensor(1.3602, grad_fn=<NllLossBackward>)\n",
      "epoch  68\n",
      "tensor(1.3859, grad_fn=<NllLossBackward>)\n",
      "epoch  69\n",
      "tensor(1.3981, grad_fn=<NllLossBackward>)\n",
      "epoch  70\n",
      "tensor(1.2973, grad_fn=<NllLossBackward>)\n",
      "epoch  71\n",
      "tensor(1.4174, grad_fn=<NllLossBackward>)\n",
      "epoch  72\n",
      "tensor(1.3834, grad_fn=<NllLossBackward>)\n",
      "epoch  73\n",
      "tensor(1.3236, grad_fn=<NllLossBackward>)\n",
      "epoch  74\n",
      "tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch  75\n",
      "tensor(1.4114, grad_fn=<NllLossBackward>)\n",
      "epoch  76\n",
      "tensor(1.3437, grad_fn=<NllLossBackward>)\n",
      "epoch  77\n",
      "tensor(1.3343, grad_fn=<NllLossBackward>)\n",
      "epoch  78\n",
      "tensor(1.3721, grad_fn=<NllLossBackward>)\n",
      "epoch  79\n",
      "tensor(1.2541, grad_fn=<NllLossBackward>)\n",
      "epoch  80\n",
      "tensor(1.2710, grad_fn=<NllLossBackward>)\n",
      "epoch  81\n",
      "tensor(1.2138, grad_fn=<NllLossBackward>)\n",
      "epoch  82\n",
      "tensor(1.2506, grad_fn=<NllLossBackward>)\n",
      "epoch  83\n",
      "tensor(1.2045, grad_fn=<NllLossBackward>)\n",
      "epoch  84\n",
      "tensor(1.2487, grad_fn=<NllLossBackward>)\n",
      "epoch  85\n",
      "tensor(1.1593, grad_fn=<NllLossBackward>)\n",
      "epoch  86\n",
      "tensor(1.2155, grad_fn=<NllLossBackward>)\n",
      "epoch  87\n",
      "tensor(1.1782, grad_fn=<NllLossBackward>)\n",
      "epoch  88\n",
      "tensor(1.1728, grad_fn=<NllLossBackward>)\n",
      "epoch  89\n",
      "tensor(1.2077, grad_fn=<NllLossBackward>)\n",
      "epoch  90\n",
      "tensor(1.3945, grad_fn=<NllLossBackward>)\n",
      "epoch  91\n",
      "tensor(1.1404, grad_fn=<NllLossBackward>)\n",
      "epoch  92\n",
      "tensor(1.1545, grad_fn=<NllLossBackward>)\n",
      "epoch  93\n",
      "tensor(1.1584, grad_fn=<NllLossBackward>)\n",
      "epoch  94\n",
      "tensor(1.1329, grad_fn=<NllLossBackward>)\n",
      "epoch  95\n",
      "tensor(1.1648, grad_fn=<NllLossBackward>)\n",
      "epoch  96\n",
      "tensor(1.1527, grad_fn=<NllLossBackward>)\n",
      "epoch  97\n",
      "tensor(1.1968, grad_fn=<NllLossBackward>)\n",
      "epoch  98\n",
      "tensor(1.1777, grad_fn=<NllLossBackward>)\n",
      "epoch  99\n",
      "tensor(1.0982, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "\n",
    "#print(dir(F))\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"epoch \",epoch)\n",
    "    for batch in loader:\n",
    "        #print(\" batch num graphs: \",batch.num_graphs)\n",
    "        #print(\" batch num nodes: \", len(batch.x))\n",
    "        \n",
    "        data = batch.to(device)\n",
    "        #print(\"data.y: \",data.y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # how does this work?\n",
    "        #print(\" out: \",out, \" out.dim: \", out.dim(), \" data.y.dim(): \", data.y.dim())\n",
    "        #out = out.view(-1,1)\n",
    "        #target = data.y.view(-1,1)\n",
    "        target = data.y\n",
    "        #print(\" out: \", out,\"\\n target: \",target)\n",
    "        # transform target to a one-hot-encoding\n",
    "        # output the softmax values for each example\n",
    "        loss = F.nll_loss(out, target)\n",
    "        # C = 6, \n",
    "        # target (N) means N values, where each value 0<= <= C-1=5\n",
    "        # out = (N,C) N values/rows of C classes/columns\n",
    "        #print(\"loss: \",loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  60\n",
      "num graphs:  60\n",
      "tensor([4, 4, 4, 1, 4, 3, 3, 1, 4, 4, 1, 4, 1, 4, 3, 1, 3, 1, 1, 1, 4, 1, 1, 4,\n",
      "        1, 1, 3, 1, 1, 4, 1, 1, 4, 4, 1, 3, 1, 4, 4, 1, 1, 1, 5, 1, 1, 1, 1, 5,\n",
      "        1, 5, 5, 3, 0, 4, 4, 2, 1, 1, 3, 4])\n",
      "tensor([2, 2, 5, 1, 4, 3, 0, 0, 4, 4, 1, 4, 0, 4, 1, 0, 3, 5, 4, 1, 4, 5, 4, 4,\n",
      "        5, 5, 0, 4, 5, 4, 0, 1, 4, 4, 1, 3, 5, 2, 4, 5, 1, 1, 5, 2, 1, 1, 1, 5,\n",
      "        0, 5, 5, 3, 0, 4, 4, 2, 0, 5, 3, 4])\n",
      "Accuracy: 0.5833\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"len(test_dataset): \", len(test_dataset))\n",
    "loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "for batch in loader:\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    #_, pred = model(test_dataset).max(dim=1)\n",
    "    _, pred = model(batch).max(dim=1)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    \n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    print('Accuracy: {:.4f}'.format(acc))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "1. trained a graph classification model (using global_add_pool)\n",
    "\n",
    "### Pending:\n",
    "- review nll_loss (it is negative now, data.y needs to be in one-hot-encoding?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load a dataset and use MPNN to classify it (train-validation-test setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load a dataset and use MetaLayer to classify it (train-validation-test setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implement the F1 scores and Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Implement a train-val-test setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
