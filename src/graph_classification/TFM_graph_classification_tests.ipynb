{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- loading of graph classification datasets such as PPI, PROTEINS or ENZYMES\n",
    "- using GNN for graph classification such as MPNN, GGNN and Graph Nets\n",
    "Goals:\n",
    "- how to load and feed the graphs to the models\n",
    "- how to use GNN models\n",
    "- compare GNN models\n",
    "- compare to published results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter as Param\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "\n",
    "class GatedGraphConv(MessagePassing):\n",
    "    r\"\"\"The gated graph convolution operator from the `\"Gated Graph Sequence\n",
    "    Neural Networks\" <https://arxiv.org/abs/1511.05493>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{h}_i^{(0)} &= \\mathbf{x}_i \\, \\Vert \\, \\mathbf{0}\n",
    "\n",
    "        \\mathbf{m}_i^{(l+1)} &= \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{\\Theta}\n",
    "        \\cdot \\mathbf{h}_j^{(l)}\n",
    "\n",
    "        \\mathbf{h}_i^{(l+1)} &= \\textrm{GRU} (\\mathbf{m}_i^{(l+1)},\n",
    "        \\mathbf{h}_i^{(l)})\n",
    "\n",
    "    up to representation :math:`\\mathbf{h}_i^{(L)}`.\n",
    "    The number of input channels of :math:`\\mathbf{x}_i` needs to be less or\n",
    "    equal than :obj:`out_channels`.\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): Size of each input sample.\n",
    "        num_layers (int): The sequence length :math:`L`.\n",
    "        aggr (string): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels, num_layers, aggr='add', bias=True):\n",
    "        super(GatedGraphConv, self).__init__(aggr)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.weight = Param(Tensor(num_layers, out_channels, out_channels))\n",
    "        self.rnn = torch.nn.GRUCell(out_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.out_channels\n",
    "        uniform(size, self.weight)\n",
    "        self.rnn.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\"\"\"\n",
    "        h = x if x.dim() == 2 else x.unsqueeze(-1)\n",
    "        assert h.size(1) <= self.out_channels\n",
    "\n",
    "        if h.size(1) < self.out_channels:\n",
    "            zero = h.new_zeros(h.size(0), self.out_channels - h.size(1))\n",
    "            h = torch.cat([h, zero], dim=1)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            m = torch.matmul(h, self.weight[i])\n",
    "            # original master 1.0.3 (new version with problems when using rnn)\n",
    "            #m = self.propagate(edge_index, x=m)\n",
    "            # hacky version to use with the pip installation of pytorch-geometric 20190325\n",
    "            m = self.propagate('add',edge_index, x=m)\n",
    "            h = self.rnn(m, h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, num_layers={})'.format(\n",
    "            self.__class__.__name__, self.out_channels, self.num_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net1, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net2, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Net3(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20, d3=10,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net3, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, d3)\n",
    "        self.dense2_bn = nn.BatchNorm1d(d3)\n",
    "        self.fc3 = nn.Linear(d3, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.dense2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class Net4(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net4, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models for hyperparameter search\n",
    "model_list =[\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}},\n",
    "    \n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}},\n",
    "]\n",
    "\n",
    "model_list2 = []\n",
    "for modelclass in [Net1, Net2, Net3,Net4]:\n",
    "    for d1 in [25,50,100,200]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [100,200,300]:\n",
    "                    model_list2.append(\n",
    "                        {\n",
    "                        'model': modelclass,\n",
    "                        'epochs': epochs,\n",
    "                        'kwargs':{'d1': d1,'d2': d2,'num_layers':2, \n",
    "                                  'aggr_type':aggr_type}},\n",
    "                    )\n",
    "                    \n",
    "model_list = model_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load a dataset and use GGNN to classify it (train-test setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "\n",
    "# shuffling dataset\n",
    "dataset = dataset.shuffle()\n",
    "#equivalent\n",
    "#perm = torch.randperm(len(dataset))\n",
    "#dataset = dataset[perm]\n",
    "\n",
    "# train-test setting\n",
    "train_dataset = dataset[:540]\n",
    "test_dataset = dataset[540:]\n",
    "\n",
    "print(\"num_classes:\",dataset.num_classes)\n",
    "\n",
    "# batch feeding\n",
    "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for batch in loader:\n",
    "    break\n",
    "    # 32 graphs in each batch\n",
    "    #batch\n",
    "    #>>> Batch(x=[1082, 21], edge_index=[2, 4066], y=[32], batch=[1082])\n",
    "    #print(batch.num_graphs)\n",
    "    #>>> 32\n",
    "    \n",
    "    #print(dir(batch))\n",
    "    \n",
    "    # the batch contains batch, a list of where does each node belong \n",
    "    #(to whhich graph of the batch does each node belong)\n",
    "    print(batch.batch)\n",
    "    #print(batch.test_mask)\n",
    "    \n",
    "    #print(dir(batch[0])) # this does not work\n",
    "    \n",
    "    # compute avereage node features in the node fimension for each graph individually\n",
    "    #x = scatter_mean(data.x, data.batch, dim=0)\n",
    "    #x.size()\n",
    "    #>>> torch.Size([32, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "tensor(1.8038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  1\n",
      "tensor(1.8178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  2\n",
      "tensor(1.7949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  3\n",
      "tensor(1.8255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  4\n",
      "tensor(1.6719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  5\n",
      "tensor(1.7289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  6\n",
      "tensor(1.6843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  7\n",
      "tensor(1.6204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  8\n",
      "tensor(1.7464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  9\n",
      "tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  10\n",
      "tensor(1.7534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  11\n",
      "tensor(1.7968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  12\n",
      "tensor(1.6996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  13\n",
      "tensor(1.7226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  14\n",
      "tensor(1.7017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  15\n",
      "tensor(1.7962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  16\n",
      "tensor(1.6409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  17\n",
      "tensor(1.7005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  18\n",
      "tensor(1.7430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  19\n",
      "tensor(1.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  20\n",
      "tensor(1.7728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  21\n",
      "tensor(1.7115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  22\n",
      "tensor(1.6791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  23\n",
      "tensor(1.5577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  24\n",
      "tensor(1.7115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  25\n",
      "tensor(1.6791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  26\n",
      "tensor(1.5714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  27\n",
      "tensor(1.6112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  28\n",
      "tensor(1.6344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  29\n",
      "tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  30\n",
      "tensor(1.6034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  31\n",
      "tensor(1.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  32\n",
      "tensor(1.6793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  33\n",
      "tensor(1.5853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  34\n",
      "tensor(1.6382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  35\n",
      "tensor(1.5510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  36\n",
      "tensor(1.7884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  37\n",
      "tensor(1.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  38\n",
      "tensor(1.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  39\n",
      "tensor(1.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  40\n",
      "tensor(1.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  41\n",
      "tensor(1.6600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  42\n",
      "tensor(1.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  43\n",
      "tensor(1.6475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  44\n",
      "tensor(1.6001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  45\n",
      "tensor(1.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  46\n",
      "tensor(1.6550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  47\n",
      "tensor(1.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  48\n",
      "tensor(1.6738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  49\n",
      "tensor(1.5965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  50\n",
      "tensor(1.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  51\n",
      "tensor(1.6653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  52\n",
      "tensor(1.7512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  53\n",
      "tensor(1.7341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  54\n",
      "tensor(1.6095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  55\n",
      "tensor(1.4261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  56\n",
      "tensor(1.6073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  57\n",
      "tensor(1.4698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  58\n",
      "tensor(1.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  59\n",
      "tensor(1.6109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  60\n",
      "tensor(1.8103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  61\n",
      "tensor(1.6202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  62\n",
      "tensor(1.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  63\n",
      "tensor(1.4226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  64\n",
      "tensor(1.8678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  65\n",
      "tensor(1.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  66\n",
      "tensor(1.6349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  67\n",
      "tensor(1.6991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  68\n",
      "tensor(1.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  69\n",
      "tensor(1.7049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  70\n",
      "tensor(1.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  71\n",
      "tensor(1.6472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  72\n",
      "tensor(1.6929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  73\n",
      "tensor(1.6221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  74\n",
      "tensor(1.7252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  75\n",
      "tensor(1.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  76\n",
      "tensor(1.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  77\n",
      "tensor(1.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  78\n",
      "tensor(1.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  79\n",
      "tensor(1.7280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  80\n",
      "tensor(1.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  81\n",
      "tensor(1.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  82\n",
      "tensor(1.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  83\n",
      "tensor(1.5025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  84\n",
      "tensor(1.6281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  85\n",
      "tensor(1.5904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  86\n",
      "tensor(1.9189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  87\n",
      "tensor(1.6204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  88\n",
      "tensor(1.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  89\n",
      "tensor(1.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  90\n",
      "tensor(1.5523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  91\n",
      "tensor(1.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  92\n",
      "tensor(1.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  93\n",
      "tensor(1.5067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  94\n",
      "tensor(1.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  95\n",
      "tensor(1.8952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  96\n",
      "tensor(1.6236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  97\n",
      "tensor(1.4358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  98\n",
      "tensor(1.8003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "epoch  99\n",
      "tensor(1.3587, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net1().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "model.train()\n",
    "\n",
    "#print(dir(F))\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"epoch \",epoch)\n",
    "    for batch in loader:\n",
    "        #print(\" batch num graphs: \",batch.num_graphs)\n",
    "        #print(\" batch num nodes: \", len(batch.x))\n",
    "        \n",
    "        data = batch.to(device)\n",
    "        #print(\"data.y: \",data.y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # how does this work?\n",
    "        #print(\" out: \",out, \" out.dim: \", out.dim(), \" data.y.dim(): \", data.y.dim())\n",
    "        #out = out.view(-1,1)\n",
    "        #target = data.y.view(-1,1)\n",
    "        target = data.y\n",
    "        #print(\" out: \", out,\"\\n target: \",target)\n",
    "        # transform target to a one-hot-encoding\n",
    "        # output the softmax values for each example\n",
    "        loss = F.nll_loss(out, target)\n",
    "        # C = 6, \n",
    "        # target (N) means N values, where each value 0<= <= C-1=5\n",
    "        # out = (N,C) N values/rows of C classes/columns\n",
    "        #print(\"loss: \",loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  60\n",
      "num graphs:  60\n",
      "tensor([0, 3, 0, 1, 3, 4, 1, 5, 5, 2, 5, 1, 3, 3, 3, 1, 0, 0, 4, 5, 3, 4, 3, 1,\n",
      "        3, 3, 3, 1, 3, 5, 0, 2, 3, 4, 2, 3, 1, 4, 3, 1, 2, 5, 3, 3, 3, 1, 4, 2,\n",
      "        1, 3, 4, 3, 4, 5, 4, 2, 3, 5, 5, 3], device='cuda:0')\n",
      "tensor([0, 2, 1, 5, 3, 1, 4, 4, 3, 2, 5, 1, 3, 2, 2, 1, 3, 4, 1, 5, 3, 3, 4, 0,\n",
      "        0, 3, 3, 1, 4, 5, 1, 2, 0, 2, 2, 4, 4, 2, 0, 0, 0, 5, 4, 0, 0, 5, 0, 2,\n",
      "        5, 4, 0, 3, 5, 5, 5, 2, 3, 3, 0, 3], device='cuda:0')\n",
      "Accuracy: 0.3667\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print(\"len(test_dataset): \", len(test_dataset))\n",
    "loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "for batch in loader:\n",
    "    batch = batch.to(device)\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    #_, pred = model(test_dataset).max(dim=1)\n",
    "    _, pred = model(batch).max(dim=1)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    \n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    print('Accuracy: {:.4f}'.format(acc))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "1. trained a graph classification model (using global_add_pool)\n",
    "2. reviewed nll_loss: needs log_softmax on output, and vectors of class id in the target (one-hot enc done insde nll_loss)\n",
    "\n",
    "### Pending:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load a dataset and use MPNN to classify it (train-validation-test setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 600  val_lim: 336  test_lim: 480\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "\n",
    "# shuffling dataset\n",
    "dataset = dataset.shuffle()\n",
    "#equivalent\n",
    "#perm = torch.randperm(len(dataset))\n",
    "#dataset = dataset[perm]\n",
    "\n",
    "# train-val-test setting\n",
    "n = len(dataset)\n",
    "test_lim= int(0.8*n)\n",
    "val_lim=int(0.8*n*0.7)\n",
    "train_dataset = dataset[:val_lim]\n",
    "val_dataset = dataset[val_lim:test_lim]\n",
    "test_dataset = dataset[test_lim:]\n",
    "print(\" n:\",n,\" val_lim:\",val_lim,\" test_lim:\",test_lim)\n",
    "\n",
    "\n",
    "# batch feeding\n",
    "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "loader_val = DataLoader(val_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 0  train loss: 0.058928173035383224  val loss: 0.062085118144750595\n",
      " epoch: 1  train loss: 0.0585266537964344  val loss: 0.06184013560414314\n",
      " epoch: 2  train loss: 0.05815182626247406  val loss: 0.06170050427317619\n",
      " epoch: 3  train loss: 0.05785343423485756  val loss: 0.062231261283159256\n",
      " epoch: 4  train loss: 0.05791543796658516  val loss: 0.061325814574956894\n",
      " epoch: 5  train loss: 0.05772858113050461  val loss: 0.06141205132007599\n",
      " epoch: 6  train loss: 0.057644322514534  val loss: 0.061759836971759796\n",
      " epoch: 7  train loss: 0.05713678523898125  val loss: 0.06161436811089516\n",
      " epoch: 8  train loss: 0.05711313337087631  val loss: 0.06190349534153938\n",
      " epoch: 9  train loss: 0.05724392086267471  val loss: 0.06133158877491951\n",
      " epoch: 10  train loss: 0.05697968974709511  val loss: 0.06169113144278526\n",
      " epoch: 11  train loss: 0.05740954726934433  val loss: 0.061494164168834686\n",
      " epoch: 12  train loss: 0.05702315643429756  val loss: 0.06123407557606697\n",
      " epoch: 13  train loss: 0.05707987770438194  val loss: 0.061742331832647324\n",
      " epoch: 14  train loss: 0.056900542229413986  val loss: 0.06172298267483711\n",
      " epoch: 15  train loss: 0.05689483880996704  val loss: 0.06211172416806221\n",
      " epoch: 16  train loss: 0.056951187551021576  val loss: 0.06138614937663078\n",
      " epoch: 17  train loss: 0.05696619674563408  val loss: 0.062009770423173904\n",
      " epoch: 18  train loss: 0.056275852024555206  val loss: 0.06162701174616814\n",
      " epoch: 19  train loss: 0.05648920685052872  val loss: 0.06207597255706787\n",
      " epoch: 20  train loss: 0.055915817618370056  val loss: 0.061277277767658234\n",
      " epoch: 21  train loss: 0.05580843612551689  val loss: 0.06233813986182213\n",
      " epoch: 22  train loss: 0.056303057819604874  val loss: 0.062238771468400955\n",
      " epoch: 23  train loss: 0.0559704527258873  val loss: 0.06200012192130089\n",
      " epoch: 24  train loss: 0.05616341903805733  val loss: 0.06172613427042961\n",
      " epoch: 25  train loss: 0.05631614848971367  val loss: 0.0615907646715641\n",
      " epoch: 26  train loss: 0.05575565993785858  val loss: 0.061639249324798584\n",
      " epoch: 27  train loss: 0.055376969277858734  val loss: 0.06240754574537277\n",
      " epoch: 28  train loss: 0.05611111596226692  val loss: 0.062251586467027664\n",
      " epoch: 29  train loss: 0.05556328594684601  val loss: 0.06246696040034294\n",
      " epoch: 30  train loss: 0.05588250607252121  val loss: 0.06185047701001167\n",
      " epoch: 31  train loss: 0.056009095162153244  val loss: 0.06228650361299515\n",
      " epoch: 32  train loss: 0.055057913064956665  val loss: 0.06405226141214371\n",
      " epoch: 33  train loss: 0.05518311634659767  val loss: 0.06176171079277992\n",
      " epoch: 34  train loss: 0.055603861808776855  val loss: 0.06285408139228821\n",
      " epoch: 35  train loss: 0.054947156459093094  val loss: 0.06173107400536537\n",
      " epoch: 36  train loss: 0.05566307157278061  val loss: 0.06093839183449745\n",
      " epoch: 37  train loss: 0.05446064472198486  val loss: 0.0611773319542408\n",
      " epoch: 38  train loss: 0.05461174622178078  val loss: 0.06254182010889053\n",
      " epoch: 39  train loss: 0.054870594292879105  val loss: 0.06131363660097122\n",
      " epoch: 40  train loss: 0.054285600781440735  val loss: 0.06254643946886063\n",
      " epoch: 41  train loss: 0.05537249892950058  val loss: 0.063417948782444\n",
      " epoch: 42  train loss: 0.05527671426534653  val loss: 0.06114889681339264\n",
      " epoch: 43  train loss: 0.05415654927492142  val loss: 0.06001100316643715\n",
      " epoch: 44  train loss: 0.05377574265003204  val loss: 0.06201284006237984\n",
      " epoch: 45  train loss: 0.05354052782058716  val loss: 0.061765722930431366\n",
      " epoch: 46  train loss: 0.05305415391921997  val loss: 0.061394527554512024\n",
      " epoch: 47  train loss: 0.052885521203279495  val loss: 0.06173764541745186\n",
      " epoch: 48  train loss: 0.053437650203704834  val loss: 0.06229135021567345\n",
      " epoch: 49  train loss: 0.053286075592041016  val loss: 0.0600113682448864\n",
      " epoch: 50  train loss: 0.0521971620619297  val loss: 0.06095368415117264\n",
      " epoch: 51  train loss: 0.05283699929714203  val loss: 0.061130914837121964\n",
      " epoch: 52  train loss: 0.05261635780334473  val loss: 0.060263846069574356\n",
      " epoch: 53  train loss: 0.05335121601819992  val loss: 0.05912262946367264\n",
      " epoch: 54  train loss: 0.051893699914216995  val loss: 0.05900951102375984\n",
      " epoch: 55  train loss: 0.05225706100463867  val loss: 0.061941392719745636\n",
      " epoch: 56  train loss: 0.05193772166967392  val loss: 0.059798963367938995\n",
      " epoch: 57  train loss: 0.05115901306271553  val loss: 0.059891726821660995\n",
      " epoch: 58  train loss: 0.05151255056262016  val loss: 0.06055149808526039\n",
      " epoch: 59  train loss: 0.05101479962468147  val loss: 0.060193054378032684\n",
      " epoch: 60  train loss: 0.05164319649338722  val loss: 0.060189493000507355\n",
      " epoch: 61  train loss: 0.05020861327648163  val loss: 0.06186797097325325\n",
      " epoch: 62  train loss: 0.0502452477812767  val loss: 0.059667833149433136\n",
      " epoch: 63  train loss: 0.04990513622760773  val loss: 0.06124712899327278\n",
      " epoch: 64  train loss: 0.05011970177292824  val loss: 0.06074143946170807\n",
      " epoch: 65  train loss: 0.050208911299705505  val loss: 0.06057904288172722\n",
      " epoch: 66  train loss: 0.05046563968062401  val loss: 0.06164562702178955\n",
      " epoch: 67  train loss: 0.05148887634277344  val loss: 0.059211842715740204\n",
      " epoch: 68  train loss: 0.049656640738248825  val loss: 0.05973805487155914\n",
      " epoch: 69  train loss: 0.04962794482707977  val loss: 0.06076492369174957\n",
      " epoch: 70  train loss: 0.04872025176882744  val loss: 0.05981170013546944\n",
      " epoch: 71  train loss: 0.04984984174370766  val loss: 0.0602991059422493\n",
      " epoch: 72  train loss: 0.05015379562973976  val loss: 0.060953956097364426\n",
      " epoch: 73  train loss: 0.04942668601870537  val loss: 0.06119280308485031\n",
      " epoch: 74  train loss: 0.05014454945921898  val loss: 0.06459105014801025\n",
      " epoch: 75  train loss: 0.04996223747730255  val loss: 0.06106306612491608\n",
      " epoch: 76  train loss: 0.04945593699812889  val loss: 0.059721339493989944\n",
      " epoch: 77  train loss: 0.049448031932115555  val loss: 0.059897199273109436\n",
      " epoch: 78  train loss: 0.04916540905833244  val loss: 0.060805242508649826\n",
      " epoch: 79  train loss: 0.049674782902002335  val loss: 0.05932675674557686\n",
      " epoch: 80  train loss: 0.04967937618494034  val loss: 0.060335446149110794\n",
      " epoch: 81  train loss: 0.04888857156038284  val loss: 0.061490509659051895\n",
      " epoch: 82  train loss: 0.049033768475055695  val loss: 0.06201569363474846\n",
      " epoch: 83  train loss: 0.05016697198152542  val loss: 0.062288325279951096\n",
      " epoch: 84  train loss: 0.048290178179740906  val loss: 0.06284012645483017\n",
      " epoch: 85  train loss: 0.04849625378847122  val loss: 0.06076355651021004\n",
      " epoch: 86  train loss: 0.04914313182234764  val loss: 0.059674348682165146\n",
      " epoch: 87  train loss: 0.04928029328584671  val loss: 0.06143077090382576\n",
      " epoch: 88  train loss: 0.04799902066588402  val loss: 0.06252600252628326\n",
      " epoch: 89  train loss: 0.04814305901527405  val loss: 0.06302379071712494\n",
      " epoch: 90  train loss: 0.04812068119645119  val loss: 0.06082385033369064\n",
      " epoch: 91  train loss: 0.0482122004032135  val loss: 0.06059948727488518\n",
      " epoch: 92  train loss: 0.049169979989528656  val loss: 0.0613541342318058\n",
      " epoch: 93  train loss: 0.04866110906004906  val loss: 0.05997061729431152\n",
      " epoch: 94  train loss: 0.048037752509117126  val loss: 0.06183040887117386\n",
      " epoch: 95  train loss: 0.047759994864463806  val loss: 0.06346593797206879\n",
      " epoch: 96  train loss: 0.04794065281748772  val loss: 0.06037825345993042\n",
      " epoch: 97  train loss: 0.04813893139362335  val loss: 0.062087588012218475\n",
      " epoch: 98  train loss: 0.046824563294649124  val loss: 0.06089428439736366\n",
      " epoch: 99  train loss: 0.047309018671512604  val loss: 0.06057002767920494\n",
      " epoch: 100  train loss: 0.04774852097034454  val loss: 0.06198827549815178\n",
      " epoch: 101  train loss: 0.049096204340457916  val loss: 0.06159903481602669\n",
      " epoch: 102  train loss: 0.048013098537921906  val loss: 0.06032942607998848\n",
      " epoch: 103  train loss: 0.048230160027742386  val loss: 0.06067417189478874\n",
      " epoch: 104  train loss: 0.04869939759373665  val loss: 0.06214136630296707\n",
      " epoch: 105  train loss: 0.04820788651704788  val loss: 0.061871487647295\n",
      " epoch: 106  train loss: 0.04853251948952675  val loss: 0.058598101139068604\n",
      " epoch: 107  train loss: 0.04812921956181526  val loss: 0.06143558770418167\n",
      " epoch: 108  train loss: 0.046992894262075424  val loss: 0.059705402702093124\n",
      " epoch: 109  train loss: 0.04760488122701645  val loss: 0.05950367450714111\n",
      " epoch: 110  train loss: 0.046839144080877304  val loss: 0.06433630734682083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 111  train loss: 0.04774610325694084  val loss: 0.06160545349121094\n",
      " epoch: 112  train loss: 0.04807206243276596  val loss: 0.061285700649023056\n",
      " epoch: 113  train loss: 0.04802212491631508  val loss: 0.05872964859008789\n",
      " epoch: 114  train loss: 0.0464320071041584  val loss: 0.06033194810152054\n",
      " epoch: 115  train loss: 0.04734060540795326  val loss: 0.06036359816789627\n",
      " epoch: 116  train loss: 0.04745844379067421  val loss: 0.06060352548956871\n",
      " epoch: 117  train loss: 0.047824833542108536  val loss: 0.06102550029754639\n",
      " epoch: 118  train loss: 0.04683050885796547  val loss: 0.060989294201135635\n",
      " epoch: 119  train loss: 0.0471283495426178  val loss: 0.060817766934633255\n",
      " epoch: 120  train loss: 0.04654458165168762  val loss: 0.05880863964557648\n",
      " epoch: 121  train loss: 0.04648607596755028  val loss: 0.05892401188611984\n",
      " epoch: 122  train loss: 0.045622941106557846  val loss: 0.05947423726320267\n",
      " epoch: 123  train loss: 0.04594486579298973  val loss: 0.05981556698679924\n",
      " epoch: 124  train loss: 0.045448463410139084  val loss: 0.06126825138926506\n",
      " epoch: 125  train loss: 0.04724051430821419  val loss: 0.0592772550880909\n",
      " epoch: 126  train loss: 0.04601527005434036  val loss: 0.05827295780181885\n",
      " epoch: 127  train loss: 0.046143271028995514  val loss: 0.05982939526438713\n",
      " epoch: 128  train loss: 0.04627281054854393  val loss: 0.06404928863048553\n",
      " epoch: 129  train loss: 0.0473128966987133  val loss: 0.0594061017036438\n",
      " epoch: 130  train loss: 0.0456327423453331  val loss: 0.0618019700050354\n",
      " epoch: 131  train loss: 0.046335555613040924  val loss: 0.0599132776260376\n",
      " epoch: 132  train loss: 0.04533800855278969  val loss: 0.05888120457530022\n",
      " epoch: 133  train loss: 0.04638798162341118  val loss: 0.061175387352705\n",
      " epoch: 134  train loss: 0.04530129209160805  val loss: 0.060277003794908524\n",
      " epoch: 135  train loss: 0.04426710680127144  val loss: 0.060308534651994705\n",
      " epoch: 136  train loss: 0.04429316148161888  val loss: 0.05937784165143967\n",
      " epoch: 137  train loss: 0.04530814662575722  val loss: 0.060005951672792435\n",
      " epoch: 138  train loss: 0.04469148814678192  val loss: 0.060711801052093506\n",
      " epoch: 139  train loss: 0.04440579190850258  val loss: 0.05843747407197952\n",
      " epoch: 140  train loss: 0.04412353038787842  val loss: 0.06128943711519241\n",
      " epoch: 141  train loss: 0.04452362284064293  val loss: 0.060034699738025665\n",
      " epoch: 142  train loss: 0.044055406004190445  val loss: 0.06150776892900467\n",
      " epoch: 143  train loss: 0.044443126767873764  val loss: 0.06049405410885811\n",
      " epoch: 144  train loss: 0.04492364451289177  val loss: 0.0648529976606369\n",
      " epoch: 145  train loss: 0.04436863958835602  val loss: 0.0595177561044693\n",
      " epoch: 146  train loss: 0.04365748167037964  val loss: 0.06247515231370926\n",
      " epoch: 147  train loss: 0.04398943483829498  val loss: 0.059961896389722824\n",
      " epoch: 148  train loss: 0.0433952659368515  val loss: 0.061956737190485\n",
      " epoch: 149  train loss: 0.042582567781209946  val loss: 0.06382548063993454\n",
      " epoch: 150  train loss: 0.042938899248838425  val loss: 0.06237604841589928\n",
      " epoch: 151  train loss: 0.042141642421483994  val loss: 0.060088932514190674\n",
      " epoch: 152  train loss: 0.041455335915088654  val loss: 0.060507919639348984\n",
      " epoch: 153  train loss: 0.04376998543739319  val loss: 0.06144293025135994\n",
      " epoch: 154  train loss: 0.0436210073530674  val loss: 0.06408129632472992\n",
      " epoch: 155  train loss: 0.043672580271959305  val loss: 0.06034115329384804\n",
      " epoch: 156  train loss: 0.04424586147069931  val loss: 0.05949782207608223\n",
      " epoch: 157  train loss: 0.04441593587398529  val loss: 0.062333039939403534\n",
      " epoch: 158  train loss: 0.04473915323615074  val loss: 0.06249941140413284\n",
      " epoch: 159  train loss: 0.042942311614751816  val loss: 0.062447063624858856\n",
      " epoch: 160  train loss: 0.04222162812948227  val loss: 0.06444112211465836\n",
      " epoch: 161  train loss: 0.04280919209122658  val loss: 0.06031129136681557\n",
      " epoch: 162  train loss: 0.04359888657927513  val loss: 0.06185928359627724\n",
      " epoch: 163  train loss: 0.04338488355278969  val loss: 0.06036067008972168\n",
      " epoch: 164  train loss: 0.04382810741662979  val loss: 0.0609852559864521\n",
      " epoch: 165  train loss: 0.0459190234541893  val loss: 0.06605490297079086\n",
      " epoch: 166  train loss: 0.044535476714372635  val loss: 0.06228066235780716\n",
      " epoch: 167  train loss: 0.04418753460049629  val loss: 0.060638267546892166\n",
      " epoch: 168  train loss: 0.04360748454928398  val loss: 0.062334317713975906\n",
      " epoch: 169  train loss: 0.04181084781885147  val loss: 0.061657026410102844\n",
      " epoch: 170  train loss: 0.04084505885839462  val loss: 0.059351127594709396\n",
      " epoch: 171  train loss: 0.041180744767189026  val loss: 0.062182340770959854\n",
      " epoch: 172  train loss: 0.04267274588346481  val loss: 0.061046965420246124\n",
      " epoch: 173  train loss: 0.04293615743517876  val loss: 0.06249336525797844\n",
      " epoch: 174  train loss: 0.042769379913806915  val loss: 0.06320015341043472\n",
      " epoch: 175  train loss: 0.04341892898082733  val loss: 0.06133824586868286\n",
      " epoch: 176  train loss: 0.04330883175134659  val loss: 0.06043875217437744\n",
      " epoch: 177  train loss: 0.042191486805677414  val loss: 0.06177150085568428\n",
      " epoch: 178  train loss: 0.04108288139104843  val loss: 0.06138436868786812\n",
      " epoch: 179  train loss: 0.04123507812619209  val loss: 0.06278069317340851\n",
      " epoch: 180  train loss: 0.04162447154521942  val loss: 0.06361296027898788\n",
      " epoch: 181  train loss: 0.04257873818278313  val loss: 0.06103874742984772\n",
      " epoch: 182  train loss: 0.04170325770974159  val loss: 0.06466057151556015\n",
      " epoch: 183  train loss: 0.04000258073210716  val loss: 0.06169707328081131\n",
      " epoch: 184  train loss: 0.04035457596182823  val loss: 0.06356410682201385\n",
      " epoch: 185  train loss: 0.040912844240665436  val loss: 0.06146499142050743\n",
      " epoch: 186  train loss: 0.04075368866324425  val loss: 0.06250035017728806\n",
      " epoch: 187  train loss: 0.039690662175416946  val loss: 0.0635140985250473\n",
      " epoch: 188  train loss: 0.04014858976006508  val loss: 0.06246915087103844\n",
      " epoch: 189  train loss: 0.04168003052473068  val loss: 0.06124264746904373\n",
      " epoch: 190  train loss: 0.040227945894002914  val loss: 0.06059752032160759\n",
      " epoch: 191  train loss: 0.0407852828502655  val loss: 0.062012143433094025\n",
      " epoch: 192  train loss: 0.039473582059144974  val loss: 0.06271449476480484\n",
      " epoch: 193  train loss: 0.040150027722120285  val loss: 0.06420303881168365\n",
      " epoch: 194  train loss: 0.04069589450955391  val loss: 0.061993181705474854\n",
      " epoch: 195  train loss: 0.0402434878051281  val loss: 0.06162967160344124\n",
      " epoch: 196  train loss: 0.04005909711122513  val loss: 0.06458257138729095\n",
      " epoch: 197  train loss: 0.04024035483598709  val loss: 0.06185561418533325\n",
      " epoch: 198  train loss: 0.04041153937578201  val loss: 0.06166413798928261\n",
      " epoch: 199  train loss: 0.03962215408682823  val loss: 0.06257294863462448\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net1().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(out, target)\n",
    "        loss_train +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train = loss_train / len(train_dataset)\n",
    "    train_loss_history.append(loss_train.item())\n",
    "        \n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    for batch in loader_val:\n",
    "        data = batch.to(device)\n",
    "        \n",
    "        pred = model(batch)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        loss_val += loss\n",
    "    loss_val = loss_val / len(val_dataset)\n",
    "    val_loss_history.append(loss_val.item())\n",
    "        \n",
    "    print(\" epoch:\",epoch,\" train loss:\",loss_train.item(),\" val loss:\", loss_val.item())\n",
    "        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train & validation loss')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot evolution of validation and training loss\n",
    "#plt.plot(val_loss_history, color = 'orange', linewidth = 2, label='validation')\n",
    "#plt.plot(train_loss_history, color = 'blue', linewidth = 2,label='train')\n",
    "\n",
    "#plt.legend()\n",
    "#plt.xlabel('# epochs')\n",
    "#plt.ylabel('loss')\n",
    "#plt.title('Train & validation loss')\n",
    "\n",
    "# with 300 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train & validation loss')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VFX6xz9vCgkkoYXepIqCYgF7RewiiqLYXde6a1nrWte1rO7aXcvyW1ddu4LYcO0KCDYkICCICEqvoZckpJ3fH+893Ds3M8mEZFLP53ny3Du3zbmTmfO9bznvEWMMDofD4XDsLEm13QCHw+Fw1G+ckDgcDoejSjghcTgcDkeVcELicDgcjirhhMThcDgcVcIJicPhcDiqhBMSR71HRJJFZKuIdKvttlSEiBwtIosCr+eJyGHxHLsT7/WsiNy2s+eXc92/icgL1X1dR/3FCYmjxvE6fftXKiL5gdfnVvZ6xpgSY0ymMWbJTrYnXUReFZENIrJGRB7ZmevsDMaYvsaYyVW9johcIiITQ9e+xBhzf1Wv7XBUREptN8DR+DDGZNp174n7EmPM57GOF5EUY0xxApt0MbAH0AMoBgYl8L0cjgaHs0gcdQ7PdTJaRF4XkS3AeSJykIh8JyIbRWSliDwhIqne8SkiYkSku/f6FW//RyKyRUS+FZEe5bxlEbDRGLPRGLPVGDOxgvbdLiJvhLY9LSKPeuuXiMhc771/FZFLyrnWMhE50ltvJiIve5bRHGBg6Ng7ROQ377pzRGSYt31P4CngMM+qWxv4HO4KnH+FiCwQkXUi8q6IdAx9fpd7+zeIyBPlfQahdg332rNRRMaLSN/AvttEZIWIbBaRnwP3eqCITPe2rxaRh+J9P0fdwwmJo64yHHgNaAGMRi2FPwFtgEOA44HLyzn/HOAvQGtgCXBvOcfmoJ3wnXG27XVgqIhkgHbEwBleewFWAycBzYFLgSdFZEAc170H6Ar0BE4ELgzt/wW99xbAfcBrItLeGPMjcBUw2XPxtQlfWESO9a4/AugMrABeDR12Iipe+6DifXRFDRaR3YGXgauBtsDnwDgRSRWR/uj/aF9jTHPgBPR/AfAk8JC3vTcwtqL3ctRdnJA46ipfGWPeN8aUGmPyjTFTjTFTjDHFxpjfgGeAI8o5f6wxJscYU4R2mHtHO0hE2gDvocI0TETuCOxb5XWUEXjvPxs4xdt0DLDBGJPj7X/fGPObUcYDXwBRA+ohzgT+ZozZYIxZjFoZwfcdY4xZ6X0mrwGLiN8Ndy7wrDFmhjGmALgFOEJEugSO+bsxZpMxZhEwkRifWYizgHHGmPHeZ/0PVOgOQMU/HejvuScXep8dqBXYR0SyjTFbjDFT4rwPRx3ECYmjrrI0+EJEdhORD7zOfTP6dF3myTvAqsB6HpAZ47iRwCxjzKeomJzruZB6AaXAzzHOew0421s/B98aQUSGisgUEVkvIhuBYytoq6Ujkfe9OLhTRH4nIjM9F9JGYLc4rwvQKXg9Y8xmYANqnVji/czKu24psAzobIyZB9yA/q/WeK7KDt6hFwH9gHki8r2InBjnfTjqIE5IHHWVcFnqf6NWQG/PHXInINXwPilAKoAxZi1qXVwKfAg8aGKXxx4DHC0inVHL5DUAEWmKumn+DrQ3xrQEPo2zratQ15ZlRzqziPQERgF/ALK96/4cuG5FZbxXALsErpcFtAKWx9Guylw3Cehir2uMecUYcwiayJCMfi4YY+YZY84C2gGPAG+JSHoV2+KoJZyQOOoLWcAmYJvnbiovPlIZPgAO9gLkqUAh8C2wK/pUHhVjzCrgK+AFYJ4xZr63Kw1oAuQCJSIyFBgSZ1vGALeJSEvRMTFXBfZlomKRC4iIXIpaJJbVQBebgBCF14GLRWSAiKShHfpkY8yyONtWXpuHiciR3nvfBGwBpojI7iIy2Hu/fO+vFL2B80WkjWfBbPLurbSKbXHUEk5IHPWFG9Dg8xbUOhldHRc1xixAA+MXA+uAGahr5ijgERE5ppzTXwOOJuDWMsZsBK4D3gHWo8Ht/8XZnL8CK9HYx0fAS4HrzkID1N97x/QFgnGFz4D5wGoRCbqo7Pkfoy6md7zzu6FxkyphjJmD/l9GoSJ3PDDMi5ekAQ8Ca1FrqxVwu3fqicBc0ay8h4GRxpjCqrbHUTuIm9jK4XA4HFXBWSQOh8PhqBJOSBwOh8NRJZyQOBwOh6NKOCFxOBwOR5VoFEUb27RpY7p3717bzXA4HI56xbRp09YaY9pWdFyjEJLu3buTk5NT281wOByOeoWILK74KOfacjgcDkcVcULicDgcjirhhMThcDgcVcIJicPhcDiqhBMSh8PhcFQJJyQOh8PhqBJOSBwOh8NRJZyQOBwOB8CKj+Cb86A45jQ0jhg4IXE4HA6Anx+DRa/C6om13ZJ6hxMSh8PhACjeqsuiTbXbjnqIExKHw+EAKN6my6LNtduOeogTEofD4QA/NuKEpNI4IXE4HA6AEickO0tChUREjheReSKyQERuibI/TURGe/uniEj3wL4BIvKtiMwRkR9FJN3bPtG75gzvr10i78HhcDQSnGtrp0lYGXkRSQaeBo4BlgFTRWScMeanwGEXAxuMMb1F5CzgAWCkiKQArwDnG2Nmikg2UBQ471xjjKsL73A4qg9rkRQ7IaksibRI9gcWGGN+M8YUAm8Ap4SOOQV40VsfCwwREQGOBWYZY2YCGGPWGWNKEthWh8PRmCkt0j9wFslOkEgh6QwsDbxe5m2LeowxphjYBGQDuwJGRD4Rkeki8ufQef/13Fp/8YSnDCJymYjkiEhObm5uddyPw+FIJPNHwddnQ2ktPDMGByEWban596/n1NVgewpwKHCutxwuIkO8fecaY/YEDvP+zo92AWPMM8aYQcaYQW3bVjhTpMPhqG1+fgwWvwGb59b8e5cEhSTBFknBGlj5GRiT2PepQRIpJMuBroHXXbxtUY/x4iItgHWo9TLJGLPWGJMHfAjsC2CMWe4ttwCvoS40hyM6s/4Kk88AU1rbLXFUhLUEaqNEiQ20Q+KFJOcamHAsrPu++q5ZuAEmngzL3q++a1aCRArJVKCPiPQQkSbAWcC40DHjgAu99RHAeGOMAT4B9hSRZp7AHAH8JCIpItIGQERSgaHA7ATeg6O+88tTsHQs5C2t+FhH7WJHlpckSEgKcuGrsyD36yjvXYMWSd4SXW5dWH3XXPU5rPgfzH+6+q5ZCRKWtWWMKRaRq1BRSAaeN8bMEZF7gBxjzDjgOeBlEVkArEfFBmPMBhF5FBUjA3xojPlARDKATzwRSQY+B/6TqHtw1HOM8ctdFG6CjNptTrXzw03qIjn2W0hpWtutqRqm1LcKEmWRLHsHloyGpBRoe0jkvpp0bdnvZNGG6rvm9vXecm31XbMSJExIAIwxH6JuqeC2OwPrBcAZMc59BU0BDm7bBgys/pY6GiTF28Am+zXE+klL3oJtCzWm0Hrf2m5N1SjJR58Z2XmLpLRYRSIWect0WRjluxB0bRVvUWGTBDls7Pvbzn9nmHIJbJgJx34DSalQaIVkXdXbtxPU1WC7ozooLYYvT4E5/6jtltQORRsD6w1QSEq8zq8mnkK3r4Pxx8DSdxJz/aKt/npJfuXPX/Q6vJmlFlos8lfosnhr2X1hKyjaMdWFtXgKd1JITCksfAXW58DW3yKv5YTEUe1s/BGWj4NfnqztltQOwSfPaE+h9R3b+dVE57F6gvrhf30uMdcvDqTcxnJtbV+vD0fRWDMRSgpg3ZTY75FnhSRKem/QIoHEubdKS/z331khyVsOpdt1PX+VLq11U7wFSgqr1sadwAlJQ8YG9QrWNM6spYZskRjjd341ISQ7MqoS9KQevG4011b+Kni3M3x9VvTzrVVW3v8530sajTZOJPyeiRKS4H0W7mSMZOsCf71gVdlrFda8VeKEpCGzzRMSU1w1f2x9pbABC0lJATtiCjXh2rJP0eEn9+oi6NqKZpFsnKX3nDs5+vk7hKQcAcgvzyJJsJCsy4F5T0V+D62FZd1T8bLlV3/dWiRB66YW3FtOSBoywZTXgtWVO7c4v/4PmAr+aGtLSDbOgd9eqv7PMtih18QTqH2STpSQVGSR5HnWRMGa6J287TxjCUDJ9oDYRLGqShLs2vrudzDtalj1hb+tcD3MuBnG9fJTkk0pTLlUxz/FYmtASAqckDgSjbVIoHJCkvutBi7nPV79bapJghZJbcVIpl4O312oT9TVSbDja2iurWgWic24AtiyoOz+iiyS/JWR7xUW9kRaJAVrYNMcXQ+O2i9cD+un6fqGGbrcNBd+fRbm3KcPc9EI3r/9XQc9DrWQAuyEpCGTt5NCsuRNTZtd82X1t6kmqQsWyeZ5urRuleoiaBnUiGurli2S/EBRjC3zI/cZE4eQBD9/U/Y+qhIjWTMZVnxSzv7A7yhoTRRu8AXSCp0VFlMS++FjawWuLRcjcVQrO22ReH7o/Eq6w+oaiY6RFG4q39Ip2up3cDsbWI1FcQ1bJIkWkogYSZQn8bygkIQskqJNgfFCsYQkVJ0pHCex9yUp5V8nTPE2mHA8TBoWu9jj6oCQBOMbJQWwbbHXvpCQAGyYXvZaxpR1bZUURKZMO9eWo9ooLYp8CotXSIq2woYfKndOXSWRFklJIXywO3y4p19+PIztJCCxQlITT6C2kyzdHjsFtypUxiLZGhKSoEUWSwDyQhZhOE5iXVtNO5R/nTCrJ2h7Swtjl+GJZZGAJsJAQEgC0ywFRcWyfa3XNq/oecGqst8t59pyVBt5y9mR1QMqKhNPhqlXlX/euu/8p7uCVfU74F7VGMmqL+CLoyKfhi3L3tEff97S2IK7LVBLqaFYJOH3Tsj1K4qRhFxb8QhJ2LUYyyJJr6SQLP8gehstBWthU6AcYKwYU8FKFWgbKwFYH8UisULUop933pqywuEsEke1EX46WvWFFnVbMCp2EA9gzVf+ekl+Ykf4JpoIi2Qngqfjj9YnzrkPl903/1/+uvVTh9m6yF9PpJAUb9WspEQSdNsk4jtRnkVSsh22B+YUCru2wkIS7eEn7NoKu6Hse1ZGSIyBFYEKUNGEZN13upQKqlHlr4TNP2s70tsDogIU/r/ae2/RD5q01oe+MsLqCUkNzuvihKShYd0ONj6SsYsu7Q/JlPoZJJal72jJBYDcryL31Wf3VlViJBtmxt63cQ6smeS/LoghJNsWBdqSQCGBxD+F1rRFYozvfrJun6YdISlNv5NBIQgKiSmOjBeUlsDa72HzL/o6qYn3HmGLxLq2OuoyHiHZNCcyoSWaa2vzz7qsqBZawRq/rHzbw6D5ruoyDVoz4P8+W+zpu+E2ebOXp7bUpf08cv4I/9u9/ESAasIJSV3GlFY8In3VeJh5h3YkX42Esa1g83z/C956v7LnBLNBSovgm3Pg2wv0Guun6vaMHrqsz0IStkgqM7r/txf99XApiyWjI1/HskgSKSThp/ZEx0kiLJIECEnw+iV5WpTw7fYa27APQc26QVYvXQ9aJWHXjhWB7eth4gnw6QH+9zqrj3dMOEbi3VM4RmJMbPfuyk91mZyuy2gWiRWwNgdHbpfk0IEB66b1QGjl1aYNureMgRWeK63TCb71tMlLKW6+qy7tdyH3axWy1ObR21+NOCGpDKZUx1gkIthY5r0MfHqICsPM22NnhHx/meacv9sVlozRJ7uFL/gWSfagsucEn7a3LvJHSa/8RH9AaW2h1QDdH6uTtKyeAF+fUzenJw2WSMHE75Ip2Q6LX/VfB8cgAKz3khEye+qyTlgkCQ6wRlgMCXZtFedpzaySPK0XZzvoZp0hs7euB4PWsYRk0imw6jN9Uk9tDlm7Qqu9vfeI4dqyFknxFv2Mx/WEr86M3mY7JqTdYF1GE5ItnpC0DQlJsy5lj13pWQ6tB0KrvXR944/+fvtZpLdXC8cKyWbPIrEiuX2dWuOb5qgFVgOVoZ2QlEfut1rawPLTg/DZwTpZUlVZPcEfYxCkeJvGMPJXqH+1aDPMuR8mnlTWX7r1N/8HVZLv+2EXj4GVH+t6q30hJTPyvI0BIdkSaMOSMbps0c/z01K+RWKMBqMXv66DqOoaNsCe2iLydUVMv0FdDZZwoHajFxDteLy3P1aMJBRsL86HbXFOsLXgP7Dgmdj7a9y1lWCLJBwj2VHNNtdPdmjaGZp20vXgZx5NSEpL/NHiJ86A09fD0J8hrY13TCzXVmf/fTfO1oeBpWP9h4kVH8N7PbVvsFZR+yN1WZ6QtB6k5d4tGd399aS0yM+g9b7Qor+uB93QO6yRE7XEfdOQRZLpWWuFG/x7bz0IktPKtquacUISi9xv4IsjNT88b5l+MeeP0n0rq+hzXDNZO+CJJ2lnPPdhLaNRtBU+2AM+GeSn/rXYQ388uZNhysWRZvaqz3XZ6UQY9DQc971aE1sX6A8ga1dof5QvCsne5EcbZvrXCYrZCk98WvQvKyTLP4Rp10d2Iuum+uulcVQcLc5P3DiEMKVF2iFJst/5xBMnWfKWzjKX1ASO8H64QYukYK1+H1IyoO2h3rYoQlK0OTRIbANMv06fcDcG/N5L3op8WLHnfn8ZfH+5H7sKU5NCUlrsWa0x3rs6CNfa2h4Qkh2urS7RH3CiCUnhBsCoNZKxCyQlgwikZHnvEbZIvHtqvrsuty2OtCiX/89bvq/ZeEve9IPc7a1FEnpIKNqq352kJuqWsyIGkNnDX2+1j7+e0QPSsmMIief66nSSLq1FYisBp7eDJq30vq3ohC2hBOGEJBatB0Gbg/SL8OUwWPauH3dY+210f/uMW2DCCeVnRRmjx4FaE4te05nuplyky22LNHi28CU9puNxcOQH2nEtehV+CUylaYWk81DY9Y/Qeh/oerq/v9/N+gOyP762h+qXuWij/6W3T0zgfyFb9Pe/pPYHO/UKmPeYzklh731hII4QT0f26UHwv37luwbXTILZf6t6teKgNWItkoqExBj40atxtPeD0Ol4tfIK1/sdqbXmWg5QVwtEFxI7hsSKmH1KNMX+uIKtC+GrEZodFixxERy09v3lsOnnste3nbntGBPp2gq7soq3epOGVWNqeIRra4v/Xdy+NmSRlCMkNtgcFPG07Mj3SfU+r8INMPcR/2k+OI6kSWuvJH3gQWmZN0u4ve6671TgklJVCJLT9fsVtHSs0GT11t9hWlt/X9AiyQ7EMVt7sZGMbpDcTO9z+zp9MFrrZYB1OFqXzbpG3luT1tDEu18rfOHYTIJwQhKL5CZw2FtqLm74Ab4KTORYtMnPlPjxXvj6XJj7KPz0gLqUlr2jTyPBDqC0SF0mk0+Dtd/423+4XpemFBb8n799mTeBUOt91a974Ave8TeoRVNa4heA63CMf94uI3XZtDN0P89b90Sh1T7Q0vO92jhJNPdahGvL6ySt8Kz9Vl1tJdth8Rv+ORXFUkqLtBPOW1K+u2z69TDrL5E/4p3BxkdSW/jBxopcW+um6BNgejvo84dI94G9P5vn32pvX2yj3btN/W25p1pFxVv9Kq/WIrFPm0WbYO6DgXMDgeSSPH2ACGOFJKObLhNpkYSFZNNP8FYbyKlgTFJV3sNSkAv5gRhJeRaJDcQXbfY/jyatI69n3bxLxsAPN+pDXbAkf3JTv5MPDiRc/blnKXnXXfutLjN66KyMTb2YR95yfYj4eBD85E0ol+UFwYMWiU1msbEbixUSSfLHimyao98nU6zi0cR7MOoyLDLW0qS1/32wv9c2B1ETOCEpj7RsGPyx1/kaQHx3xtpvtCP/8U5Y/Jp28JbfXoDPj9CRzz95HcTiMfDzo2rZgD9n9A5fvDdS1U7vaZ/IW3mBsm4joM+V6kL64kiYcIw+HWXs4vtGQf21B78Ogz9SMQQ/c6vjsX6w0aYa7rBIxL9G0LWVv7psbGb239R6CrpuKsruCqbihoPXQezTeDR/c3E+fLwfTLu2/PcC3/po0tL/4VVkkdg4T48L/c9uh0/ea3OEkITENoj9IWfsom0AP6C7yQugBvP/5z3hv4f1vdtU1WgZWdYV06xb7GOqi3A8Ifer8ku67ww7hEQit4djJOUJiU1+iMcisZ/15p8968fo552UApnddZ8tUSJJer+rvigr2DbAbTv0vKXqrlw/zY852mOCQpLlJQ006+YH+CEyOSbo3tph3fTx96c0g70Cs582aQV7P+C5t7zPwz4IJRgnJBWR1VtjDwOfhENHQzcvg2P1lzDtGl23X+COnitk1Wf+l3DGzTrVrbU2el8OBzwPh73Njh9NWlsYcI+uD7jPf+/kZpFfnH0fhd2u1yeo1RN0W/fz1fcbpPtZ+iRs6XcznJYLHYZAuyO89o/XH1z+Sg322SehtLaQ3jbyB2s7yqadYZez9YdnZ13c6z7/uPIIik4sISna7FsS0Trndd9rCYmgJRTz/YIWSRxCUrTFv26vi/3t9kduA+5WSFrurU+Tyen6NBtOJ7VC2LQLpLaK3Ldxtv4PbecgKZossWSsvrYJFK0H+fdSUgjL3vMzkuwTdEuvs1nxUeKskrC1YEfsRxP7yvLDTTBxqH9faSELomCN/9lHs0hKS3y3oLUkigMWSVhIrCvQsm2hL5QpGZHXsQ9z1j205ZeyqeA7BMFzM+Uti0xggYBF4rm2JAXaHAh73An7PuI/rID/4Ai+kGyMISQA3c/W33RqS03/zR4ER0+C7AO1r6ghnJDEQ3IT6HsVdDvDtyQWv6YmfmYvOGkOnDgbjvif5ndbev4OEJh5qz7FpWTCPg9Br4vUfWJ9o11Pg/63w/BV0P8WP8Wx1d7qWw22Y99H4IQf4KBXNAvFClB5iEC69zTU/gh1taz73s9Rz+rjC481p4O+aFunqGkn2Ot+/0m5y3DocYF/XHkEYwAx02UDtamiuYusOG9fW3EMJWiRxCMka7/Tzrn1ftC8r7893QrJSu3MN88FBFru4X2uNpa0Sq23ySO0OoC1SJp18Z8Qg23LX+6PMbBZP9Zdai2SoJAsfh0mnaruU/CFpNNJ2pFsz1W3YHUQngQtbJHYB4HCDTsXeN+2RO/RlGoGpA0MJ6X5/yvL5p/VCm/SSp/Ag0JijPddMl58wBOhos2+hRZ2baWGhKS0yLfKk5vpMhi/AMg+wH/PCi2SZf4gREvzkGsrtblaOQPuVi9BVi+995Z7RQqpFZLNP8UWEkmCoz6D4St8y7flHnDct7DrldQUTkgqS8sB/hciozsc9LI+lbbsr51+z9/rvnZHqOWx19/8c7ufF/lF3u16/WLsepV2Srbz7nCULoPZHEFa7QU9ztUOL2yNVERqc+2gTLHvymne17dI7DIlS/3FJXn+E1azTmr27/OQ/rgG/hPS2um+7bl+516cV7aQYTwWSbCkSMEqzWYb18ffbsdvmJKKx2VEs0jKi5HYQZrhcTc7XFvewDhTok/GKV6nE0xKWPBvWPoW/PyI/7Se0bWskIBaJbZz6DxMl3ZcghUS25aijX4qsRWbHcH2TNj/P/odXPhS9JhXZVj+IbyVDfOe9LeVN27Eup3yV3nTD8QRgP/8cI0hbPk1MhssNcvvzC32e2PTclMyve+lV77HxtJa7R0ZC9sew7UVToUH/38ftkjsNvtwlbesbLaXfeiz7rDNP/v/gy7D9cHEPhCkexZJWCzT28FxU+CI9yO3W2sz+F0JCwlowD+ladntNYgTksqSlAJDJsJRn8PJC6BtKJjV9VQ46gs4/D3t5Pvdqu6g5GbQ95rIY3cZCSf/ok8QQXb/M3QdAX3/lJh76DBEl9aV06I/9LoUDngW9rhDt4n4T382PdX+mPteA8d9p51kchPtKE2JPq2t+gLe7QKfHR75noU7YZEsek0Dz/Oe0G22KjFEjvMIU7zNr4XVrEt8MRI78CvoEgTftVWw0h/kaeMSEBmMX/WZrm/5JdK1FRQSO6J5fY5aLZKs6dugIlGc52cDtfQGhRZu9D8/24YdQpIBzftoWQ2IPW3ruhz93xRUkN1lA8w2QwjKH2xq73Pm7Tpwb+lb5V+/pFD/z0WbtPZbECsS0bAZcsHvZcFqv5ZVm4N8IamMRQL+/94+HFhRAI1x2fcLTkplsa4ta7Ws/lzbldwUDhsLx3/vXzdokYRptZf+noI066bvvT0Xcr2SPMHAfB3CCcnO0LK/dsZBt1OQDkf5nZcIHPwqjNgALXaP7/pZveCwN7WDSATtPYvHlKhvt88fVBB6XRzZ6dkv7WovzTjoyw1if2gLX9S5GQo36A88WLp7exwWSV5ASApW+U/hC1/Q84M/5GARvzDfXaTBzsyeKsY2JbK8c+xTqe28LU0Dri2b/p0REBJrkWz5xc/k2bIgkGkUEhLrGl32HmA0eyezp3ai23P9JIiMHoHBcxt9l4r9jGzg3j5FW/97tDTgDTO1TMj4o+GdDhqzi2U52Iyx4CBMa5GEO2Xwx3jYJ+bV46Nf1xJ8oFgeegJPyfQ73TDB7KSgkNjPvM2BkUIS0yKJIiQ27hXNtdUsKCTWMu+qx6Y292vZtdhDr20fcLJ29RNnwvdgr1cRItDlFF0vKdDr2XhsHcMJSU0g4mcB1QXaHKw/sJQsHaMSK7PDlkmxP6CKhGT2veoyszGUYAHICNdWHNVyt/ziB94LN8CsO/3y9uD/YIvztFO2lU63LVEXS3JTOPJDdSdYd8DmwJiZFR/BD3/W80qLfZdRi5B1GHRtlWeRLHrFb19pof7wU1tCamakkHQaqks770RWH/1+NN9NX9vONau37/Mu3Og/YW9f51U/sOmq1sXmCUlBFLG043KaddPlzFs1yB0N61YLVsu17pxo3xNrkdgYWe43ZY8JEvweBAtfgnaUYdeWxVrD4H/f8pb7rq3sA3whKQ5aJDGytoJYMbKxsdQsX4AydlHXE/jFIJt1gaM+hcGfqIcC9KGyzYH+Ne3/M0ibgzVpZ98o1aRj0eU0f73ZLnWrHwnghKQxktIUjp8OQ+eWdeUECT+dN+sc/bhw6e0e5+symB4a4dqKYZEEXVvh6qvBsu3gC8mcv2sQ2mbFLXtPl51O9DsGu9zyix/Hybka5j6kbdwyXzPRMnbxLUmLFZJtS/32BS3nJDKNAAAgAElEQVQS6/+2QhTEPoEGhWSXM/0UbPBFzvrhrZBk9tJO1WZ0Ba24bUsiXVvgWy9hq2vTT+puSmoCx36jY6NAqzSErRJjAkKywt9vM9LS4xCSTT+W/d+ZUvhiiA7sDQargw8GoO8d0yKJIiSrPtfPJrOXCmlUiyTGOBKITKoAdSdbrFUSFBJLk2y1LIPCAb61Ge3aoA8Mfa8q/zcXpv1gP6YSLT5SR3BC0ljJ6BZbGCxl3DwVWCR2vbsnJGsCQrI9ZJEYo53zF0f7kwPtcG0FEgg6HOON3fE6Nfujsh2m9R3bdGg7kLPLqf41mrTQdpXka1wif7WfYrtlfiA+Erpf8MpOtFbryD65Bi2STifCboExRMFrhIVEvIFr+//HP8aKki3NYV1ErfbSjsdaJcHYx9bfvJI04leejebaKi2CnGsAo27LZp2h63DtTEvyysaMClb5LrPibYFUY88iieaSyVum1pe9limFtVMij1mXoy6v5e9HnyTMUpIXaZEEYwnRLJLl3kODHXQXLUYSdm0lpfifWXAgb2oLP3YIvls3q48eHwyQh8XJEhxFHs0i2RmSm2jlCtuWOooTEkdsmu8eOSFPLCFpGuhg2hwI2ftrsHjjLD9TKmiRlG7Xjnn+v2D1FxrTyF+pVkZSqh/AtG046guNdWT08FKq0WNNqZ/CvG6KPu2umaRt7nxS6F68J8TN83xBAH0K3hEfifKkKOJnz9m5IYIWiYhmsfW7VTP2up/r77NCYjueZl3VBZI9CAb8TTu5zifrPmuRgHYYtiqBLfsRzJyysaKUDD9rz7q2tueqq27NJP1cV3+hYtj/9kC7vE45XIwyPEGS3V9cgUUSTnzI/TrS2rHpvcG2B+nmVY3I7B1pkQQDy1FjJN77RhOSWCPbwY+TZO/vn9N5WGRxw73uUzeU/f8ERTTsLrO0OcCPi0SzSHaW/rdpqaTel1XfNauZhAqJiBwvIvNEZIGI3BJlf5qIjPb2TxGR7oF9A0TkWxGZIyI/iki6t32g93qBiDwhUtn8V0fcJDfxEwSS06OnsULkjyz7QHWdtd4PMH45mPBArvxV/ij/7bkw5VJdb9YtUrAye2g7Bj4Op/ym1wftRDb/4ndyecu0Wq4p0XEZ4bZmBYUk4Mff+qsvJC1iuByCriiIFBLQznzv++HA53zLAvxBarZ8RvApdY/bdZCoHWMQPG/gk36nZi2SIJsCQmLZ4dpaC1P/oJUVFr2q/7fDx0Van8G4T5DwzIN2v3VtBWMk9gk9f1nZMURz7oOxLf3Co0EhCbsAU1tq+Z89/qrxuqBFEuyMgxZJ8MElJRN2Ocu7lk3/3aDfC0kum2oLfpwks5f/P7FiZsnsoW4oG5MIfsfDVs6O6zaHHr9TyyQca6sKLfpphY1WUSzmOkLChEREkoGngROAfsDZItIvdNjFwAZjTG/gMeAB79wU4BXgCmNMf+BIwA5MGAVcCvTx/o5P1D048F01TTvFHrMS/JG18dIgrb/YujnsuA/7VLvmS825T24GiN/ZZOwS+eQbzlJJD4xbsR2VZbY3ZifcKYDfYYSFZMsC/zqx5m0ICklKpm8lRKN5lKfo7P3gkNGw39ORxwY/z6zemj23x1+g03H+9qhC4nXGEUISCLbb1N2up8Pgz/z/icUKSV4FQmLdUDtcW4H/S8s9tKMuWOMPvmw9SJ/2TbFaBYvH6AND8P+0OSQkWZ4VMuAu/eyC4yGs+CelRXbewe/brlf7Fl9yuoqDjb00aRX9O9vtTG1rq320avagp3z3USwihCSGawv0YeLYr+tsUDxRJNIi2R9YYIz5zRhTCLwBnBI65hTAlpAdCwzxLIxjgVnGmJkAxph1xpgSEekINDfGfGeMMcBLwKk4EkdQSGJhf2SS5Aefbedrn/atRWJdOPO94HjX02HAvdrRN99NS8gEn3yD5bYhkJ20xs98sgHUkjx9At3lnLJttE+3G2dFlm3fNEefvFNbRLrUggSFJKNb+YNAM3r4Y0WskIhokD18L0FEYL9/la1UEE20NpdnkeT6JUwO+A+0O7Ts+TssklC8wqb+hsvCFEexSNI7+texA0Vb9Idhv/oxoHXfaXZcEOs+sxZj+DO3FokkBcqPdI78zINVb3e7LvJ8WwIIYlsOe/8djp+qopU9SEeAV+TYCAbcY7m2GjGJFJLOQLBA/zJvW9RjjDHFwCYgG9gVMCLyiYhMF5E/B44PFviJdk0AROQyEckRkZzc3HLGDzjKxwYgo03Za2neV3+0HY7z3QZWgDbO0liGtUiskNhy7F1PVTfP0Ln6t8uZkU++GaHOd8dI+jX+k26PC/39PS/SlNtobQTN0irdru1IbalPz6Aj+sN5/zvO3c2ffKhZt+jHWJKb+NZPLGGqDEGLZEcJEC/ulJwR2id+2ZLUFrFdkU0riJHYztgKjR2QmNrST+1Ob+cLpRX09PYq9DbRYd1UnRQKfIvJVjzofbkKUbeRkW2wMZLUlr5ghL8DmT1gv//T9Fv7YGFpFxgIW50dfrwWSSOlrgbbU4BDgXO95XARGVL+KZEYY54xxgwyxgxq27ZtxSc4otN6oNYAKy/3PbU5DFsEh7/tb2veVwPnW3/1UklL9bhgR5zZS4OIYeyPNq1NWVFIywbEm1/e68D6/MHf3+eP0dsYrp/UbWRkR28tqWgkpfiB+HB8JBoHvwKHvlk9g8eCQhKO1QQtkqTkyA6uPOunWZQYSf5Kv0bUDiHx9u8oN9Pct/7S2/lZRDbN2z61p7fRfSX53mRMAr1+H9mGDkNg+HJ9kAhiLZImrXVSpoH/1GKlYfpcrnWqwkQISTV2+PEE2xsxiRSS5UBwzH8Xb1vUY7y4SAtgHWppTDLGrDXG5AEfAvt6xwcnO452TUd107R97Kd1S2qmn1YJKiLNPevDDjxr0lqLPO5ylnYOJ0yP7Ax3vJ/nWonWESel+B1mSb4e07I/7P9vLfESqxpAUopm6YBWQ97jjkghiTa3fRCbuRUWpKjH7q1l/6uDoFXRtFOoDlRozEXExEnlCEm0GMn0GzWNt/MwXzTzlusDwI4R/V39/1d6O78umx3TEk66sLQ9RKslR9xXjM54RzmR1vqd63tN5YLMGd19SyaWa2tniCfY3ohJpJBMBfqISA8RaQKcBYwLHTMOsH6JEcB4L/bxCbCniDTzBOYI4CdjzEpgs4gc6MVSLgDeS+A9OKqCdW+tnqjLJq1VlA55XX3b0WoOgWZd9bxIA8/RSAv4q61bq/dlkeXfo3HYO3DcVNj7H165icA8LuVZJKApmH2u1JpkNUlqyLU16Cn/dXiemKCQlGeRhGMkayZpNevkdM2OiyhUuULHrKS3VxHZMQAyICSWYGcbrEHXdURkphXEdg/tsEhiuOUqQsS3ShJmkTjXVpiUig/ZOYwxxSJyFSoKycDzxpg5InIPkGOMGQc8B7wsIguA9ajYYIzZICKPomJkgA+NMTaH8I/AC0BT4CPvz1EXaTUAFuEPFozXt5ycDgc+H3t/0DrqeVH87WnWyXfrgG+RpGVXbGlkdof9nir/mEQQdG2lZev4mCM/1IGGdkzNjv3BGfi6x75msH6YKYV5/9TXu9+kAmQr8uav9DO5rHVo02mbdlDLS5L8agGxLJKup5WtIBzLIml3uI7m7nVJ7PZXRO/LNCuwokysymDddklNolvRjZyECQmAMeZD1C0V3HZnYL0AiJKrCcaYV9AU4PD2HKAak7QdCcNO62uzgarrSS44FiFcMbUyZB+gGVYdj698Of6aImyRgM55M2x+2WPT47RIktNVlLav03To5e+rIPS+ouz+3K91m3WV7XmXVjnOPlDjMs138/8fQSFpOUBdmE076f8oWNkgKTV2Z5zeFoZUUPixItodDqcurfi4ytCsq1qwGd3r7nelFkmokDQEioshxX1KO0e45Eh1CcnAJ2D6dTqdcFVosbuW8Q+6yuoaYYukPKLNCR6Lpp1UKH5+RDOpOh4faa212EOf6pd6CRTWIul0vP5ZWg1UIZGkyPdPSlYX5o77aKUCUlqk1kh964yTm2hWoU3tdkRQV7O2ap0lS+CMM+CCC2q7JfWYpu11wJglnKq5s/S9Cs7cAh2Orvq1MntGTxeuK1RKSIIWSffyj7VxkF+f02XYTWZn77SzUsbKQNsxRXOb2NMqgAqHFez6mj6blFpx0kkjxX0qMRCB99+H11+HH36o+HhHDAY9Acd8DX2vrZrfO0wwQ6whE821FQsrJOntKvbj2zgJ6LidLqGxwuHkg1hCYjPhmnaJvj+IdX25YHWDwwlJDLp2hSu9KY9vu61221LvaXswDHwsvjEYjkgqY5HYgHBGHONX2noj3jsPg2MmlxXm7NAA1FhC0uZA2OdhzfaqCDsy3qXPNjickJTDrbdCVhZ8/DF8+WVtt8bRKEluGigVX0EH3O5wHTG+518rvm6vi+GMTXDEe9FH4Gf08C2HpCaxS+SIwO43QLvDKn5PZ5E0WJyQlEObNnDjjbp+662xZyd1OBKGiI7u3vvB2ONuLMlpsP//RQbDy6O864n47q2MXcqPf8SLE5IGixOSCrjuOmjbFr79VmMmDkeN0/sy6BdjatxEYkf7V9c84V1PU3Hqenr1XM9RZ3BCUgFZWXDHHbp+662wfXv5xzscDYaup2mmUscTqud62ftp1d3gqHdHg8AJSRxcfjn06gU//QR33QXLl8OqVbXdKocjwbQeCGfmw25/qu2WOOo4TkjiIC0NXnwRkpLgH/+ALl2gd2/49NPabpnDkWCqIzbiaPA4IYmTQw5R1xZAaips2wYnnQQ33QSLF9du2xwOh6M2cUJSCe69F2bPhk2b4IYbtHzKww/DvvvCmjW13TqHw+GoHZyQVAIR6N8fmjZVAfnuOxg0CNavhxdeKHu8MTBmDCxcWONNdTgcjhrDCUkVOOAADb4D/PvfUOpV0/71V7VWXn8dRo6EE07Q1w6Hw9EQcUJSRY4/Hrp1g99+gwcf1EKPvXtr/OT++/WYefM0WO9wOBwNESckVSQ5GS67TNdvvRXGjtX1Tz+FOXOgSRN9ff310KIFnHIKbNnin79hg1owBQU1226Hw+GoLtxMG9XAtddCXp5aHi1bwuDBWn6+tBQeeABeesmvIDxuHOy3H+y1l26b781PtO++kJNT/6ZpcDgcDick1UBGBtx3X9ntkyfrYMYzz4Svv9bxJxdeqIIzb54e06wZFBbC9Onw88+w++4123aHw+GoKk5IEsS55+ofaJbXGd6EwtOmwRdfqAXTvTvsvz9cfLFaLf/7nxMSh8NR/3BCUsNkZcGpp0ZuGzpUheT999W6adHCFyGHw+Go6zghqQMce6zOCz95sv4lJcGQIdDBmwdo3jzo00e3g9b52rDBWS8Oh6Nu4LK26gAtWsDhh/uvS0vhjTd0/cUXYbfdNOsL4NVXNb147739OIvD4XDUJk5I6ghXXw277AIXXaSvX3lFR8Y//LC+fuIJzQQ77zyt81VY6I9N2bJFA/pWfCx2gKTD4XAkEickdYRTT4VFi+Bf/1ILZdo0eOYZre0FKiovv6zjVv7wB9328ssqFm++qX9/+hMUFem4lJNO0uvY8x0OhyNROCGpY6Snw4gRun7FFbq87jqNkaSlwVtvwVNPQY8esGwZTJyoc8qDFo589FEdo/Lhh7B1q5vV0eFwJB4nJHWQe+6Bgw/W9dRUjY9Mm6YTap1yigbdzz9f948aBZ995p97yy3q+urWTV9PmRL9Pb76KvY+h8PhqAxijKntNiScQYMGmZycnNpuRqUwBr78Ui2UAw8su3/JErVSCgv1dYcOapGUlkKnTvDeezqCvn17WLlSxeXgg6FnT3j+eT0mLQ3WrlWxcjgcjjAiMs0YM6ii45xFUkcRgSOPjC4ioBbHlVf6r0eMgOHD9bxRo2DgQGjdGlavVtEZNQp+/FEFZtQonXt+82ZYsCD69adP1xRjh8PhqIiEComIHC8i80RkgYjcEmV/moiM9vZPEZHu3vbuIpIvIjO8v/8LnDPRu6bd1y6R91CXuf12DaiDViF+8UVNCR42TAXlgAN03/jxfvYXRJZzmTOn7HU/+ECFqFs3fY9GYLQ6HI4qkDAhEZFk4GngBKAfcLaI9AsddjGwwRjTG3gMeCCw71djzN7e3xWh884N7Gu0cxNmZ2u14bvuUiHJyFB3l8UKyS23qNsrI0Nf5+f7x0TL6powQZdbt2op/JkzE9J8h8PRQEikRbI/sMAY85sxphB4AzgldMwpgJ2pYywwRMTVv60MRx8Nf/2rpgWHsW6xNWs0QP/SS35Ze0s0IbGVirOydDljRvW11+FwNDwSKSSdgaWB18u8bVGPMcYUA5uAbG9fDxH5QUS+FJHDQuf913Nr/SWW8IjIZSKSIyI5ubm5Vb6Z+sghh0CvXmqZfPcdnHaaCg/AQQfp0grJ2rWaLbZkiS8k552nS2eROByO8qirtbZWAt2MMetEZCDwroj0N8ZsRt1ay0UkC3gLOB94KXwBY8wzwDOgWVs12PY6Q2amzncSlNq//EUzuP75T50DZcECHRk/fLimBH/yiQbZs7N1UOOoUU5IHA5H+STSIlkOdA287uJti3qMiKQALYB1xpjtxph1AMaYacCvwK7e6+XecgvwGupCc8QgbK8deKAOYtxrL42nlJSolfLVV7r/m290uc8+egyokLiAu8PhiEUihWQq0EdEeohIE+AsYFzomHHAhd76CGC8McaISFsvWI+I9AT6AL+JSIqItPG2pwJDAVcEZCfp31+X33+vsZNevfx9e+8NnTtrCvH69ToY0uFwOKKRMCHxYh5XAZ8Ac4Exxpg5InKPiAzzDnsOyBaRBcD1gE0RPhyYJSIz0CD8FcaY9UAa8ImIzAJmoBbNfxJ1Dw2dgQN1mZSkLi1bYRjUIhGJtEocDocjGgmNkRhjPgQ+DG27M7BeAJwR5by30PhHePs2YGD1t7RxcvXV0LGjTqzVti3ssYcWfiwuVosEVEgmTFAhOemk2m2vw+Gom7iR7Y2YrCwtW9+2rb5u0wYefxxuvtmfNMtaJBMn1koTHQ5HPaCuZm05aolg2RVQKyQzUwtDTpoUOQGXw+FwgLNIHBXQti3ceKOu//nPLnvL4XCUxQmJo0JuuAHatdOy8/feW9utcTgcdQ0nJI4KycyEZ5/V7K6//lXnjXc4HA5LXEIiIn8SkeaiPCci00Xk2EQ3zlF3OPlkHQ0P6uIqKtL1V1/VVOF33ol+3ooVWsre4XA0XOK1SH7vlSc5FmiFliX5R8Ja5aiTXHmlZnOtWAHvvquj4m++WYs6nnYa/CP0jcjP1zIsBx3kYisOR0MmXiGxhTZOBF42xswJbHM0EkTgqqt0/amndJ6T5cuhZUvd9sADkYLx5ZdqjSxcCKtW1Xx7HQ5HzRCvkEwTkU9RIfnEK5hYmrhmOeoq55+v408mTYLrrtNt116rwfiNG2HxYv/Yjz/212PNxOhwOOo/8QrJxWj5kv2MMXlAKnBRwlrlqLNkZcGtt+q6nV3xggv8kfAzZuiYkylTtOyKxQmJw9FwiVdIDgLmGWM2ish5wB3o3CGORsitt2p5+dRUnSu+Rw9fSN57T2drPPRQ+Pln/5z582unrQ6HI/HEKySjgDwR2Qu4AS3rXmYOEEfj4YordN4Smwq8zz66fOklKC3Vel0A6em6dBaJw9FwiVdIio0xBp0a9yljzNNAVuKa5agPZGT4U/dai6TUi5wNGKBLW3LFCYnD0XCJt9bWFhG5FU37PUxEktA4icMB6CRZzZpBXh6kpelEWXl5Opf8I4+okOTna1ZXs2a13VqHw1GdxGuRjAS2o+NJVqGzHT6UsFY56h3Jyb4VcuyxGpRv316n7G3ZUqfz7dwZdt0VcnNrt60Oh6N6iUtIPPF4FWghIkOBAmOMi5E4IjjqKF1ecIG/TQR699b1DRt03Mmf/lTzbXM4HIkj3hIpZwLfo5NQnQlMEZERiWyYo/5x5506AdaI0DcjM9NfT02F11+Hjz6q2bY5HI7EEa9r63Z0DMmFxpgLgP2BvySuWY76SFqa794KctBBuuzVC+66S9efe67GmuVwOBJMvEKSZIxZE3i9rhLnOho5N94IDz2kgxTPOku3jR+vtbosxcXw6KORY08cDkf9IN6srY9F5BPgde/1SEJzsTscsWjd2p8cKztbBzAuXKi1uCZOhHPOgcmTdd6TL7/UQY3lsWmTziM/bJiWtnc4HLVLXEJijLlJRE4HDvE2PWOMiVE43OEonyFDdH6TM86A9etVTJo21X1z51Z8/u23w9NPw+jRcOaZCW2qw+GIg7jnbDfGvAW8lcC2OBoJRx+tQrJ+vb6ePNm3LBYuVDdXSjnfzEmTdGlrfTkcjtqlXMeAiGwRkc1R/raIyOaaaqSjYWHThAFatNClHRFfXBxZQThMXh789JOuL12amPY5HI7KUa6QGGOyjDHNo/xlGWOa11QjHQ2Ltm3h7LM1w+uDD/zt4s1wU16Bx5kz/SD9smWJa6PD4YgfF6p01AqvvaaicMghcPrp0LGjzrII5QvJtGn+urNIHI66gRMSR63z5ps64v3AA/V1eQUec3L89aVL3RS+DkddwAmJo9YRiSylEq9Fsm2bpgI7HI7axQmJo87Qp48uYwnJxo0aaE9Ohu7ddZtzbzkctU9ChUREjheReSKyQERuibI/TURGe/uniEh3b3t3EckXkRne3/8FzhkoIj965zwhYkO0jvpOr15qmSxcCL/8AoWF/r7SUvj973V50EG+6DghcThqn4QJiYgkA08DJwD9gLNFpF/osIuBDcaY3sBjwAOBfb8aY/b2/q4IbB8FXAr08f6OT9Q9OGqW9HTo0kWzsvr21eC7MRoXGTEC3nlH04Wfew66dtVzognJhAnw8ce6PmtW5Nzx8fLcczrK3uFwVEwiLZL9gQXGmN+MMYXAG+gMi0FOAV701scCQ8qzMESkI9DcGPOdN2PjS8Cp1d90R20xbJguk5M1Nfj3v4f991cRadJEs7123dUXknAK8PTpOh/K0KGwciUcd5zOIX/zzf5YlYr46Se45BJ9b4fDUTGJFJLOQPB5cZm3LeoxxphiYBOQ7e3rISI/iMiXInJY4Phg1xHtmo56zFNPQUGBLgFeeEGtkiuv1NjJiSfq9mgWyfbtOhdKcbFaNfffD6tW6b4HH4S//S2+Nvzwgy4XL44sLOlwOKJTV4PtK4Fuxph9gOuB10SkUgMgReQyEckRkZxcNyVfvSItDS69VC0RgL/8RYWlWzf/mC5ddBkUkmef1bIpttzKqFG63HdfXT75pIpNRcycqcuSEli9eufvw+FoLCRSSJYDXQOvu3jboh4jIilAC2CdMWa7MWYdgDFmGvArsKt3fJcKrol33jPGmEHGmEFt27athttx1CTJyVpqfupUuPvusvutRZKTA1dfDWvXwjff6LbrrtOltSbuvhv22kuPeftt/xorVqibrGlTFSnrJps1yz/GjZ53OComkUIyFegjIj1EpAlwFjAudMw44EJvfQQw3hhjRKStF6xHRHqiQfXfjDErgc0icqAXS7kAqKDouKO+kpEBgwb5pVOC9OihJek3b1Zr5V//8i2JM8/U/aBxlcGD4QovXePxx7XacHGxnjN/vrrSli6Ft7ySpPY64ITE4YiHhAmJF/O4CvgEmAuMMcbMEZF7RMQLqfIckC0iC1AXlk0RPhyYJSIz0CD8FcYYr1YsfwSeBRagloqbtLUR0rQpzJsH992nr8eP10mxkpJgjz00wA5w5JEqSOeeq1P+fv+9CssJJ/izNF50kX+NNWv8uApEFxI3mt7hCGGMafB/AwcONI6GyfLlxmjXrn99++r2OXOMGTjQmM8/94994w1jTj7ZmFat/ON3392YxYt1vXlzYz7+OPJ6N93knz9pkjH9+xtz9NGx27NqlTElJYm5V4ejpgFyTBx9bF0NtjsccdGpE+yyi//azhnfr5/GT4YM8feNHAnjxmkKseXyyzU+0ru3usmef1632/L21iJ56y044ggN5n/+ucZbwkyaBB06wB13VO4eFizQybry8ip3nsNRV3BC4qj3HHywv77XXhUff/zxOj/8SSf5bi07R4qNkxx7rC6tkDzxRKRLK9pMjuO8COBTT8HWrfG3/957NVX52WfjP8fhqEs4IXHUeyorJKCZXf/7HzT3ksqt5VJSonW8rr1WXy9bpnGTyZM1cH+qN/w1mpB8/70ut2yBN96Iv/12oq5gQUqHoz7hhMRR7wkKiXVtVZYTTtC5US65BGbMgL331u3Ll8O776o1cvTRcMABuj0sJMXFkULw73/H977GaJIA6PtaHn9c56V3OOoDcc/Z7nDUVQYM0Npc6en++JLKkpUFX30Vua11a51X/pln9PXw4Tq7I5QVktmzNcbRrZtaJDk5Oh6lImFbudJ3g/30kw6YnD3bHwtz9NFaKr+gAA4/fOfuzeFINM4icdR7UlJ07Mf330cfc7Kz2NHz06ZpWvGwYbD77rotLCRTpujy0EPhjDN0fezYit9j3jx/vbhYReTxx/1tf/6zCsjgwb7l4nDUNZyQOBoEaWkaw6hOmjXz1y++GNq1g5499X2WLIkMqFshOeAArVQMOvNjRWNOgkIC8OGHMHq0L4jjxqmVUloK99xTtftxOBKFExKHIwaHeaVCTzhBR8GDWj92LpR774U774S77tLqxKBCcuSROur+55/9QDrAjz9q/a9gFWIrJO3a6fL++6GoSEvo29hPdraK1xtvaPqxw1HXcELicMTgzjt1bpP331cBsey2my4ffFDF5O67dfbGI4/UApGpqX52l83e+uQTnZP+j3/UcSgWKyTWiiko0JH4d96p1+7WTce2XHKJWjc2XhPmkkv0vMGDdYS+w1GTOCFxOGKQmanikJwcuT2YYnzTTXDVVZpK/PYnlMMAAB/aSURBVMUXKiKgJVkAHn4Y/v53OPlkf8Dhjz/651shOftstTrS01W4BgzQsS2LF2tsxs7TYkvcB3n/fS33kpendcT+8peyx2zYEF/lY4djZ3BZWw5HJbnqKh35Pnx47CyxwYN1sON//wu33abbdt9dg/Q2aL59OyxapEK1//5q/TRvrrXCwljxmjVLLRMbQ9m6VedqAbjmGh04+euvkef++isMHAj77QeffValW3c4ouIsEoejkrRqpZ12RanGTz4J/fvr+t//Dv/8p65bIfn0U42X7LqrWiMHHxxdRADat9fU402bVBgef1wD/u++q5WL99kHHnpILaLVqyPLrdx8s543fjxs21a1e3c4ouEsEocjQWRkwHff6ej43Xbzy61YIXngAV1eemnF1xJRq+Tzz+Gyy9R6+fpraNNG9591lorRLrto7a5Fi7Te2PjxftmX0lJ1jR16aLXepsPhLBKHI5FkZvrB+c6dVVzWroX33lMhaNUqPiEBf3DjhAm6/OwzLd0CfoaXnYdl4UK49Va/Zlhmpi6nTq3a/axa5ZeCcTgsTkgcjhpCxBeVq6/W5VVX+Z18RYRHyW/apOnAqakaAwFfSF55Bf7xD42nXHGFutYgfiG5914455zIVGWACy7Q7DM3ONIRxAmJw1GDWCFZulRdUTZQHg+xClLuu69O9AW+kLz7ri4vu0zHrlh31uTJGk85/fTY72OMxltefz1yHExxsZ5vTGTmmcPhhMThqEGskIBOCdy+ffzn7r67n4ps04shsmilFZKCAl1aAenfX1OLly3T4pBvvx0pEkE2bdJ6YRA5AHLuXP+6ixbF325Hw8cJicNRgwSFxLq34iUtDX73OzjoILUYkrxfbzQhsRx0kC5TU/2KxpbgBF9Bli7114NCMn26v754caWa7mjgOCFxOGqQAw5QQRg8WMeOVJZnn4VvvoGOHXX0fNu2OmjSEhSS9u0jX594oi4vuECXr70WvRZYLCEJlsl3QuII4oTE4ahBunbV9Fw7m2JVGD1aO32bAgy6boP3Bx8cWQ35ttvUtfX88zpF8cKFGpBftSryukuW+OuxhMS5thxBnJA4HDVMly7xZ2qVR0qKWjdBRHwrJOjyAo2vdO6sy/PP12233aYZXyUl/nFBi2TBAh2BX1ISOfHW4sWa0WXPW74cXnqpbJaXo3HghMThaGAcfbQG1ocOjX3MXXdpfa62bWHFisgsrKBFUlKi9cCmTfMn7srM1GD8ZZdpSZdff4Xrr4cLL/QHPzoaF05IHI4GxkMP6TzzwcB+mPR0+P3v4fjj9XVwdkhrkbRsqcvbbvPjMEccoXPag18o8sMP4dtvddt331XXXTjqE05IHI4GRnKyTh0cD3bOlcmTNe13/XpfSI45RpcffAD5+VqG5bHHtAxLkI8+8s+JVp24PlHRRGSO6DghcTgaMXacycSJOnK+Xz9fFEaO1GWPHioWr7+uk2yFheTjj/316dPrb2f8979rEkIwRuSIDyckDkcjZrfdNNNrzRqNjaxerTM0ZmfrLI2zZmnmlnWBge/asgSFY9MmzQarj4wbpxlstpaZI36ckDgcjRiR6NWAu3XTfXvu6ZdfsViLJCsrsv6XHSBZX91bK1bo8pdfarcd9REnJA5HI+e003R5993QoYOulzfXyiGHqIj8/veRKcZ2wGNwBHx9wRh/PI0TksrjhMThaOScdx6sXKnzxF9+uW7r2zf28Z076xz1jz6qsy6CZoGdd56uB4Vk82YdX1JYmJi2Vxfr1/ttdEJSeRIqJCJyvIjME5EFInJLlP1pIjLa2z9FRLqH9ncTka0icmNg2yIR+VFEZohITiLb73A0BkR8S+T22+HFF+GWMr/WSJKS9O+II7SK8RFH+KISDLjff7+OL3nwwcS1vzpYudJfnz/fDaysLAkTEhFJBp4GTgD6AWeLSL/QYRcDG4wxvYHHgAdC+x8FPopy+cHGmL2NMYOqudkOR6MmNVVrcbVuHd/xvXrB7Nma0dWjh85lv2aN3zF//bUuX389Me2tLoJCkpfnx0sc8ZFIi2R/YIEx5jdjTCHwBnBK6JhTgBe99bHAEBGtDiQipwILgTk4HI46S58+OtOjiM51AhpwLynx3Vw//RRZtysan3+u89rXRvpwUEjAubcqSyKFpDMQzMhe5m2LeowxphjYBGSLSCZwM3B3lOsa4FMRmSYil8V6cxG5TERyRCQnNze3CrfhcDjiZd99dTl9us6imJfn73vzzdjnbd0KI0bAtdeqoFQnv/0WWUssGmELxAlJ5airwfa7gMeMMVuj7DvUGLMv6jK7UkQOj3YBY8wzxphBxphBbdu2TWBTHQ6HJSgkOV4E007e9cYbOstiNF55RceggMZoqosHH1T322OPlX+ctUjatdOlE5LKkUghWQ4Ekwi7eNuiHiMiKUALYB1wAPCgiCwCrgVuE5GrAIwxy73lGuAd1IXmcDjqAEHXlhWSq67Sisfz5sF99+m2tWvhxhu1urAx8OST/jXefluzvarKwoVw8826/tJL5R9rheRw77F03ryqv39jIpFCMhXoIyI9RKQJcBYQnoVhHHChtz4CGG+Uw4wx3Y0x3YHHgfuNMU+JSIaIZAGISAZwLDA7gffgcDgqQd++OoBx8WL45BPdduCBamWIwD33aF2vG2+ERx7ROes/+0xjKB076riU/HwYO7Zq7di6FS66yH+dkVH+8VZITjhBl59/rlWNHfGRMCHxYh5XAZ8Ac4Exxpg5InKPiAzzDnsOjYksAK4HKkg6pD3wlYjMBL4HPjDGfFzBOQ6Ho4ZITvan9J0/X5cDB8JRR2lKcWmp1vB6+WXd9+mn8Mc/6vo118DFF+v6G2/sfBtWr9ZBk19+6W+rqH6WFZJDDtGstcJCuO66nW9DY0NMfa2wVgkGDRpkcnLckBOHoya44w7fhXXUUfDFF7peVKTVhqdM0dfNmvnB+Pbt1QLIz9c4RWoqrFu3cxOAXX01PPUU7Lqrusn23FO3b9+u1w1jjL5PXp7GafLy9NwtW7TuVnAq48aGiEyLZ5hFXQ22OxyOesqdd+q8JAsWqMVhSU3VeeKbN1f3V3C64TvvVPdTmzY6r31hoZavv+WWyhVRLCryrZnXX4f+/bWirzGxx4Zs2aLikZGhpV86dNDsMYAXXqjUrTdanJA4HI5qpUkTFYNevdTVFaRnTw3E//ADDBmiAnL22XDJJf4xtmbXZZfBAw/ovvIcJ8F9n36qgfx+/fzAv60bFsu9ZQWmY0d/jns7FfHbb6uV5CgfJyQOh6NG6dnTr+V1991qpTRp4u+3QmIzt377DaZOjX6t4mLYay8YNAhmzoTnn9ft553ni4IVkuAUwkHsoMkuXfxtffpoyZctW+D99yt3f40RJyQOh6NOsc8+/niO5s11GavEytKlOt/8tGka5H/7bd1+zjn+MdEskk2bNOj/5pvw8MO67ayzIq9tr/Hqqzt/L40FJyQOh6NOkZSkAwltRw8wenT00emrV/vrqak6UddDD0XO4titmy6DQvLqqzBmDJx5prrZ2rXT4pJBTj1Vl3Y++vIoKYF33mm8NbqckDgcjjrHhRdq0PyYY7QY5MqV8M03ZY9bs0aXJ52ksYy5c3WMSpBori2bSWb505+0FH74vJQUyM2FgoLy2/v00zqvS+/evoXTmHBC4nA46iwicPLJuh7u/MG3SNq1KxvYt4RdW6WlfibYNdfA0KE6MDJMcrLOvQKwbFn57Rw9Wpf5+XDTTRrXaUyk1HYDaouioiKWLVtGQUWPGo64SE9Pp0uXLqRGS9R3OKrA4MHwxBMwcWLZfVZIbD2vaISFZMYM2LBB3V+PP+4H5WOdu3ixWjO9e0c/ZtUqdX+lpWmA/quv1F3Ws2eFt9ZgaLRCsmzZMrKysujevTtS3jfJUSHGGNatW8eyZcvo0aNHbTfH0cA4/HDt7L/9VkfEP/+8Znp17Oi7tsoTknbtNCts3TodL2ItmyFDyhcRiB5fsXzzjaYvt2mjKcjHHAN77KFCMnMmnH565e+1vtJoXVsFBQVkZ2c7EakGRITs7Gxn3TkSQuvWmuJbWKixk4kTtVowxGeRJCXpSHWA77/3heSooyp+71hjUJYvh+HD9VrWrTV8uLYTVEgaE41WSAAnItWI+ywdiWTwYF3awYdWDIIxkvI47jhdvvYajB+vlsjRR1f8vtGExBg491y1hvr2VaFKS9NYjhWSGTOiX2/zZh1939Bo1ELicDjqB1ZIUjxn/OTJaqHEY5GAX9X32We1Iz/qqIrPAd+1Fcz4WrdOC0I2a6btmDpVX7dtq5ZP06Z6/IYNkddasEDLr0QL7Nd3nJDUEzK96nUrVqxgxIgRUY858sgjqag45eOPP05eYNq6E088kY0bN1ZfQx2OBHDssVqV9+mntfxJXp66qeKJkQAceqjW0rIWzdlnx/e+0SwSW15+111VPPbdV0vCgGZ67bGHrs+aFXmtjz/WrK4339TMsYaEE5J6RqdOnRhbhckawkLy4Ycf0rJly+pomsORMNLSdE6Tyy7zYxuffALr16trqXXris8fMkTXmzTRMR/xUJ6QxMriiuXeslWPN27U+VcaEo02ayuC1xLk3z8ndqW5W265ha5du3KlZ+feddddpKSkMGHCBDZs2EBRURF/+9vfOOWUUyLOW7RoEUOHDmX27Nnk5+dz0UUXMXPmTHbbbTfyA9Xl/vCHPzB16lTy8/MZMWIEd999N0888QQrVqxg8ODBtGnThgkTJtC9e3dycnJo06YNjz76KM97xYouueQSrr32WhYtWsQJJ5zAoYceyjfffEPnzp157733aNq0aQI+MIejYo46SsvE2yq/bdvGHkMSZNgwrTg8dCi0ahXfe7VurS6szZu1rEqLFuqiAi1KGQ07H0s44G6FBDSzy1ouDQFnkdQSI0eOZMyYMTtejxkzhgsvvJB33nmH6dOnM2HCBG644QbKmy9m1KhRNGvWjLlz53L33Xczbdq0Hfvuu+8+cnJymDVrFl9++SWzZs3immuuoVOnTkyYMIEJodrc06ZN47///S9Tpkzhu+++4z//+Q8//PADAPPnz+fKK69kzpw5tGzZkrfeequaPw2HI36OPFKFw3bo8cQ6AH73O7VqRo2K/71Eylol1iKJJSTRMrfWr/cn+gIVkoaEs0igXMshUeyzzz6sWbOGFStWkJubS6tWrejQoQPXXXcdkyZNIikpieXLl7N69Wo6dOgQ9RqTJk3immuuAWDAgAEMGDBgx74xY8bwzDPPUFxczMqVK/npp58i9of56quvGD58OBnenKSnnXYakydPZtiwYfTo0YO9vcesgQMHsmjRomr6FByOytOqlc5kOGmSvo5XSJKTNc5SWbp21Tncly5VK6IiIbE/s9mzNbCfmqrxHFDrKTe34QmJs0hqkTPOOIOxY8cyevRoRo4cyauvvkpubi7Tpk1jxowZtG/ffqfGZixcuJCHH36YL774glmzZnHSSSdVaYxHWlrajvXk5GSKi4t3+loOR3Vgy6ZAxam/VcUWgLQCUlGMpHlzHdVeWKgCBDrRF2jacIsWOlq+oul/QeeOr8zEXrWFE5JaZOTIkbzxxhuMHTuWM844g02bNtGuXTtSU1OZMGECixcvLvf8ww8/nNdeew2A2bNnM8tLE9m8eTMZGRm0aNGC1atX89FHH+04Jysriy1btpS51mGHHca7775LXl4e27Zt4513/r+9ew+SqjzzOP79OQjjqCSIloWgXHZZZhgJMIxcCklIRQnegL1oUJKoGy8xm10twxqEFDu71sZLNK64MRRbqwbKJO5WoiHRReRicKtQGAQRYZTrRlzFYRINCLgL++wf7zlMT9M93TM9fbonPJ+qrul++/TpZ97uOc+85/K8zzBp0qQu/G2d6zpXXtl6P98RSWfFxzwaG+HgwVASpWfP1jpcmaTv3opHTxMmwNix4X48D0o2R46EhHnVVWHelXLmiaSEamtrOXDgAP3796dfv37MmjWLxsZGRowYweLFi6murm739bfddhsHDx6kpqaG+fPnM2bMGABGjhzJ6NGjqa6u5rrrrmPixInHX3PLLbcwdepUPh+fmB+pq6vjhhtuYOzYsYwbN46bbrqJ0fEUc86VmWHDWkcExU4k8YZ/3brWYoyDB7d/gD/1zK21a8OooqoqnCgQ7xLL8X8ie/aEZPLxx7mXLTkz+4O/jRkzxtJt3br1hDZXGO9Tl6T77jMDszVrivs+R46Y9expJpk98UR4zyuuaP81zz4blrv0UrPJk8P9uXPDc/feGx7feWf76/jVr8JyYLZsWZf8Kh0GNFoe21gfkTjnuqW77gpnQxV7D2yvXmH3llnrTI3ZDrTH4hHJqlWhNlifPqG8PLQec8k1ykgtRR+foVauPJE457olKf/rQQoVX7m+fHn4mWuv78CB4aD6sWNhF9gjj0B83W++iSQ+qA+eSJxzrtuLj5NAOAV41qz2l5fg1lvDqcAvvwxf+Urrc+mJ5OjRMMFWQ0M40yuWOiJJvQalHPl1JM45l0M8IoFwQWM+87fdf3+4pevXL7y+uTnUDFuxAh59NDy3fHmoydW794m7tubNgwMHwugmLrbd3Ax9+4YyMaXkIxLnnMth6FC4555QNPLiiwtb1ymnwIAB4f5vfgOLF4f7PXqEM7yefDIcj0kfkXz3uyHhxNP+rlwZzlibO7eweLqCJxLnnMvDd74D3/hG16wr3r21aRP88pchucyZE9o2bAjXqhw+HGZfPO+8ttWC44scFywICWfBAti/v+36jx3rmjjz5YmkRD788EMee+yxDr/Oy7471/3FieSBB8JxkUsuab3IcuPG1gPtQ4aceAV9U1OYh+W558Ljw4chdVPy7LMhAX3rW8X9HVIVNZFImirpLUk7JM3J8HwvSU9Hz78qaVDa8xdIOihpdr7r7C6yJZJc5Ue87Ltz3V+cSKK6qFx/PYwYEUYmW7fCm2+G9iFDwm61VE1N8NRTYdQxaFBoe/TRMIp56KFQIv/DD0OByqTmPSlaIpFUAfwAuAwYDlwraXjaYl8Dfmdmfww8DKQfmvo+cLy+R57r7ESsxbm1Z86cOezcuZNRo0Zx0UUXMWnSJKZNm8bw4eHXmTFjBmPGjKG2tpZFixYdf92gQYPYv38/e/bsoaamhptvvpna2lqmTJnSpoy8c658xYkEQgK55ppw5XtNTUgQ8ZRDQ4aEsirQer1MU1NIEhASx4QJYddWdTXMnh12d512WpjJMbG54/O5arEzN2AC8ELK47uBu9OWeQGYEN3vAewHFD2eAXwPaABm57vOTLdcV7bHV4929a09u3fvttraWjMzW716tVVVVdmuXbuOP9/S0mJmZocOHbLa2lrbv3+/mZkNHDjQmpubbffu3VZRUWEbN240M7Orr77alixZ0v6bFplf2e5cfl58sXU7sXp1a/uXv9x2G7JsmdnRo2br15tt3x7aqqrCz969zT75xGzfPrP6+tDWq5fZ00+b3XhjePzgg4XFSRlc2d4fSK1vuTdqy7iMmR0FPgL6SjoD+Dbw951YJwCSbpHUKKmxubm53UCLlUo6YuzYsQwePPj44wULFjBy5EjGjx/PO++8w/YMJ5J7eXfnuqdx48LV77ffHuZXidXVtd6vrQ1TDFdUQH19qO/Vq1c4ZRjCPPQ9e4bqx6tXh+Mta9eG0U08i+TKlcn8PuV6HUkD8LCZHVSufURZmNkiYBFAfX198hOOdFA8DwjASy+9xIoVK1i7di1VVVVMnjw5Yxn49PLuvmvLue7hzDNPnIoX2l4xf+edbXeRV1SEeeLfeCM8njat9bkzzmgtwQKtiWTNmnAwv2fPros9k2KOSN4Fzk95PCBqy7iMpB7Ap4AWYBzwgKQ9wB3AXEnfzHOd3UK2cu4AH330EX369KGqqoqmpiZeiSczcM79QaurC6VVLrgg89XzcUHwioowIsnmvPPC8ZaPP4b164sTa6pijkjWA0MlDSZs7GcC16UtsxS4HlgL/AWwKtovd7wMm6QG4KCZ/XOUbHKts1vo27cvEydO5MILL+S0007j3JRa2FOnTmXhwoXU1NQwbNgwxo8fX8JInXNJ6d07jFQqK8NurHQ1NeHnpEm564xNmRJGPgXMaZe3+MB2cVYuXQ78E1ABPG5m/yjpHwgHcJZKqgSWAKOB3wIzzWxX2joaCInkwWzrzBVHfX29NTY2tmnbtm0bNfGn4rqE96lzxdXUBNdeG46HXHpp+8ua5T57NBdJG8ysPtdyRT1GYmbPA8+ntc1PuX8EuDrHOhpyrdM5504G1dWt157kUmgS6Qi/st0551xBTupEUszdeicb70vnTl4nbSKprKykpaXFN4BdwMxoaWmhsrKy1KE450qgXK8jKboBAwawd+9ecl2s6PJTWVnJgLg2tnPupHLSJpJTTz21zZXkzjnnOuek3bXlnHOua3gicc45VxBPJM455wpS1Cvby4WkZuC/Ovnyswnl7cuNx9Vx5Rqbx9Ux5RoXlG9snY1roJmdk2uhkyKRFEJSYz4lApLmcXVcucbmcXVMucYF5RtbsePyXVvOOecK4onEOedcQTyR5LYo9yIl4XF1XLnG5nF1TLnGBeUbW1Hj8mMkzjnnCuIjEueccwXxROKcc64gnkiykDRV0luSdkiaU+JYzpe0WtJWSW9Kuj1qb5D0rqRN0e3yEsS2R9Ib0fs3Rm1nSXpR0vboZ45JQbs8pmEpfbJJ0u8l3VGq/pL0uKQPJG1JacvYRwoWRN+7zZLqEo7re5Kaovd+RtKno/ZBkg6n9N3ChOPK+tlJujvqr7ckfTHhuJ5OiWmPpE1Re5L9lW37kNx3zMz8lnYjTOO7ExgC9AReB4aXMJ5+QF10/0zgbWA40ADMLnFf7QHOTmt7AJgT3Z8D3F/iz/J9YGCp+gv4LFAHbMnVR8DlwH8AAsYDryYc1xSgR3T//pS4BqUuV4L+yvjZRX8HrwO9gMHR321FUnGlPf8QML8E/ZVt+5DYd8xHJJmNBXaY2S4z+x/gp8D0UgVjZu+Z2WvR/QPANqB/qeLJw3TgR9H9HwEzShjLF4CdZtbZygYFM7M1wG/TmrP10XRgsQWvAJ+W1C+puMxsuZkdjR6+AiQ+N0CW/spmOvBTM/vEzHYDOwh/v4nGJUnANcBPivHe7Wln+5DYd8wTSWb9gXdSHu+lTDbckgYBo4FXo6ZvRsPTx5PehRQxYLmkDZJuidrONbP3ovvvA+eWIK7YTNr+cZe6v2LZ+qicvnt/SfjPNTZY0kZJv5Y0qQTxZPrsyqW/JgH7zGx7Slvi/ZW2fUjsO+aJpBuRdAbwM+AOM/s98EPgj4BRwHuEoXXSLjazOuAy4K8kfTb1SQtj6ZKcYy6pJzAN+PeoqRz66wSl7KNsJM0DjgJPRU3vAReY2WjgTuDHknonGFJZfnYprqXtPyyJ91eG7cNxxf6OeSLJ7F3g/JTHA6K2kpF0KuFL8pSZ/RzAzPaZ2TEz+z/gXyjSkL49ZvZu9PMD4Jkohn3xUDn6+UHScUUuA14zs31RjCXvrxTZ+qjk3z1JNwBXArOiDRDRrqOW6P4GwrGIP0kqpnY+u3Lorx7AnwFPx21J91em7QMJfsc8kWS2HhgqaXD0X+1MYGmpgon2v/4rsM3Mvp/Snrpf80+BLemvLXJcp0s6M75POFC7hdBX10eLXQ/8Ism4UrT5L7HU/ZUmWx8tBb4anVkzHvgoZfdE0UmaCtwFTDOzQynt50iqiO4PAYYCuxKMK9tntxSYKamXpMFRXOuSiityCdBkZnvjhiT7K9v2gSS/Y0mcVdAdb4QzG94m/Ccxr8SxXEwYlm4GNkW3y4ElwBtR+1KgX8JxDSGcMfM68GbcT0BfYCWwHVgBnFWCPjsdaAE+ldJWkv4iJLP3gP8l7I/+WrY+IpxJ84Poe/cGUJ9wXDsI+8/j79nCaNk/jz7jTcBrwFUJx5X1swPmRf31FnBZknFF7U8CX09bNsn+yrZ9SOw75iVSnHPOFcR3bTnnnCuIJxLnnHMF8UTinHOuIJ5InHPOFcQTiXPOuYJ4InGuAyTdK+nzkmZIurtEMbwkqb4U7+1cJp5InOuYcYRihp8D1pQ4FufKgicS5/KgME/HZuAiYC1wE/BDSfMzLHuOpJ9JWh/dJkbtDZKWSFobzRFxc9SuaP1bFOZ2+VLKur4dtb0u6b6Ut7la0jpJb8cFASXVRm2bouKGQ4vYJc4d16PUATjXHZjZ30r6N+CrhCJ8L5nZxCyLPwI8bGb/KekC4AWgJnruM4Q5IE4HNkp6DphAKEY4EjgbWC9pTdQ2HRhnZocknZXyHj3MbKzCBE9/RyjT8XXgETN7KirtU9FlHeBcOzyROJe/OkI5mGrCnA/ZXAIMDyWQAOgdVWYF+IWZHQYOS1pNKD54MfATMztGKLT3a8LI53PAExbVvDKz1Lkw4sJ8GwiTKEEYKc2TNAD4ubUtae5c0XgicS4HSaMI9ZQGAPuBqtCsTcCEKDGkOgUYb2ZH0tYDJ5by7myNok+in8eI/o7N7MeSXgWuAJ6XdKuZrerk+p3Lmx8jcS4HM9tkZqNoncJ0FfBFMxuVIYkALAf+On4QJaLYdEmVkvoCkwmVpl8GviSpQtI5hCld1wEvAjdKqorWk7pr6wRRldldZraAUOn1M536hZ3rIE8kzuUh2sD/zsJ8GNVmtrWdxf8GqI8OeG8lHLuIbQZWE878usfM/pswj8tmwm6zVcBdZva+mS0jVLptjEY/s3OEeQ2wJVr2QmBxh39R5zrBq/86lxBJDcBBM3uw1LE415V8ROKcc64gPiJxzjlXEB+ROOecK4gnEueccwXxROKcc64gnkicc84VxBOJc865gvw/qResdkuLCGkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot evolution of validation and training loss\n",
    "plt.plot(val_loss_history, color = 'orange', linewidth = 2, label='validation')\n",
    "plt.plot(train_loss_history, color = 'blue', linewidth = 2,label='train')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train & validation loss')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.003927695099264383  val loss= 0.07085386663675308\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0030185796786099672  val loss= 0.08857820928096771\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0020953130442649126  val loss= 0.09912322461605072\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.004011702723801136  val loss= 0.06972889602184296\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0029853968881070614  val loss= 0.08465660363435745\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.002010883530601859  val loss= 0.1149858757853508\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0035634133964776993  val loss= 0.07253430038690567\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.003040524199604988  val loss= 0.08400667458772659\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0016467527020722628  val loss= 0.10290485620498657\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.003792842384427786  val loss= 0.06852924823760986\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0022720135748386383  val loss= 0.09276363998651505\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0022603082470595837  val loss= 0.08745045959949493\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0036222494672983885  val loss= 0.07479458302259445\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0026332661509513855  val loss= 0.09213656932115555\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0017791754798963666  val loss= 0.10199239104986191\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0035975994542241096  val loss= 0.08021119982004166\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.002659137360751629  val loss= 0.0976099818944931\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0019398131407797337  val loss= 0.10640142112970352\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0033736226614564657  val loss= 0.08154164999723434\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.002056857803836465  val loss= 0.09812286496162415\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.001333076972514391  val loss= 0.11876395344734192\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.003380640409886837  val loss= 0.08030392974615097\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0020158695988357067  val loss= 0.09038043767213821\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.001141563174314797  val loss= 0.10604932904243469\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0033394380006939173  val loss= 0.07148311287164688\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.002185849007219076  val loss= 0.10049735009670258\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0007944028475321829  val loss= 0.1246240958571434\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.003387057688087225  val loss= 0.07249919325113297\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0020871050655841827  val loss= 0.08817985653877258\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0011124106822535396  val loss= 0.1072038784623146\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0028440901078283787  val loss= 0.08094905316829681\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.001283960766158998  val loss= 0.11250785738229752\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0008224225020967424  val loss= 0.12443254888057709\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.003140298882499337  val loss= 0.07912108302116394\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0013071568682789803  val loss= 0.09471379220485687\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0009141196496784687  val loss= 0.13552004098892212\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0031824957113713026  val loss= 0.07803326100111008\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0013489894336089492  val loss= 0.10219234228134155\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0008460169774480164  val loss= 0.11198601871728897\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0036945815663784742  val loss= 0.07297086715698242\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.001958023989573121  val loss= 0.09856048971414566\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0012441723374649882  val loss= 0.12929455935955048\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0028607388958334923  val loss= 0.08476712554693222\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0015798351960256696  val loss= 0.10598686337471008\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0009916203562170267  val loss= 0.12400303035974503\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002686150372028351  val loss= 0.08033215999603271\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0014267110964283347  val loss= 0.10326852649450302\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0009530926472507417  val loss= 0.10943515598773956\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.00415645819157362  val loss= 0.06430511921644211\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.003368401899933815  val loss= 0.07237482070922852\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0026093553751707077  val loss= 0.07863178849220276\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.004112407565116882  val loss= 0.0656910091638565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.003232327988371253  val loss= 0.07380946725606918\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.003187831724062562  val loss= 0.07735157757997513\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.004144119564443827  val loss= 0.06473288685083389\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.003744367742910981  val loss= 0.06798547506332397\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.003187453141435981  val loss= 0.07639195024967194\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.004365058615803719  val loss= 0.06653944402933121\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.003285328159108758  val loss= 0.07133159786462784\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.003079623682424426  val loss= 0.07151955366134644\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.003912992775440216  val loss= 0.06689835339784622\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0027472549118101597  val loss= 0.0765259638428688\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0023966608569025993  val loss= 0.0809892937541008\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.003917104098945856  val loss= 0.06856946647167206\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.00290353293530643  val loss= 0.08176515996456146\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.001962840324267745  val loss= 0.08921236544847488\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0037783822044730186  val loss= 0.06977368891239166\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0026455612387508154  val loss= 0.07616440206766129\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0019398621516302228  val loss= 0.08439034223556519\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0038753084372729063  val loss= 0.06949000060558319\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.002990742912515998  val loss= 0.07192227989435196\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0021348141599446535  val loss= 0.08795680105686188\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0033801244571805  val loss= 0.07417529076337814\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0023246491327881813  val loss= 0.0746195837855339\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0016672898782417178  val loss= 0.08892574161291122\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0036400575190782547  val loss= 0.0763799324631691\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0020221860613673925  val loss= 0.07872913777828217\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0014461056562140584  val loss= 0.08848351985216141\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0034858067519962788  val loss= 0.069387286901474\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0024366765283048153  val loss= 0.07863112539052963\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0018317796057090163  val loss= 0.08469610661268234\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.00347339641302824  val loss= 0.07056906819343567\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.002354401396587491  val loss= 0.08403146266937256\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.001372521510347724  val loss= 0.09097444266080856\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0032368553802371025  val loss= 0.07793053239583969\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0021491385996341705  val loss= 0.08361724764108658\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0013382027391344309  val loss= 0.0958356112241745\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0033714030869305134  val loss= 0.07801694422960281\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.002026899717748165  val loss= 0.08907242119312286\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.001306159538216889  val loss= 0.1001729816198349\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0033406761940568686  val loss= 0.07744637131690979\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0020311963744461536  val loss= 0.08167221397161484\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0013065568637102842  val loss= 0.08853708207607269\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0034122683573514223  val loss= 0.07759615033864975\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0023910733871161938  val loss= 0.08694179356098175\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0016070884885266423  val loss= 0.07995740324258804\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0030671756248921156  val loss= 0.08119068294763565\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0012548824306577444  val loss= 0.12687154114246368\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0012688360875472426  val loss= 0.14630341529846191\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0029324418865144253  val loss= 0.09455584734678268\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0009612307185307145  val loss= 0.10204485803842545\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0007252179784700274  val loss= 0.12792091071605682\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0023056110367178917  val loss= 0.07678449898958206\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0010779070435091853  val loss= 0.1427355408668518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0002918201207648963  val loss= 0.15249493718147278\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002508194651454687  val loss= 0.08254978805780411\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0009239904466085136  val loss= 0.12770617008209229\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.00039028271567076445  val loss= 0.14154396951198578\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002130968263372779  val loss= 0.086735300719738\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0007876470917835832  val loss= 0.11744284629821777\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0009324493003077805  val loss= 0.15444189310073853\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.001969841541722417  val loss= 0.08221524208784103\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0007333449320867658  val loss= 0.11070653051137924\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0006098520243540406  val loss= 0.15029792487621307\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0014769523404538631  val loss= 0.08308904618024826\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0004409618559293449  val loss= 0.144526869058609\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.00044338792213238776  val loss= 0.17734837532043457\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0019503614166751504  val loss= 0.08267446607351303\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0008769153500907123  val loss= 0.15398263931274414\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0004092414164915681  val loss= 0.15290094912052155\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002386576496064663  val loss= 0.07296295464038849\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0005633031250908971  val loss= 0.14765425026416779\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0004681332502514124  val loss= 0.1441498100757599\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0027944250032305717  val loss= 0.08856745064258575\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0008766163373365998  val loss= 0.12965407967567444\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0006851661601103842  val loss= 0.1320885568857193\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.00209508347325027  val loss= 0.07551782578229904\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0005479029496200383  val loss= 0.1345912218093872\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.00023631412477698177  val loss= 0.15989115834236145\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0021381359547376633  val loss= 0.09570761024951935\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0005201904568821192  val loss= 0.12717899680137634\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.00017750912229530513  val loss= 0.14750976860523224\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002800134476274252  val loss= 0.08602818846702576\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.001833699643611908  val loss= 0.1087879091501236\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0028394917026162148  val loss= 0.09554324299097061\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0028353831730782986  val loss= 0.07849029451608658\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0024160495959222317  val loss= 0.08951397240161896\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0011848948197439313  val loss= 0.10528315603733063\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002112369053065777  val loss= 0.10191317647695541\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.00136409024707973  val loss= 0.10700013488531113\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0007362039177678525  val loss= 0.1482417732477188\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0027195727452635765  val loss= 0.08614958822727203\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0015501126181334257  val loss= 0.11550458520650864\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0005324209341779351  val loss= 0.1446293741464615\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0028985803946852684  val loss= 0.0805213451385498\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0016718532424420118  val loss= 0.10228917002677917\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0012102233013138175  val loss= 0.12582729756832123\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002978880889713764  val loss= 0.08546406775712967\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.00181849196087569  val loss= 0.1078987792134285\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.001164266373962164  val loss= 0.15203016996383667\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0023837327025830746  val loss= 0.08595556765794754\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0011219127336516976  val loss= 0.1196221113204956\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.000832568621262908  val loss= 0.1271931529045105\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002259846543893218  val loss= 0.08006210625171661\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0011076165828853846  val loss= 0.1410718858242035\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0007784343906678259  val loss= 0.1140655130147934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002560235094279051  val loss= 0.08519311994314194\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0009003120940178633  val loss= 0.13351081311702728\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0003893607354257256  val loss= 0.13447387516498566\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002418855670839548  val loss= 0.09221427142620087\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0010786282364279032  val loss= 0.1284964233636856\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.00048094106023199856  val loss= 0.15751582384109497\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002051748801022768  val loss= 0.10151241719722748\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0007835006108507514  val loss= 0.14061476290225983\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.00017309976101387292  val loss= 0.1440344750881195\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0017598908161744475  val loss= 0.11043772846460342\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0006323480047285557  val loss= 0.1322542130947113\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0003533522831276059  val loss= 0.1493300497531891\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002500377129763365  val loss= 0.09683819860219955\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0007043385412544012  val loss= 0.13167499005794525\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.00046265972196124494  val loss= 0.14562678337097168\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0023334708530455828  val loss= 0.09325522929430008\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0007385523640550673  val loss= 0.14192011952400208\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0003270349698141217  val loss= 0.14832735061645508\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0015506655909121037  val loss= 0.10150924324989319\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0004891887656413019  val loss= 0.13177481293678284\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0004341521125752479  val loss= 0.13836534321308136\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0017091486370190978  val loss= 0.09819672256708145\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0006761474069207907  val loss= 0.13255611062049866\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.00014136086974758655  val loss= 0.15240277349948883\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.0023016533814370632  val loss= 0.08899670094251633\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0023716059513390064  val loss= 0.12036514282226562\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0017753097927197814  val loss= 0.11418332159519196\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.0028517714235931635  val loss= 0.08105591684579849\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0017597423866391182  val loss= 0.11024833470582962\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.0020166607573628426  val loss= 0.15822400152683258\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  train loss= 0.002291367156431079  val loss= 0.08332428336143494\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  train loss= 0.0013642138801515102  val loss= 0.1188662052154541\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  train loss= 0.0005876427749171853  val loss= 0.15626835823059082\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  train loss= 0.002031169133260846  val loss= 0.1034468412399292\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  train loss= 0.0009129559621214867  val loss= 0.14175677299499512\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  train loss= 0.000653293274808675  val loss= 0.1341695785522461\n",
      " selected model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# now hyper parameter search\n",
    "# using hold out validation\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, loader, optimizer, train_loss_history):\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(out, target)\n",
    "        loss_train +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train = loss_train / len(train_dataset)\n",
    "    train_loss_history.append(loss_train.item()) \n",
    "    \n",
    "def val_loss_model(model, loader, optimizer, val_loss_history):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    for batch in loader_val:\n",
    "        data = batch.to(device)\n",
    "\n",
    "        pred = model(batch)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        loss_val += loss\n",
    "    loss_val = loss_val / len(val_dataset)\n",
    "    val_loss_history.append(loss_val.item())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for modeldict in model_list:\n",
    "    \n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        val_loss_model(model, loader, optimizer, val_loss_history)\n",
    "    print(\" trained model: \",modeldict['model'].__name__,\n",
    "          modeldict['kwargs'], \" epochs:\",modeldict['epochs'],\n",
    "          \" train loss=\",train_loss_history[-1],\n",
    "         ' val loss=',val_loss_history[-1])\n",
    "\n",
    "\n",
    "    # save results\n",
    "    modeldict['train_loss_history']=train_loss_history\n",
    "    modeldict['val_loss_history']=val_loss_history\n",
    "    modeldict['val_loss']=val_loss_history[-1]\n",
    "    \n",
    "    \n",
    "# select the best model (lower validation loss)\n",
    "losses = np.array([ modeldict['val_loss'] for modeldict in model_list])\n",
    "best_model = model_list[np.argmin(losses)]\n",
    "print(\" selected model: \",best_model['model'].__name__,best_model['kwargs'],\" epochs:\", best_model['epochs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0')\n",
      "tensor([3, 2, 1, 0, 2, 3, 5, 0, 4, 3, 4, 0, 2, 4, 4, 4, 1, 1, 0, 5, 5, 4, 3, 3,\n",
      "        1, 2, 5, 0, 1, 0, 3, 0, 3, 1, 4, 4, 4, 3, 1, 0, 4, 3, 0, 1, 4, 2, 1, 1,\n",
      "        1, 2, 1, 3, 1, 3, 1, 5, 5, 2, 2, 0, 3, 2, 1, 1, 1, 4, 0, 0, 1, 0, 5, 4,\n",
      "        2, 2, 4, 1, 2, 5, 1, 4, 4, 3, 3, 1, 5, 4, 1, 5, 2, 4, 5, 3, 3, 1, 3, 3,\n",
      "        1, 1, 2, 5, 5, 0, 3, 0, 5, 2, 4, 2, 0, 3, 5, 5, 5, 0, 1, 1, 3, 2, 2, 5],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2250\n"
     ]
    }
   ],
   "source": [
    "model = best_model['model']\n",
    "modelclass = best_model['model']\n",
    "kwargs = best_model['kwargs']\n",
    "model = modelclass(**kwargs)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"len(test_dataset): \", len(test_dataset))\n",
    "loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "for batch in loader:\n",
    "    batch = batch.to(device)\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    #_, pred = model(test_dataset).max(dim=1)\n",
    "    _, pred = model(batch).max(dim=1)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    \n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    print('Accuracy: {:.4f}'.format(acc))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "1. implemented a validation procedure and plotted train and val loss\n",
    "2. encapsulated train and validation (hold-out validation)\n",
    "3. implemente hyperparameter search and model selection\n",
    "### Pending:\n",
    "- implement K-fold cross validation? (train model and validate k times for the k folds!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 k-fold Cross validation w dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# count how many graphs of each class in the dataset\n",
    "def printDatasetBalance(dataset):\n",
    "    num_classes = dataset.num_classes\n",
    "    class_counts = { i:0 for i in range(num_classes)}\n",
    "    #print(class_counts)\n",
    "    for graph in dataset:\n",
    "        class_counts[int(graph.y.item())]+=1\n",
    "    print(class_counts)\n",
    "    \n",
    "    \n",
    "def balancedDatasetSplit_list(dataset, prop):\n",
    "    \n",
    "    dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for graph in dataset:\n",
    "        datasets_byclass[int(graph.y.item())].append(graph)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_dataset.extend(datasets_byclass[c][:limit])\n",
    "        test_dataset.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "def balancedDatasetSplit_slice(dataset, prop):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    x=torch.Tensor([True,False,True])==True\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    train_dataset_slice = [False]*n\n",
    "    test_dataset_slice = [False]*n\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "    #print(\"datasets_byclass\", datasets_byclass)\n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(\"datasets_byclass\",datasets_byclass)\n",
    "        \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_list.extend(datasets_byclass[c][:limit])\n",
    "        test_list.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    #print(\"train_list\", train_list)\n",
    "    #print(\"test_list\", test_list)\n",
    "\n",
    "        \n",
    "    # now from list of integers(indices) to boolean mask tensor\n",
    "    #for i in range(len(train_list)):\n",
    "    #    real_index = train_list[i]\n",
    "    #    train_dataset_slice[real_index] = True\n",
    "        \n",
    "    #for i in range(len(test_list)):\n",
    "    #    real_index = test_list[i]\n",
    "    #    test_dataset_slice[real_index] = True\n",
    "        \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "        \n",
    "    train_dataset = dataset[torch.LongTensor(train_list)]\n",
    "    test_dataset = dataset[torch.LongTensor(test_list)]\n",
    "        \n",
    "    #print(\"train_dataset\", train_dataset)\n",
    "    #print(\"test_dataset\", test_dataset)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def balancedDatasetKfoldSplit_slice(dataset,k):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    \n",
    "    foldsize = int(n/k)\n",
    "    num_classes = dataset.num_classes\n",
    "    num_items_x_class = int(foldsize/num_classes)\n",
    "    \n",
    "    # list of items for each class\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(datasets_byclass)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append([])\n",
    "        for c in range(num_classes):\n",
    "            for j in range(num_items_x_class):\n",
    "                index = datasets_byclass[c].pop()\n",
    "                folds[i].append(index)\n",
    "        \n",
    "    # returns a list of list of indices\n",
    "    return folds\n",
    "\n",
    "def kFolding(train_dataset, k):\n",
    "    n = len(train_dataset)\n",
    "    fold_size = int(n/k)\n",
    "    \n",
    "    # build folds\n",
    "    #folds = []\n",
    "    #for i in range(k):\n",
    "    #    i1 = i*fold_size\n",
    "    #    i2 = i1+fold_size\n",
    "    #    folds.append((i1,i2))\n",
    "    #print(folds)\n",
    "    \n",
    "    # build train-val sets\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        preval_index = (0,i*fold_size)\n",
    "        val_index = (i*fold_size,i*fold_size+fold_size)\n",
    "        postval_index = (i*fold_size+fold_size,n)\n",
    "        train_sets.append((preval_index, val_index, postval_index))\n",
    "        \n",
    "    #print(train_sets)\n",
    "    return train_sets\n",
    "\n",
    "def kFolding2(train_dataset, k):\n",
    "\n",
    "    #print(\" train_dataset len:\", len(train_dataset))\n",
    "    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        # each train_set must have a torch.LongTensor for train indices\n",
    "        # and a torch.LongTensor for val indices\n",
    "        val_merge = folds[i]\n",
    "        train_merge = [] \n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train_merge.extend(folds[j])\n",
    "        train_sets.append((torch.LongTensor(train_merge), torch.LongTensor(val_merge)))\n",
    "    \n",
    "    return train_sets\n",
    "    \n",
    "\n",
    "\n",
    "def accuracy(pred, batch):\n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, train_loss_history):\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    total_num_graphs = 0\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(out, target)\n",
    "        loss_train +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_train = loss_train /total_num_graphs\n",
    "    train_loss_history.append(loss_train.item()) \n",
    "    \n",
    "def val_loss_model(model, loader, optimizer, val_loss_history,val_accuracy_history):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    total_num_graphs = 0\n",
    "    for batch in loader_val:\n",
    "        data = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        _, predacc = pred.max(dim=1)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        loss_val += loss\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_val = loss_val / total_num_graphs\n",
    "    val_loss_history.append(loss_val.item())\n",
    "    val_accuracy_history.append(accuracy(predacc, batch))\n",
    "    \n",
    "    \n",
    "def final_model_train(modeldict, train_dataset):\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history=[]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Retrain the best model\n",
    "def final_model_train(modeldict, train_dataset):\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history=[]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 600  k folds= 3  test_lim: 480\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "{0: 80, 1: 80, 2: 80, 3: 80, 4: 80, 5: 80}\n",
      "{0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "\n",
    "# shuffling dataset\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# train-val-test setting\n",
    "k = 3\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "\n",
    "print(\" n:\",n,\" k folds=\",k,\" test_lim:\",test_lim)\n",
    "\n",
    "\n",
    "    \n",
    "#train_dataset = dataset[:test_lim]\n",
    "#test_dataset = dataset[test_lim:]\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "\n",
    "# amazingly enough the datasets appear to be in balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05130531763037046  val accuracy= 0.42857142857142855\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05099687104423841  val accuracy= 0.5476190476190476\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.054085383812586464  val accuracy= 0.4047619047619047\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05210575337211291  val accuracy= 0.42857142857142855\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05031448105971018  val accuracy= 0.4047619047619047\n",
      " trained model:  Net1 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05583833406368891  val accuracy= 0.4166666666666667\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.04922940582036972  val accuracy= 0.5\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05035300304492315  val accuracy= 0.5119047619047619\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05428597082694372  val accuracy= 0.5119047619047619\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.04901358733574549  val accuracy= 0.35714285714285715\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.050457973033189774  val accuracy= 0.44047619047619047\n",
      " trained model:  Net1 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05129615714152654  val accuracy= 0.5476190476190476\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05317797511816025  val accuracy= 0.35714285714285715\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.054147991041342415  val accuracy= 0.4166666666666667\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05985771119594574  val accuracy= 0.46428571428571425\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05338849499821663  val accuracy= 0.3333333333333333\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.0533982552587986  val accuracy= 0.46428571428571425\n",
      " trained model:  Net1 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.06173307696978251  val accuracy= 0.36904761904761907\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.052047676096359886  val accuracy= 0.36904761904761907\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05413857847452164  val accuracy= 0.36904761904761907\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.059166550636291504  val accuracy= 0.4166666666666667\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05244216447075208  val accuracy= 0.3333333333333333\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.056213922798633575  val accuracy= 0.4166666666666667\n",
      " trained model:  Net1 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.06027820830543836  val accuracy= 0.4523809523809524\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05513766035437584  val accuracy= 0.32142857142857145\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05412741626302401  val accuracy= 0.32142857142857145\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.058535429338614144  val accuracy= 0.39285714285714285\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05583488941192627  val accuracy= 0.2261904761904762\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05700136348605156  val accuracy= 0.20238095238095236\n",
      " trained model:  Net1 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05847888315717379  val accuracy= 0.44047619047619047\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05292053520679474  val accuracy= 0.3333333333333333\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.054570225377877556  val accuracy= 0.36904761904761907\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05747978140910467  val accuracy= 0.36904761904761907\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05700754622618357  val accuracy= 0.21428571428571427\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.053682342171669006  val accuracy= 0.30952380952380953\n",
      " trained model:  Net1 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05472266053160032  val accuracy= 0.4047619047619047\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05608700339992841  val accuracy= 0.2261904761904762\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05606582760810852  val accuracy= 0.3214285714285714\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.057428111632665  val accuracy= 0.15476190476190474\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05753992249568304  val accuracy= 0.15476190476190477\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05599170053998629  val accuracy= 0.16666666666666666\n",
      " trained model:  Net1 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05510649705926577  val accuracy= 0.30952380952380953\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05742762486139933  val accuracy= 0.21428571428571427\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05742965266108513  val accuracy= 0.15476190476190474\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05739587917923927  val accuracy= 0.19047619047619047\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05734589695930481  val accuracy= 0.19047619047619047\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.054804266740878425  val accuracy= 0.2619047619047619\n",
      " trained model:  Net1 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.056742159028848015  val accuracy= 0.11904761904761903\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05173748483260473  val accuracy= 0.46428571428571425\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.051564812660217285  val accuracy= 0.3452380952380953\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05030652632315954  val accuracy= 0.42857142857142855\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05327610050638517  val accuracy= 0.35714285714285715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05039196337262789  val accuracy= 0.3333333333333333\n",
      " trained model:  Net2 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05208001285791397  val accuracy= 0.4166666666666667\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05252468213438988  val accuracy= 0.3333333333333333\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.050344448536634445  val accuracy= 0.42857142857142855\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.052339255809783936  val accuracy= 0.3928571428571428\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05269894003868103  val accuracy= 0.4404761904761905\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05009169007341067  val accuracy= 0.42857142857142855\n",
      " trained model:  Net2 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.0506668488184611  val accuracy= 0.4523809523809524\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05366299922267596  val accuracy= 0.2976190476190476\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05405264596144358  val accuracy= 0.48809523809523814\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.053554234405358635  val accuracy= 0.35714285714285715\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05012450988094012  val accuracy= 0.4761904761904762\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.051834944635629654  val accuracy= 0.35714285714285715\n",
      " trained model:  Net2 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05078671251734098  val accuracy= 0.4523809523809524\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.0522556242843469  val accuracy= 0.44047619047619047\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.051053074498971306  val accuracy= 0.48809523809523814\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05416647841533025  val accuracy= 0.36904761904761907\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05383156736691793  val accuracy= 0.38095238095238093\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.052097720404465996  val accuracy= 0.46428571428571425\n",
      " trained model:  Net2 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05213627964258194  val accuracy= 0.4523809523809524\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05402109896143278  val accuracy= 0.3452380952380952\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.0562057135005792  val accuracy= 0.36904761904761907\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.0567144180337588  val accuracy= 0.35714285714285715\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.054812099784612656  val accuracy= 0.2976190476190476\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05556230743726095  val accuracy= 0.3928571428571428\n",
      " trained model:  Net2 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05613246684273084  val accuracy= 0.38095238095238093\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05362076312303543  val accuracy= 0.3333333333333333\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.054500299195448555  val accuracy= 0.42857142857142855\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05463599041104317  val accuracy= 0.35714285714285715\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05548374727368355  val accuracy= 0.3333333333333333\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05528482794761658  val accuracy= 0.3928571428571428\n",
      " trained model:  Net2 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.0564704475303491  val accuracy= 0.4047619047619048\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05703640232483546  val accuracy= 0.21428571428571427\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05881449207663536  val accuracy= 0.28571428571428575\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.057789636154969536  val accuracy= 0.2738095238095238\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05667817344268163  val accuracy= 0.21428571428571427\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05719297503431638  val accuracy= 0.25\n",
      " trained model:  Net2 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.057142352064450584  val accuracy= 0.2857142857142857\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.058519204457600914  val accuracy= 0.25\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05722669387857119  val accuracy= 0.2976190476190476\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.056973718106746674  val accuracy= 0.2380952380952381\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05702269822359085  val accuracy= 0.17857142857142858\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.057568381230036415  val accuracy= 0.21428571428571427\n",
      " trained model:  Net2 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05646477763851484  val accuracy= 0.2738095238095238\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.051152490079402924  val accuracy= 0.32142857142857145\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.0485391691327095  val accuracy= 0.4761904761904761\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.055481452494859695  val accuracy= 0.5\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.052691260973612465  val accuracy= 0.35714285714285715\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05601849779486656  val accuracy= 0.42857142857142855\n",
      " trained model:  Net3 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.050487746795018516  val accuracy= 0.4761904761904762\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05145990724364916  val accuracy= 0.46428571428571425\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.054720492412646614  val accuracy= 0.4761904761904761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.049372024834156036  val accuracy= 0.5119047619047619\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.0523978074391683  val accuracy= 0.4047619047619048\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05201594283183416  val accuracy= 0.4761904761904762\n",
      " trained model:  Net3 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05428477500875791  val accuracy= 0.5357142857142857\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05149538069963455  val accuracy= 0.4047619047619048\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05549837897221247  val accuracy= 0.36904761904761907\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.052949423591295876  val accuracy= 0.38095238095238093\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05270438268780708  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05222365135947863  val accuracy= 0.3095238095238095\n",
      " trained model:  Net3 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05298172806700071  val accuracy= 0.4047619047619048\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05435127019882202  val accuracy= 0.35714285714285715\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.051114181677500405  val accuracy= 0.3452380952380952\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05311903233329455  val accuracy= 0.38095238095238093\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05164900173743566  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.053284648805856705  val accuracy= 0.4285714285714286\n",
      " trained model:  Net3 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.058454036712646484  val accuracy= 0.4166666666666667\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05348103120923042  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.052949720372756325  val accuracy= 0.42857142857142855\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05215216800570488  val accuracy= 0.35714285714285715\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05335454891125361  val accuracy= 0.2738095238095238\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05543696756164233  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05339832976460457  val accuracy= 0.2976190476190476\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.051762634267409645  val accuracy= 0.2738095238095238\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05132100606958071  val accuracy= 0.36904761904761907\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.052411297957102455  val accuracy= 0.32142857142857145\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05539993445078532  val accuracy= 0.35714285714285715\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05294836685061455  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05432870735724767  val accuracy= 0.36904761904761907\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05455128227670988  val accuracy= 0.36904761904761907\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.053364756206671395  val accuracy= 0.2976190476190476\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05357204253474871  val accuracy= 0.35714285714285715\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.055963627994060516  val accuracy= 0.25\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.056338769694169365  val accuracy= 0.3095238095238095\n",
      " trained model:  Net3 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.053019276509682335  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05347152675191561  val accuracy= 0.3333333333333333\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05507862443725268  val accuracy= 0.27380952380952384\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.053404426823059716  val accuracy= 0.2976190476190476\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05701892698804537  val accuracy= 0.2619047619047619\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.0527263879776001  val accuracy= 0.39285714285714285\n",
      " trained model:  Net3 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.053048444290955864  val accuracy= 0.30952380952380953\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.04821902513504028  val accuracy= 0.5\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.04856414472063383  val accuracy= 0.6547619047619048\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.054746028035879135  val accuracy= 0.4761904761904761\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05039443944891294  val accuracy= 0.4404761904761904\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.0561342624326547  val accuracy= 0.5238095238095238\n",
      " trained model:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.0559442900121212  val accuracy= 0.4880952380952381\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.04833455880482992  val accuracy= 0.4404761904761905\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.056208514918883644  val accuracy= 0.4523809523809524\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.055594250559806824  val accuracy= 0.5595238095238096\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.050900574773550034  val accuracy= 0.380952380952381\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.04930384581287702  val accuracy= 0.5476190476190476\n",
      " trained model:  Net4 {'d1': 25, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05426645651459694  val accuracy= 0.48809523809523814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05115327859918276  val accuracy= 0.4761904761904762\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05443843578298887  val accuracy= 0.46428571428571425\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05347987885276476  val accuracy= 0.5119047619047619\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.051838941872119904  val accuracy= 0.36904761904761907\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.053514706591765084  val accuracy= 0.45238095238095233\n",
      " trained model:  Net4 {'d1': 50, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.062047707537810005  val accuracy= 0.5357142857142857\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05393653362989426  val accuracy= 0.5\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05490343024333318  val accuracy= 0.38095238095238093\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05725813532869021  val accuracy= 0.4523809523809524\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.053502537310123444  val accuracy= 0.4523809523809524\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05609915032982826  val accuracy= 0.46428571428571425\n",
      " trained model:  Net4 {'d1': 50, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05421685054898262  val accuracy= 0.5833333333333334\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.054269175976514816  val accuracy= 0.2976190476190476\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.052207451313734055  val accuracy= 0.42857142857142855\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.053362030535936356  val accuracy= 0.3452380952380953\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05763557180762291  val accuracy= 0.2976190476190476\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05486374720931053  val accuracy= 0.3452380952380952\n",
      " trained model:  Net4 {'d1': 100, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.05338916555047035  val accuracy= 0.2857142857142857\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05289386337002119  val accuracy= 0.2738095238095238\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.052128550906976066  val accuracy= 0.38095238095238093\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05532739808162054  val accuracy= 0.3452380952380953\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.055083855986595154  val accuracy= 0.2738095238095238\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.0531670277317365  val accuracy= 0.35714285714285715\n",
      " trained model:  Net4 {'d1': 100, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.052866837630669274  val accuracy= 0.36904761904761907\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05539866164326668  val accuracy= 0.2976190476190476\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05369376018643379  val accuracy= 0.25\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05480287472407023  val accuracy= 0.2619047619047619\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05465940137704214  val accuracy= 0.3452380952380952\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05628614624341329  val accuracy= 0.2738095238095238\n",
      " trained model:  Net4 {'d1': 200, 'd2': 20, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.053515935937563576  val accuracy= 0.2738095238095238\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100  val loss= 0.05715851113200188  val accuracy= 0.2619047619047619\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200  val loss= 0.05426799630125364  val accuracy= 0.25\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 300  val loss= 0.05688582236568133  val accuracy= 0.3452380952380952\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 100  val loss= 0.05683455616235733  val accuracy= 0.2619047619047619\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 200  val loss= 0.05424903084834417  val accuracy= 0.35714285714285715\n",
      " trained model:  Net4 {'d1': 200, 'd2': 50, 'num_layers': 2, 'aggr_type': 'add'}  epochs: 300  val loss= 0.0548708550632  val accuracy= 0.2619047619047619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "kfolds = kFolding2(train_dataset,k)\n",
    "#print(\"kfolds\",kfolds)\n",
    "\n",
    "for modeldict in model_list:\n",
    "\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_accuracy_history = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    modeldict['cv_val_loss']=0.0\n",
    "    modeldict['cv_val_accuracy']=0.0\n",
    "    \n",
    "    for kfold in kfolds:\n",
    "        \n",
    "        # train-val for each fold configuration\n",
    "        #train1 = []\n",
    "        #train2 = []\n",
    "        #if kfold[0][0] != kfold[0][1]:\n",
    "        #    train1 = list(range(kfold[0][0],kfold[0][1])) \n",
    "        #if kfold[2][0] != kfold[2][1]:\n",
    "        #    train2 = list(range(kfold[2][0],kfold[2][1]))\n",
    "        #    \n",
    "        #train1.extend(train2)\n",
    "        #train_subset = torch.LongTensor(train1)\n",
    "        # \n",
    "        #train = train_dataset[train_subset]\n",
    "        #val = train_dataset[kfold[1][0]:kfold[1][1]] \n",
    "        \n",
    "        train = train_dataset[kfold[0]]\n",
    "        val = train_dataset[kfold[1]]\n",
    "        \n",
    "        #print(\"train balance\")\n",
    "        #printDatasetBalance(train)\n",
    "        #print(\"val balance\")\n",
    "        #printDatasetBalance(val )\n",
    "        \n",
    "        loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "        loader_val = DataLoader(val, batch_size=32, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_model(model, loader, optimizer, train_loss_history)\n",
    "            val_loss_model(model, loader_val, optimizer, val_loss_history, val_accuracy_history)\n",
    "\n",
    "        # save results\n",
    "        modeldict['train_loss_history']=train_loss_history\n",
    "        modeldict['val_loss_history']=val_loss_history\n",
    "        modeldict['val_accuracy_history']=val_accuracy_history\n",
    "        modeldict['val_loss']=val_loss_history[-1]\n",
    "        modeldict['accuracy']=val_accuracy_history[-1]\n",
    "        modeldict['cv_val_loss']+=modeldict['val_loss']\n",
    "        modeldict['cv_val_accuracy']+=modeldict['accuracy']\n",
    "        \n",
    "    modeldict['cv_val_loss']=modeldict['cv_val_loss']/len(kfolds)\n",
    "    modeldict['cv_val_accuracy']=modeldict['cv_val_accuracy']/len(kfolds)\n",
    "    print(\" trained model: \",modeldict['model'].__name__,\n",
    "              modeldict['kwargs'], \" epochs:\",modeldict['epochs'],\n",
    "             ' val loss=',modeldict['cv_val_loss'],\n",
    "          ' val accuracy=',modeldict['cv_val_accuracy'])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " selected model from loss:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 100 0.04821902513504028 0.5\n",
      " selected model from accuracy:  Net4 {'d1': 25, 'd2': 20, 'num_layers': 2, 'aggr_type': 'mean'}  epochs: 200 0.04856414472063383 0.6547619047619048\n"
     ]
    }
   ],
   "source": [
    "# select the best model (lower validation loss)\n",
    "losses = np.array([ modeldict['cv_val_loss'] for modeldict in model_list])\n",
    "accuracies = np.array([ modeldict['cv_val_accuracy'] for modeldict in model_list])\n",
    "best_model_loss = model_list[np.argmin(losses)]\n",
    "best_model_acc = model_list[np.argmax(accuracies)]\n",
    "print(\" selected model from loss: \",best_model_loss['model'].__name__,best_model_loss['kwargs'],\" epochs:\", \n",
    "      best_model_loss['epochs'], best_model_loss['cv_val_loss'], best_model_loss['cv_val_accuracy'])\n",
    "print(\" selected model from accuracy: \",best_model_acc['model'].__name__,best_model_acc['kwargs'],\" epochs:\", \n",
    "      best_model_acc['epochs'],  best_model_acc['cv_val_loss'], best_model_acc['cv_val_accuracy'])\n",
    "best_model = best_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model_loss\n",
    "best_model = best_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = final_model_train(best_model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 4, 3, 2, 0, 5, 4, 0, 4, 2, 0, 4, 1, 3, 3, 5, 5, 3, 3, 0, 4, 5, 1, 1,\n",
      "        5, 4, 1, 5, 2, 2, 0, 4, 0, 3, 5, 1, 2, 4, 4, 2, 0, 5, 5, 2, 0, 4, 1, 3,\n",
      "        0, 4, 4, 4, 2, 2, 4, 4, 5, 2, 1, 4, 3, 5, 2, 4, 5, 4, 2, 5, 2, 4, 0, 1,\n",
      "        0, 1, 0, 2, 3, 0, 2, 2, 2, 4, 3, 2, 0, 2, 2, 1, 3, 0, 5, 5, 4, 0, 5, 0,\n",
      "        2, 3, 5, 5, 0, 5, 5, 1, 4, 3, 4, 1, 1, 3, 0, 0, 5, 3, 2, 4, 3, 5, 4, 1],\n",
      "       device='cuda:0')\n",
      "tensor([4, 3, 3, 2, 1, 5, 0, 4, 4, 0, 2, 3, 1, 4, 3, 5, 5, 4, 3, 0, 5, 5, 0, 0,\n",
      "        5, 2, 1, 0, 1, 2, 2, 4, 5, 0, 5, 1, 0, 0, 0, 2, 0, 5, 5, 2, 0, 1, 1, 4,\n",
      "        0, 3, 2, 4, 2, 1, 0, 4, 5, 2, 1, 1, 3, 1, 2, 4, 5, 4, 2, 3, 4, 4, 4, 3,\n",
      "        0, 0, 3, 2, 3, 1, 2, 5, 2, 4, 3, 2, 1, 1, 2, 1, 3, 0, 5, 1, 3, 1, 0, 0,\n",
      "        2, 3, 5, 5, 0, 3, 5, 1, 1, 3, 3, 2, 2, 3, 5, 4, 4, 4, 4, 4, 3, 1, 5, 5],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"len(test_dataset): \", len(test_dataset))\n",
    "loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "for batch in loader:\n",
    "    batch = batch.to(device)\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    #_, pred = model(test_dataset).max(dim=1)\n",
    "    _, pred = model(batch).max(dim=1)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    \n",
    "    acc = accuracy(pred, batch)\n",
    "    print('Accuracy: {:.4f}'.format(acc))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "1. implemented K-fold cross validation? (train model and validate k times for the k folds!)\n",
    "2. Fixed - review  whyy predict is NOT DETERMINISTIC -> final retraining after model selection\n",
    "3. implemented dataset balancing: accuracy raised from 38%-40% to 52%\n",
    "\n",
    "\n",
    "### Pending:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement the F1 scores and cross-validate with F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PRE, REC and F1\n",
    "def PRE(measuresdict):\n",
    "    m = measuresdict\n",
    "    measuresdict['PRE'] = float(m['TP'])/float(m['TP']+m['FP'])\n",
    "    \n",
    "def REC(m):\n",
    "    m['REC'] = float(m['TP'])/float(m['FN']+m['TP'])\n",
    "\n",
    "def F1(m):\n",
    "    m['F1']=2.0*(m['PRE']*m['REC'])/(m['PRE']+m['REC'])\n",
    "    \n",
    "def macroAndMicroScores(m):\n",
    "    # average all precisions\n",
    "    # average all recalls\n",
    "    # compute macroF1\n",
    "    macroPRE = 0.0\n",
    "    macroREC = 0.0\n",
    "    num_classes = 0\n",
    "    microPREnumerator = 0.0\n",
    "    microPREdenominator = 0.0\n",
    "    microRECnumerator = 0.0\n",
    "    microRECdenominator = 0.0\n",
    "    for k,v in m.items():\n",
    "        try:\n",
    "            a = int(k)\n",
    "            macroPRE+=m[k]['PRE']\n",
    "            macroREC+=m[k]['REC']\n",
    "            \n",
    "            microPREnumerator+=m[k]['TP']\n",
    "            microPREdenominator+=m[k]['TP']\n",
    "            microPREdenominator+=m[k]['FP']\n",
    "            \n",
    "            microRECnumerator+=m[k]['TP']\n",
    "            microRECdenominator+=m[k]['TP']\n",
    "            microRECdenominator+=m[k]['FN']\n",
    "            \n",
    "            num_classes+=1\n",
    "        except:\n",
    "            # only keys related to classes\n",
    "            # avoid macro and micro keys\n",
    "            pass\n",
    "        \n",
    "    macroPRE = macroPRE/float(num_classes)\n",
    "    macroREC = macroREC/float(num_classes)\n",
    "    macroF1 = 2.0*(macroPRE*macroREC)/(macroPRE+macroREC)\n",
    "    m['macroPRE'] = macroPRE\n",
    "    m['macroREC'] = macroREC\n",
    "    m['macroF1'] = macroF1\n",
    "    \n",
    "    microPRE = microPREnumerator/microPREdenominator\n",
    "    microREC = microRECnumerator/microRECdenominator\n",
    "    microF1 = 2.0*(microPRE*microREC)/(microPRE+microREC)\n",
    "    m['microPRE'] = microPRE\n",
    "    m['microREC'] = microREC\n",
    "    m['microF1'] = microF1\n",
    "    \n",
    "\n",
    "def F1Score(pred, target):\n",
    "    predset = set(pred)\n",
    "    targetset = set(target)\n",
    "    #print(predset)\n",
    "    #print(targetset)\n",
    "    num_classes = max(len(predset),len(targetset))\n",
    "\n",
    "    # for each class save pred_indices, and target_indices\n",
    "    preddict = { i:[] for i in range(num_classes) }\n",
    "    targetdict = { i:[] for i in range(num_classes) }\n",
    "    #print(preddict)\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        preddict[pred[i]].append(i)\n",
    "        targetdict[target[i]].append(i)\n",
    "\n",
    "    #print(\"preddict\", preddict)\n",
    "    #print(\"targetdict\", targetdict)\n",
    "\n",
    "    measures = { \n",
    "        i:{'TP':0, 'TN':0, 'FP':0, \n",
    "           'FN':0, 'PRE':0.0, 'REC': 0.0, 'F1':0.0} \n",
    "        for i in range(num_classes)}\n",
    "    for i in range(num_classes):\n",
    "        for j in range(len(preddict[i])):\n",
    "            if preddict[i][j] in targetdict[i]:\n",
    "                measures[i]['TP']+=1\n",
    "            else:\n",
    "                measures[i]['FP']+=1\n",
    "\n",
    "        for j in range(len(targetdict[i])):\n",
    "            if targetdict[i][j] not in preddict[i]:\n",
    "                measures[i]['FN']+=1\n",
    "\n",
    "        for j in range(len(pred2)):\n",
    "            if pred2[j] not in preddict[i] and pred2[j] not in targetdict[i]:\n",
    "                measures[i]['TN']+=1\n",
    "\n",
    "    #print(\" single measures\",measures)\n",
    "    for k,mdict in measures.items():\n",
    "        try:\n",
    "            PRE(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute PRE on class \",k)\n",
    "            pass\n",
    "        try:\n",
    "            REC(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute REC on class \",k)\n",
    "            pass\n",
    "        try:\n",
    "            F1(mdict)\n",
    "        except:\n",
    "            #print(\"could not compute F1 on class \",k)\n",
    "            pass\n",
    "\n",
    "    macroAndMicroScores(measures)\n",
    "\n",
    "    return measures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# count how many graphs of each class in the dataset\n",
    "def printDatasetBalance(dataset):\n",
    "    num_classes = dataset.num_classes\n",
    "    class_counts = { i:0 for i in range(num_classes)}\n",
    "    #print(class_counts)\n",
    "    for graph in dataset:\n",
    "        class_counts[int(graph.y.item())]+=1\n",
    "    print(class_counts)\n",
    "    \n",
    "    \n",
    "def balancedDatasetSplit_list(dataset, prop):\n",
    "    \n",
    "    dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for graph in dataset:\n",
    "        datasets_byclass[int(graph.y.item())].append(graph)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_dataset.extend(datasets_byclass[c][:limit])\n",
    "        test_dataset.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "    \n",
    "def balancedDatasetSplit_slice(dataset, prop):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    test_lim= int(prop*n)\n",
    "    num_classes = dataset.num_classes\n",
    "    \n",
    "    x=torch.Tensor([True,False,True])==True\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    train_dataset_slice = [False]*n\n",
    "    test_dataset_slice = [False]*n\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "    #print(\"datasets_byclass\", datasets_byclass)\n",
    "    \n",
    "    # for each class repeat balanced split\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(\"datasets_byclass\",datasets_byclass)\n",
    "        \n",
    "    for c in range(num_classes):\n",
    "        nc = len(datasets_byclass[c])\n",
    "        limit = int(prop*nc)\n",
    "        train_list.extend(datasets_byclass[c][:limit])\n",
    "        test_list.extend(datasets_byclass[c][limit:])\n",
    "        \n",
    "    #print(\"train_list\", train_list)\n",
    "    #print(\"test_list\", test_list)\n",
    "\n",
    "        \n",
    "    # now from list of integers(indices) to boolean mask tensor\n",
    "    #for i in range(len(train_list)):\n",
    "    #    real_index = train_list[i]\n",
    "    #    train_dataset_slice[real_index] = True\n",
    "        \n",
    "    #for i in range(len(test_list)):\n",
    "    #    real_index = test_list[i]\n",
    "    #    test_dataset_slice[real_index] = True\n",
    "        \n",
    "    #print(\"train_dataset_slice\", train_dataset_slice)\n",
    "    #print(\"test_dataset_slice\", test_dataset_slice)\n",
    "        \n",
    "    train_dataset = dataset[torch.LongTensor(train_list)]\n",
    "    test_dataset = dataset[torch.LongTensor(test_list)]\n",
    "        \n",
    "    #print(\"train_dataset\", train_dataset)\n",
    "    #print(\"test_dataset\", test_dataset)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def balancedDatasetKfoldSplit_slice(dataset,k):\n",
    "    \n",
    "    #dataset = dataset.shuffle()\n",
    "    n = len(dataset)\n",
    "    \n",
    "    foldsize = int(n/k)\n",
    "    num_classes = dataset.num_classes\n",
    "    num_items_x_class = int(foldsize/num_classes)\n",
    "    \n",
    "    # list of items for each class\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    datasets_byclass = {i:[] for i in range(num_classes)}\n",
    "    for i in range(n):\n",
    "        graph = dataset[i]\n",
    "        datasets_byclass[int(graph.y.item())].append(i)\n",
    "\n",
    "    #print(datasets_byclass)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append([])\n",
    "        for c in range(num_classes):\n",
    "            for j in range(num_items_x_class):\n",
    "                index = datasets_byclass[c].pop()\n",
    "                folds[i].append(index)\n",
    "        \n",
    "    # returns a list of list of indices\n",
    "    return folds\n",
    "\n",
    "def kFolding(train_dataset, k):\n",
    "    n = len(train_dataset)\n",
    "    fold_size = int(n/k)\n",
    "    \n",
    "    # build folds\n",
    "    #folds = []\n",
    "    #for i in range(k):\n",
    "    #    i1 = i*fold_size\n",
    "    #    i2 = i1+fold_size\n",
    "    #    folds.append((i1,i2))\n",
    "    #print(folds)\n",
    "    \n",
    "    # build train-val sets\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        preval_index = (0,i*fold_size)\n",
    "        val_index = (i*fold_size,i*fold_size+fold_size)\n",
    "        postval_index = (i*fold_size+fold_size,n)\n",
    "        train_sets.append((preval_index, val_index, postval_index))\n",
    "        \n",
    "    #print(train_sets)\n",
    "    return train_sets\n",
    "\n",
    "def kFolding2(train_dataset, k):\n",
    "\n",
    "    #print(\" train_dataset len:\", len(train_dataset))\n",
    "    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)\n",
    "    train_sets =[]\n",
    "    for i in range(k):\n",
    "        # each train_set must have a torch.LongTensor for train indices\n",
    "        # and a torch.LongTensor for val indices\n",
    "        val_merge = folds[i]\n",
    "        train_merge = [] \n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train_merge.extend(folds[j])\n",
    "        train_sets.append((torch.LongTensor(train_merge), torch.LongTensor(val_merge)))\n",
    "    \n",
    "    return train_sets\n",
    "    \n",
    "\n",
    "\n",
    "def accuracy(pred, batch):\n",
    "    correct = pred.eq(batch.y).sum().item()\n",
    "    #acc = correct / test_dataset.sum().item()\n",
    "    acc = correct / batch.num_graphs\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, train_loss_history):\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    total_num_graphs = 0\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y\n",
    "        loss = F.nll_loss(out, target)\n",
    "        loss_train +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_train = loss_train /total_num_graphs\n",
    "    train_loss_history.append(loss_train.item()) \n",
    "    \n",
    "def val_loss_model(model, loader, optimizer, val_history):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    total_num_graphs = 0\n",
    "    total_pred = []\n",
    "    total_acc = []\n",
    "    total_gt = []\n",
    "    \n",
    "    for batch in loader_val:\n",
    "        data = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        total_pred.extend(pred.flatten().tolist())\n",
    "        total_gt.extend(batch.y.flatten().tolist())\n",
    "        \n",
    "        _, predacc = pred.max(dim=1)\n",
    "        total_acc.extend(predacc.flatten().tolist())\n",
    "        \n",
    "        target = data.y\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        loss_val += loss\n",
    "        total_num_graphs += data.num_graphs\n",
    "        \n",
    "    loss_val = loss_val / total_num_graphs\n",
    "    val_history['loss'].append(loss_val.item())\n",
    "    \n",
    "    # accuracy needs correction\n",
    "    val_history['accuracy'].append(accuracy(predacc, batch))\n",
    "    \n",
    "    # compute F1 scores\n",
    "    #pred2 = pred.to('cpu')\n",
    "    #pred2 = pred2.flatten().tolist()\n",
    "    #target = batch.y.to('cpu')\n",
    "    #target = target.flatten().tolist()\n",
    "    \n",
    "    #print(\"total_acc\",total_acc)\n",
    "    #print(\"total_gt\",total_gt)\n",
    "    measures = F1Score(total_acc, total_gt)\n",
    "    val_history['microF1'].append(measures['microF1'])\n",
    "    val_history['macroF1'].append(measures['macroF1'])\n",
    "    \n",
    "    \n",
    "def final_model_train(modeldict, train_dataset):\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history=[]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Retrain the best model\n",
    "def final_model_train(modeldict, train_dataset):\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history=[]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, loader, optimizer, train_loss_history)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 600  k folds= 3  test_lim: 480\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "{0: 80, 1: 80, 2: 80, 3: 80, 4: 80, 5: 80}\n",
      "{0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20}\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\" n:\",n,\" k folds=\",k,\" test_lim:\",test_lim)\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kfolds = kFolding2(train_dataset,k)\n",
    "\n",
    "for modeldict in model_list:\n",
    "\n",
    "    epochs = modeldict['epochs']\n",
    "    modelclass = modeldict['model']\n",
    "    kwargs = modeldict['kwargs']\n",
    "    model = modelclass(**kwargs)\n",
    "    model = model.to(device)\n",
    "    train_loss_history = []\n",
    "    val_history = {'loss':[], 'accuracy':[], 'microF1':[],'macroF1':[]}\n",
    "    modeldict['cv_val_loss']=0.0\n",
    "    modeldict['cv_val_accuracy']=0.0\n",
    "    modeldict['cv_val_microF1'] =0.0\n",
    "    modeldict['cv_val_macroF1'] =0.0\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    for kfold in kfolds:\n",
    "        \n",
    "        train = train_dataset[kfold[0]]\n",
    "        val = train_dataset[kfold[1]]\n",
    "        loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "        loader_val = DataLoader(val, batch_size=32, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            train_model(model, loader, optimizer, train_loss_history)\n",
    "            val_loss_model(model, loader_val, optimizer, val_history)\n",
    "\n",
    "        # save results\n",
    "        modeldict['train_loss_history']=train_loss_history\n",
    "        modeldict['val_loss_history']=val_history['loss']\n",
    "        modeldict['val_accuracy_history']=val_history['accuracy']\n",
    "        modeldict['val_loss']=val_history['loss'][-1]\n",
    "        modeldict['accuracy']=val_history['accuracy'][-1]\n",
    "        modeldict['microF1']=val_history['microF1'][-1]\n",
    "        modeldict['macroF1']=val_history['macroF1'][-1]\n",
    "        \n",
    "        modeldict['cv_val_loss']+=modeldict['val_loss']\n",
    "        modeldict['cv_val_accuracy']+=modeldict['accuracy']\n",
    "        modeldict['cv_val_microF1']+=modeldict['microF1']\n",
    "        modeldict['cv_val_macroF1']+=modeldict['macroF1']\n",
    "        \n",
    "    modeldict['cv_val_loss']=modeldict['cv_val_loss']/len(kfolds)\n",
    "    modeldict['cv_val_accuracy']=modeldict['cv_val_accuracy']/len(kfolds)\n",
    "    modeldict['cv_val_microF1']=modeldict['cv_val_microF1']/len(kfolds)\n",
    "    modeldict['cv_val_macroF1']=modeldict['cv_val_macroF1']/len(kfolds)\n",
    "    print(\" trained model: \",modeldict['model'].__name__,\n",
    "              modeldict['kwargs'], \" epochs:\",modeldict['epochs'],\n",
    "             ' val loss=',modeldict['cv_val_loss'],\n",
    "          ' val accuracy=',modeldict['cv_val_accuracy'],\n",
    "         ' val microF1=',modeldict['cv_val_microF1'],\n",
    "          ' val macroF1=',modeldict['cv_val_macroF1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best model (lower validation loss)\n",
    "losses = np.array([ modeldict['cv_val_loss'] for modeldict in model_list])\n",
    "accuracies = np.array([ modeldict['cv_val_accuracy'] for modeldict in model_list])\n",
    "microF1 = np.array([ modeldict['cv_val_microF1'] for modeldict in model_list])\n",
    "macroF1 = np.array([ modeldict['cv_val_macroF1'] for modeldict in model_list])\n",
    "best_model_loss = model_list[np.argmin(losses)]\n",
    "best_model_acc = model_list[np.argmax(accuracies)]\n",
    "best_model_microF1 = model_list[np.argmin(microF1)]\n",
    "best_model_macroF1 = model_list[np.argmax(macroF1)]\n",
    "\n",
    "print(\" selected model from loss: \",best_model_loss['model'].__name__,\n",
    "      best_model_loss['kwargs'],\" epochs:\", best_model_loss['epochs'], \n",
    "      best_model_loss['cv_val_loss'], best_model_loss['cv_val_accuracy'], \n",
    "      best_model_loss['cv_val_microF1'], best_model_loss['cv_val_macroF1'])\n",
    "print(\" selected model from accuracy: \",best_model_acc['model'].__name__,\n",
    "      best_model_acc['kwargs'],\" epochs:\",best_model_acc['epochs'],  \n",
    "      best_model_acc['cv_val_loss'], best_model_acc['cv_val_accuracy'], \n",
    "      best_model_loss['cv_val_microF1'], best_model_loss['cv_val_macroF1'])\n",
    "print(\" selected model from microF1: \",best_model_microF1['model'].__name__,best_model_microF1['kwargs'],\n",
    "      \" epochs:\", best_model_microF1['epochs'],  \n",
    "      best_model_microF1['cv_val_loss'], best_model_microF1['cv_val_accuracy'], \n",
    "      best_model_microF1['cv_val_microF1'], best_model_microF1['cv_val_macroF1'])\n",
    "\n",
    "print(\" selected model from macroF1: \",best_model_macroF1['model'].__name__,best_model_macroF1['kwargs'],\n",
    "      \" epochs:\", best_model_macroF1['epochs'],  \n",
    "      best_model_macroF1['cv_val_loss'], best_model_macroF1['cv_val_accuracy'], \n",
    "      best_model_macroF1['cv_val_microF1'], best_model_macroF1['cv_val_macroF1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = best_model_loss\n",
    "#best_model = best_model_acc\n",
    "best_model = best_model_microF1\n",
    "#best_model = best_model_macroF1\n",
    "model = final_model_train(best_model, train_dataset)\n",
    "\n",
    "model.eval()\n",
    "print(\"len(test_dataset): \", len(test_dataset))\n",
    "loader = DataLoader(test_dataset, batch_size= len(test_dataset), shuffle=True)\n",
    "for batch in loader:\n",
    "    batch = batch.to(device)\n",
    "    print(\"num graphs: \", batch.num_graphs)\n",
    "    #_, pred = model(test_dataset).max(dim=1)\n",
    "    _, pred = model(batch).max(dim=1)\n",
    "    print(pred)\n",
    "    print(batch.y)\n",
    "    \n",
    "    acc = accuracy(pred, batch)    \n",
    "    pred2 = pred.to('cpu')\n",
    "    pred2 = pred2.flatten().tolist()\n",
    "    target = batch.y.to('cpu')\n",
    "    target = target.flatten().tolist()\n",
    "    measures = F1Score(pred2, target)\n",
    "    print('Accuracy: {:.4f}'.format(acc),\" macroF1:\",measures['macroF1'], \" microF1:\", measures['microF1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Load a dataset and use MetaLayer to classify it (train-validation-test setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
