{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This notebook will experiment with:\n",
    "- train & compare differnt GNN models for graph classificaiton in common benchmarks (PPI, Proteins, ENZYMES,..)\n",
    "- compare results to publication results\n",
    "\n",
    "Most of the experiments will be done in PyTorch/PyTorch Geometric, but some models are implemented in Tensor Flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import MessagePassing\n",
    "#from torch_geometric.nn.conv.gated_graph_conv import GatedGraphConv\n",
    "from torch_geometric.nn.glob.glob import global_mean_pool, global_add_pool\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "from TFM_graph_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net1, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net2, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Net3(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20, d3=10,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net3, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, d3)\n",
    "        self.dense2_bn = nn.BatchNorm1d(d3)\n",
    "        self.fc3 = nn.Linear(d3, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = F.relu(self.dense2_bn(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class Net4(torch.nn.Module):\n",
    "    def __init__(self, d1=50,d2=20,num_classes=6, num_layers=2, aggr_type='mean'):\n",
    "        super(Net4, self).__init__()\n",
    "        self.ggnn = GatedGraphConv(out_channels=d1, num_layers=num_layers,aggr=aggr_type, bias=True)\n",
    "        self.fc1 = nn.Linear(d1, d2)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d2)\n",
    "        self.fc2 = nn.Linear(d2, num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_vector = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.ggnn(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, batch_vector) # this makes the output to be graph level?\n",
    "        #x = self.fc1(x)\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = self.pool1(x, batch )\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        #x = torch.argmax(x, dim=1)  # we output softmax to use the nll_loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class META1(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META1, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "        # version using only u\n",
    "        x = F.relu(self.dense1_bn(self.fc1(u2)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    \n",
    "class META2(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META2, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "        \n",
    "        # version using only x \n",
    "        x = self.global_pool(x2,batch) # separate by graph level\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class META3(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META3, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "\n",
    "        # version using x and  u\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        ubatch = list(set([ elem.item() for elem in batch]))\n",
    "        #print(ubatch)\n",
    "        x = self.global_pool(x, torch.cat([batch,torch.LongTensor( ubatch).to(device) ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class META4(torch.nn.Module):\n",
    "    def __init__(self, d1=3, d2=50, d3=15, d4 =15,d5=10,num_classes=6):\n",
    "        super(META4, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*3, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1*6, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(16, d2), ReLU(), Lin(d2, d3))\n",
    "        \n",
    "        self.fc1 = nn.Linear(d4, d5)\n",
    "        self.dense1_bn = nn.BatchNorm1d(d5)\n",
    "        self.fc2 = nn.Linear(d5, num_classes)\n",
    "        self.dense2_bn = nn.BatchNorm1d(num_classes)\n",
    "        self.global_pool = global_mean_pool\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            #print(\"edge_model\")\n",
    "            #print(source.size())\n",
    "            #print(target.size())\n",
    "            #print(edge_attr.size())\n",
    "            out = torch.cat([source, target, edge_attr], dim=1)\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "\n",
    "            \n",
    "            row, col = edge_index\n",
    "            \n",
    "            #print(\"node_model\")\n",
    "            #print(row.size())\n",
    "            #print(col.size())\n",
    "            #print(x[col].size())\n",
    "            #print(edge_attr.size())\n",
    "            \n",
    "            out = torch.cat([x[col], edge_attr], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            \n",
    "            #print(\"global_Model\")\n",
    "            #print(\"u.size():\")\n",
    "            #print(u.size())\n",
    "            #print(\"scatter_mean(x,batch,..):\")\n",
    "            #smean = scatter_mean(x, batch, dim=0)\n",
    "            #print(smean.size())\n",
    "            \n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            #print(\"out.size():\")\n",
    "            #print(out.size())\n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch        \n",
    "        \n",
    "        # output of meta is x,edge_attr, u\n",
    "        x2, edge_attr2, u2 =  self.op(x, edge_index, edge_attr, u, batch)\n",
    "        \n",
    "        # idea1 is to cat x2, edge_attr2 and u2?\n",
    "        # idea2 is to update edge_attr and u...\n",
    "        data.x = x2\n",
    "        data.edge_attr = edge_attr2\n",
    "        data.u = u2\n",
    "\n",
    "    \n",
    "        # version using x and  u and edge_attr\n",
    "        x = F.relu(torch.cat([x2,u2], dim=0))\n",
    "        #x = x2\n",
    "        x = F.dropout(x, training=self.training) # until here the output is for each node\n",
    "        \n",
    "        x = self.global_pool(x, torch.cat([batch, ],dim=0)) # this makes the output to be graph level?\n",
    "        x = F.relu(self.dense1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class META5(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Not using edge attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, d1=3, d2=50, d3=15):\n",
    "        super(META5, self).__init__()\n",
    "\n",
    "        self.edge_mlp = Seq(Lin(d1*2, d2), ReLU(), Lin(d2, d3))\n",
    "        self.node_mlp = Seq(Lin(d1, d2), ReLU(), Lin(d2, d3))\n",
    "        self.global_mlp = Seq(Lin(2, d2), ReLU(), Lin(d2, d3))\n",
    "\n",
    "        def edge_model(source, target, edge_attr, u):\n",
    "            # source, target: [E, F_x], where E is the number of edges.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u], where B is the number of graphs.\n",
    "            out = torch.cat([source, target], dim=1)\n",
    "            #print(\"edge_model\")\n",
    "            #print(out.size())\n",
    "            return self.edge_mlp(out)\n",
    "\n",
    "        def node_model(x, edge_index, edge_attr, u):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            row, col = edge_index\n",
    "            out = torch.cat([x[col]], dim=1)\n",
    "            out = self.node_mlp(out)\n",
    "            return scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "\n",
    "        def global_model(x, edge_index, edge_attr, u, batch):\n",
    "            # x: [N, F_x], where N is the number of nodes.\n",
    "            # edge_index: [2, E] with max entry N - 1.\n",
    "            # edge_attr: [E, F_e]\n",
    "            # u: [B, F_u]\n",
    "            # batch: [N] with max entry B - 1.\n",
    "            out = torch.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "            \n",
    "            return self.global_mlp(out)\n",
    "\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        #print(\"Forward: \")\n",
    "        #print(x.size())\n",
    "        return self.op(x, edge_index, edge_attr, u, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models for hyperparameter search\n",
    "model_list =[\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 50,'d2': 20,'num_layers':2, 'aggr_type':'add'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    \n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 20,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 100,\n",
    "    'model': Net1,\n",
    "    'kwargs':{'d1': 100,'d2': 50,'num_layers':2, 'aggr_type':'mean'}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "]\n",
    "\n",
    "model_list2 = []\n",
    "for modelclass in [Net1, Net2, Net3,Net4]:\n",
    "    for d1 in [25,50,100,200]:\n",
    "        for d2 in [20,50]:\n",
    "            for aggr_type in ['mean','add']:\n",
    "                for epochs in [10,20,300]:\n",
    "                    model_list2.append(\n",
    "                        {\n",
    "                        'model': modelclass,\n",
    "                        'epochs': epochs,\n",
    "                        'kwargs':{'d1': d1,'d2': d2,'num_layers':2, \n",
    "                                  'aggr_type':aggr_type}, \n",
    "                        'learning_rate': 0.01, 'weight_decay':5e-4, \n",
    "                        'batch_size': 32},\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "model_list3 =[\n",
    "    {'epochs': 200,\n",
    "    'model': META1,\n",
    "    'kwargs':{'d1': 3,'d2': 20, 'd3': 15, 'd4': 15, 'd5':10}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': META2,\n",
    "    'kwargs':{'d1': 3,'d2': 20, 'd3': 15, 'd4': 15, 'd5':10}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    {'epochs': 200,\n",
    "    'model': META3,\n",
    "    'kwargs':{'d1': 3,'d2': 20, 'd3': 15, 'd4': 15, 'd5':10}, \n",
    "    'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "    #{'epochs': 200,\n",
    "    #'model': META4,\n",
    "    #'kwargs':{'d1': 3,'d2': 20, 'd3': 15}, \n",
    "    #'learning_rate': 0.01, 'weight_decay':5e-4, 'batch_size': 32},\n",
    "]\n",
    "    \n",
    "    \n",
    "model_list = model_list2\n",
    "model_list = model_list[:2]\n",
    "model_list = model_list3[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n: 600  k folds= 3\n",
      "Datasets balancing: \n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "{0: 80, 1: 80, 2: 80, 3: 80, 4: 80, 5: 80}\n",
      "{0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "k = 3\n",
    "n = len(dataset)\n",
    "print(\" n:\",n,\" k folds=\",k)\n",
    "train_dataset, test_dataset = balancedDatasetSplit_slice(dataset, prop=0.8)\n",
    "print(\"Datasets balancing: \")\n",
    "printDatasetBalance(dataset )\n",
    "printDatasetBalance(train_dataset )\n",
    "printDatasetBalance(test_dataset )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'apply', 'cat_dim', 'clone', 'contains_isolated_nodes', 'contains_self_loops', 'contiguous', 'edge_attr', 'edge_index', 'from_dict', 'is_coalesced', 'is_directed', 'is_undirected', 'keys', 'num_edges', 'num_features', 'num_nodes', 'pos', 'to', 'x', 'y']\n",
      "3\n",
      "110\n",
      "\n",
      "['__call__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'apply', 'batch', 'cat_dim', 'clone', 'contains_isolated_nodes', 'contains_self_loops', 'contiguous', 'cumsum', 'edge_attr', 'edge_index', 'from_data_list', 'from_dict', 'is_coalesced', 'is_directed', 'is_undirected', 'keys', 'num_edges', 'num_features', 'num_graphs', 'num_nodes', 'pos', 'to', 'x', 'y']\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "None\n",
      "tensor([[   0,    0,    1,  ..., 1073, 1073, 1073],\n",
      "        [   1,    2,    0,  ..., 1061, 1067, 1072]])\n",
      "tensor([ 0,  0,  0,  ..., 31, 31, 31])\n",
      "tensor([2, 3, 5, 2, 4, 2, 1, 4, 3, 4, 0, 1, 1, 0, 1, 2, 2, 0, 0, 3, 3, 1, 2, 3,\n",
      "        2, 3, 0, 4, 2, 4, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# get  edge_index, edge_attr and global attributes..\n",
    "print(dir(dataset[1]))\n",
    "print(dataset[1].num_features)\n",
    "print(dataset[1].num_edges)\n",
    "print()\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for data in loader:\n",
    "    print(dir(data))\n",
    "    \n",
    "    print(data.x)\n",
    "    print(data.edge_attr)\n",
    "    print(data.edge_index)\n",
    "    print(data.batch)\n",
    "    print(data.y)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trained model:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 200  val loss= 0.05202676480015119  val accuracy= 0.4047619047619048  val microF1= 0.3376068376068376  val macroF1= 0.3383961681643695\n",
      "\n",
      " selected model from loss:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 200 0.05202676480015119 0.4047619047619048 0.3376068376068376 0.3383961681643695\n",
      " selected model from accuracy:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 200 0.05202676480015119 0.4047619047619048 0.3376068376068376 0.3383961681643695\n",
      " selected model from microF1:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 200 0.05202676480015119 0.4047619047619048 0.3376068376068376 0.3383961681643695\n",
      " selected model from macroF1:  META3 {'d1': 3, 'd2': 20, 'd3': 15, 'd4': 15, 'd5': 10}  epochs: 200 0.05202676480015119 0.4047619047619048 0.3376068376068376 0.3383961681643695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "modelsdict = modelSelection(model_list,k, train_dataset)\n",
    "reportModelSelectionResult(modelsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 5, 1, 5, 2, 5, 5, 1, 5, 5, 2, 5, 5, 5, 5, 1, 1, 2, 5, 2, 1, 0, 2, 5,\n",
      "        5, 4, 2, 2, 2, 2, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 2, 4, 0, 5, 5, 0, 2, 0,\n",
      "        0, 5, 2, 4, 5, 4, 2, 5, 5, 0, 4, 5, 2, 4, 4, 5, 5, 4, 1, 5, 0, 2, 2, 4,\n",
      "        5, 1, 5, 1, 5, 0, 2, 5, 2, 5, 4, 4, 5, 1, 1, 5, 1, 0, 5, 5, 4, 2, 4, 5,\n",
      "        5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 2, 5, 5, 4, 5, 2, 5, 5, 4, 5, 5, 1, 5, 5],\n",
      "       device='cuda:0')\n",
      "tensor([3, 0, 1, 4, 4, 4, 3, 1, 3, 3, 4, 4, 2, 1, 5, 1, 1, 2, 3, 2, 5, 0, 5, 3,\n",
      "        0, 0, 2, 2, 2, 2, 1, 2, 1, 4, 5, 5, 1, 1, 2, 5, 2, 4, 0, 0, 2, 5, 2, 1,\n",
      "        3, 0, 2, 0, 5, 4, 0, 2, 5, 5, 4, 4, 1, 0, 3, 0, 4, 4, 5, 5, 2, 3, 2, 4,\n",
      "        0, 5, 0, 3, 5, 1, 0, 1, 2, 4, 3, 1, 4, 3, 4, 5, 0, 1, 5, 1, 3, 2, 3, 1,\n",
      "        3, 2, 0, 4, 5, 0, 3, 5, 1, 3, 4, 1, 0, 0, 5, 3, 4, 3, 0, 3, 1, 2, 5, 4],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.3167  macroF1: 0.29489583333333336  microF1: 0.31666666666666665\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['loss'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'loss']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([5, 5, 5, 5, 5, 2, 5, 5, 5, 2, 5, 3, 2, 5, 2, 4, 5, 5, 5, 5, 2, 5, 5, 2,\n",
      "        2, 2, 5, 2, 2, 5, 5, 2, 5, 2, 5, 5, 2, 5, 5, 5, 5, 2, 2, 5, 2, 2, 5, 5,\n",
      "        2, 2, 2, 3, 5, 5, 2, 2, 5, 3, 5, 5, 5, 5, 2, 2, 1, 5, 5, 3, 4, 5, 1, 2,\n",
      "        2, 3, 2, 2, 5, 5, 3, 5, 2, 2, 2, 5, 3, 2, 2, 2, 5, 5, 5, 1, 2, 2, 2, 5,\n",
      "        5, 5, 5, 2, 2, 3, 1, 1, 2, 5, 5, 5, 5, 5, 5, 2, 2, 3, 5, 2, 2, 2, 4, 5],\n",
      "       device='cuda:0')\n",
      "tensor([4, 4, 1, 1, 3, 1, 4, 1, 0, 1, 1, 1, 5, 5, 0, 4, 3, 3, 5, 5, 0, 3, 5, 4,\n",
      "        3, 5, 1, 4, 2, 5, 4, 0, 0, 2, 0, 4, 0, 2, 1, 3, 0, 3, 3, 0, 4, 4, 3, 1,\n",
      "        0, 2, 2, 5, 3, 5, 2, 3, 5, 5, 1, 4, 2, 0, 2, 2, 3, 3, 5, 2, 1, 1, 3, 0,\n",
      "        0, 4, 2, 3, 5, 2, 1, 4, 2, 0, 2, 0, 5, 1, 3, 4, 5, 2, 3, 1, 3, 0, 2, 4,\n",
      "        2, 5, 4, 3, 2, 5, 1, 3, 4, 0, 5, 1, 0, 5, 0, 1, 4, 1, 5, 4, 2, 2, 0, 4],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2583  macroF1: 0.23426782859330317  microF1: 0.25833333333333336\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['accuracy'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'accuracy']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([2, 1, 1, 4, 5, 1, 0, 2, 4, 0, 5, 1, 5, 2, 1, 2, 0, 0, 2, 2, 5, 5, 2, 0,\n",
      "        2, 2, 2, 5, 1, 4, 0, 4, 1, 4, 2, 4, 4, 0, 2, 5, 2, 5, 5, 5, 1, 2, 2, 2,\n",
      "        1, 5, 2, 1, 4, 1, 2, 0, 2, 5, 1, 5, 2, 1, 2, 2, 1, 5, 2, 5, 5, 4, 1, 2,\n",
      "        4, 4, 1, 1, 2, 2, 0, 2, 0, 4, 2, 1, 2, 3, 4, 2, 1, 5, 1, 2, 3, 1, 2, 2,\n",
      "        0, 1, 5, 1, 4, 1, 0, 5, 1, 1, 1, 0, 4, 2, 2, 2, 2, 5, 5, 4, 4, 5, 5, 2],\n",
      "       device='cuda:0')\n",
      "tensor([2, 3, 2, 0, 4, 4, 5, 5, 3, 5, 5, 5, 5, 0, 4, 2, 4, 0, 3, 2, 2, 3, 0, 1,\n",
      "        1, 2, 2, 5, 5, 4, 5, 5, 3, 4, 2, 0, 4, 1, 4, 0, 4, 4, 4, 5, 5, 1, 0, 2,\n",
      "        4, 3, 2, 1, 1, 1, 3, 0, 4, 5, 0, 2, 4, 1, 5, 2, 4, 5, 3, 5, 4, 3, 1, 1,\n",
      "        0, 3, 3, 1, 2, 1, 5, 2, 2, 0, 3, 3, 1, 3, 0, 0, 1, 0, 3, 5, 3, 0, 2, 4,\n",
      "        1, 3, 2, 3, 1, 1, 0, 2, 5, 1, 0, 1, 0, 0, 2, 3, 1, 4, 5, 4, 0, 3, 4, 2],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.3250  macroF1: 0.36379260753217935  microF1: 0.325\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['microF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'microF1']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(test_dataset):  120\n",
      "num graphs:  120\n",
      "tensor([4, 1, 2, 3, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 1, 2, 1, 4, 0, 2, 1, 1, 2, 2,\n",
      "        1, 2, 4, 1, 2, 0, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 2, 2, 2, 3,\n",
      "        2, 1, 2, 0, 2, 1, 3, 1, 1, 4, 1, 4, 2, 3, 4, 2, 2, 2, 2, 2, 4, 2, 0, 1,\n",
      "        4, 5, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 2, 3, 1, 2, 2, 2, 2, 2, 5, 1, 0, 1,\n",
      "        0, 2, 1, 1, 1, 3, 0, 3, 1, 1, 1, 3, 2, 0, 2, 1, 2, 2, 1, 4, 1, 1, 1, 2],\n",
      "       device='cuda:0')\n",
      "tensor([4, 4, 1, 3, 3, 1, 1, 3, 2, 2, 1, 0, 5, 0, 4, 2, 0, 1, 0, 3, 3, 1, 1, 1,\n",
      "        3, 3, 1, 0, 2, 2, 0, 4, 5, 5, 1, 5, 2, 1, 0, 2, 5, 1, 3, 2, 2, 0, 3, 3,\n",
      "        3, 3, 4, 0, 0, 5, 0, 2, 4, 1, 3, 2, 0, 0, 3, 2, 2, 4, 3, 2, 0, 2, 4, 5,\n",
      "        5, 4, 1, 5, 1, 3, 4, 0, 3, 0, 4, 1, 4, 4, 0, 4, 2, 5, 2, 5, 5, 0, 5, 3,\n",
      "        5, 2, 5, 2, 4, 0, 1, 4, 3, 4, 5, 4, 3, 5, 0, 1, 5, 2, 4, 5, 1, 5, 1, 4],\n",
      "       device='cuda:0')\n",
      "Accuracy: 0.2583  macroF1: 0.266778352575742  microF1: 0.25833333333333336\n"
     ]
    }
   ],
   "source": [
    "bmodel = final_model_train(modelsdict['best_models']['macroF1'], train_dataset)\n",
    "testresult = testModel(bmodel, test_dataset)\n",
    "modelsdict['testing'][bmodel.__class__.__name__+'macroF1']=testresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macroF1</th>\n",
       "      <th>microF1</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.294896</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>Net1loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.234268</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>Net1accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.363793</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>Net1microF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.266778</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>Net1macroF1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   macroF1   microF1          name\n",
       "0  0.316667  0.294896  0.316667      Net1loss\n",
       "1  0.258333  0.234268  0.258333  Net1accuracy\n",
       "2  0.325000  0.363793  0.325000   Net1microF1\n",
       "3  0.258333  0.266778  0.258333   Net1macroF1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reportAllTest(modelsdict)\n",
    "saveResults(modelsdict)\n",
    "# review microF1 & macroF1\n",
    "# review Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. encapsulate all training, model selection,.. everything\n",
    "2. present results with Pandas tables, and histograms\n",
    "3. save models and results to disk, and load them later for testing\n",
    "4. transform into a python module or package\n",
    "\n",
    "### Pending:\n",
    "\n",
    "- prepare another notebook using the python module (prepare local and on collab)\n",
    "- test other GNN layers: GAT, GCN, GraphSAGE, Metalayer\n",
    "- do a good HP search\n",
    "- look for published architectures?\n",
    "- compare with published benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
