{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "\n",
    "from torch_geometric.nn.inits import uniform\n",
    "#from ..inits import uniform\n",
    "\n",
    "class LinkSAGEConv(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "       This work, LinkGraphSAGE PyTorch Geometric implementation, is based\n",
    "       on the previous work GraphSAGE (Hamilton et. al) and PyTorch Geometric.\n",
    "    \n",
    "    \n",
    "    The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{\\hat{x}}_i &= \\mathbf{\\Theta} \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i) \\cup \\{ i \\}}}(\\mathbf{x}_j)\n",
    "\n",
    "        \\mathbf{x}^{\\prime}_i &= \\frac{\\mathbf{\\hat{x}}_i}\n",
    "        {\\| \\mathbf{\\hat{x}}_i \\|_2}.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to :obj:`False`, output features\n",
    "            will not be :math:`\\ell^2`-normalized.\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 sampling_size=4,\n",
    "                 k=1,\n",
    "                 normalize=True, \n",
    "                 bias=True):\n",
    "        super(LinkSAGEConv, self).__init__()\n",
    "\n",
    "        self.sampling_size = sampling_size\n",
    "        self.K = k\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        #self.weight = Parameter(torch.Tensor(self.sampling_size, self.out_channels))\n",
    "        self.weight = Parameter(torch.Tensor(self.in_channels, self.in_channels))\n",
    "\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(self.in_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.weight.size(0)\n",
    "        uniform(size, self.weight)\n",
    "        uniform(size, self.bias)\n",
    "\n",
    "    def forward(self, x, edge_neighbors):\n",
    "        \"\"\"\n",
    "            - where's the sampling???\n",
    "            - why ther's no normalization? (the message part) -> it's done here\n",
    "            - the scatter_mean is also done..\n",
    "            CONCLUSION: it's a more compact way to write it.\n",
    "\n",
    "            BUT NO SAMPLING WHICH IS STRANGE\n",
    "            NO K depth sampling! \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # edge_neighbor extraction oand sampling could be repeated inside the K loop..\n",
    "        edge_neighbors, _ = remove_self_loops(edge_neighbors)\n",
    "        #edge_neighbors = add_self_loops(edge_neighbors, num_nodes=x.size(0))\n",
    "\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        row, col = edge_neighbors\n",
    "\n",
    "        # sampling 1 level aggregation\n",
    "        srow, scol = self.samplingNeighbors2(edge_neighbors, self.sampling_size)\n",
    "        #srow, scol = self.samplingNeighbors(edge_neighbors, 4)\n",
    "\n",
    "        for k in range(self.K):\n",
    "\n",
    "            #print(\"scol\",scol)\n",
    "            #print(\"sampled x, x[scol]\",x[scol])\n",
    "            out = scatter_mean(x[scol], srow, dim=0, dim_size=x.size(0))\n",
    "            #print(\"scatter_mean result: \",out)\n",
    "            #print(\"weight.transpose()\", torch.transpose(self.weight,0,1))\n",
    "            #print(\"weight\", self.weight)\n",
    "            #out = torch.matmul(torch.transpose(out,0,1), self.weight)\n",
    "            \n",
    "            # out: 7x1      -> 7x1\n",
    "            # weight: 7x15  -> 15x7\n",
    "            #out = torch.matmul( out,self.weight ) \n",
    "            #out = torch.matmul( torch.transpose(out,0,1),self.weight ) \n",
    "            #out = torch.matmul( self.weight,out ) \n",
    "            #out = torch.matmul( torch.transpose(self.weight,0,1),out ) \n",
    "            #print(\"weight first col: \",self.weight[:,0].size())\n",
    "            #print(\"out first col: \",out[:,0].size())\n",
    "            out = torch.matmul(self.weight, out)\n",
    "            #print(out.size())\n",
    "            #print(out)\n",
    "            #print(x.size())\n",
    "            \n",
    "            # ouptu must be 7x1? 7xnum-filters\n",
    "            \n",
    "            if self.bias is not None:\n",
    "                out = out + self.bias\n",
    "\n",
    "            if self.normalize:\n",
    "                out = F.normalize(out, p=2, dim=-1)\n",
    "            \n",
    "            #print(\"out\",out)\n",
    "            x = out\n",
    "            \n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "    def samplingNeighbors2(self, neighbors,m):\n",
    "        \n",
    "        result = torch.LongTensor([]).to(neighbors.device)\n",
    "        #print(\"neighbors\",neighbors)\n",
    "        \n",
    "        # iterate over tensor and get indices\n",
    "        \n",
    "        i1=0\n",
    "        i2=i1\n",
    "        i = 0\n",
    "        for elem in neighbors[0]:\n",
    "            #print(\"i1: \",i1,\"  i2: \",i2)\n",
    "            #print( neighbors[0, i], int(elem.item()), neighbors[0 , i] != int(elem.item()))\n",
    "            if neighbors[0 , i1] != int(elem.item()):\n",
    "                if i1!=-1 and i2!=-1:\n",
    "                    # close previous edge list\n",
    "                    #subvect = neighbors[0][i1:i2]\n",
    "                \n",
    "                    # shuffle\n",
    "                    #subvect = subvect[torch.randperm(subvect.size()[0])]\n",
    "                    subvect = torch.LongTensor(np.arange(i1,i2)).to(neighbors.device)\n",
    "                    #print(np.arange(i1,i2))\n",
    "                    #print(subvect)\n",
    "                \n",
    "                \n",
    "                    # trim by slicing\n",
    "                    subvect = subvect[torch.randperm(subvect.size()[0])]\n",
    "                    subvect = subvect[:m]\n",
    "                    #print(\"m\",m)\n",
    "                    #print(\"subvect.size()\", subvect.size())\n",
    "                    #print(\"torch.randperm(subvect.size()[0])\",torch.randperm(subvect.size()[0]))\n",
    "                    \n",
    "                    #print(\"result before cat\",result)\n",
    "                    #print(type(result))\n",
    "                    #print(type(subvect))\n",
    "                    # append to result\n",
    "                    result = torch.cat([result,subvect], dim=0)\n",
    "                    #print(\"result after cat\",result)\n",
    "                    #print()\n",
    "                    \n",
    "                    i1 = i\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "                #print(\" equal, do nothing just increment i and i2\")\n",
    "             \n",
    "            i+=1\n",
    "            i2=i\n",
    "                \n",
    "        #print(\" final result\", result)\n",
    "        #print(neighbors[0, result], neighbors[1, result])\n",
    "        return neighbors[0, result], neighbors[1, result]\n",
    "        \n",
    "  \n",
    "    \n",
    "class NetLSAGE1(torch.nn.Module):\n",
    "    def __init__(self, dataset, d1=16,d2=16, sampling_size=4, k=1):\n",
    "        super(NetLSAGE1, self).__init__()\n",
    "        self.conv1 = LinkSAGEConv(\n",
    "            int(dataset.y.size()[0]), \n",
    "            int(dataset.y.size()[0]), \n",
    "            sampling_size,\n",
    "            k,\n",
    "            normalize=False,\n",
    "            bias=False)\n",
    "        self.fc1 = nn.Linear(dataset.num_features, d2)\n",
    "        self.fc2 = nn.Linear(d2, dataset.num_features)\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        print(\"init NetLSAGE1 \",dataset.y.size()[0])\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_neighbors = data.x, data.edge_neighbors\n",
    "\n",
    "        x = self.conv1(x, edge_neighbors)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # output as multiclass target\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # output as regression target\n",
    "        return x\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Net1-gcn(%d,%d)-gcn(%d,%d)\" % (dataset.num_features,self.d1,self.d1,\n",
    "                                               dataset.num_classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#from torch_geometric.nn import GCNConv\n",
    "#from torch_geometric.nn import LinkGCNConv\n",
    "#from torch_geometric.nn import SAGEConv\n",
    "#from torch_geometric.nn import LinkSAGEConv\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "\n",
    "import networkx as nx\n",
    "import time\n",
    "from torch_geometric.data import DataLoader\n",
    "import importlib\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyOwnDataset2():\n",
    "    def __init__(self,  root, name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        #print(self.data)\n",
    "        #print(self.data.edge_index)\n",
    "        #print(self.data.num_features)\n",
    "        self.num_features = self.data.num_features\n",
    "        self.num_classes = 1\n",
    "        self.filename = name\n",
    "        \n",
    "        # prepare edge_neighbors\n",
    "        edges_dict = {}\n",
    "        i=0\n",
    "        for edge in self.data.edge_index[0]:\n",
    "            edges_dict[i]= (self.data.edge_index[0][i],\n",
    "                            self.data.edge_index[1][i])\n",
    "            i+=1\n",
    "            \n",
    "        #print(\"\\n edges_dict:\")\n",
    "        #pprint(edges_dict)\n",
    "        #print(\"\\n\")\n",
    "        \"\"\"\n",
    "            {\n",
    "            0 : (1,3),\n",
    "            1 : (1,4),\n",
    "            2 : (2,0)\n",
    "            ...\n",
    "            }\n",
    "            \n",
    "            edge_neighbors= [[],[]]\n",
    "            for edge in edges_dict.keys():\n",
    "                for node in edges_dict[edge]:\n",
    "                    for edge2 in edges_dict.keys():\n",
    "                        if edge2 != edge and \\\n",
    "                           ( edges_dict[edge2][0] == node or \\\n",
    "                             edges_dict[edge2][1] == node ):\n",
    "                             edge_neighbors[0].append(edge)\n",
    "                             edge_neighbors[1].append(edge2)\n",
    "                             \n",
    "        \"\"\"\n",
    "        edge_neighbors= [[],[]]\n",
    "        for edge in edges_dict.keys():\n",
    "            for node in edges_dict[edge]:\n",
    "                for edge2 in edges_dict.keys():\n",
    "                    if edge2 != edge and ( edges_dict[edge2][0] == node or edges_dict[edge2][1] == node ):\n",
    "                        edge_neighbors[0].append(edge)\n",
    "                        edge_neighbors[1].append(edge2)\n",
    "        \n",
    "        self.data.edge_neighbors = torch.LongTensor(edge_neighbors)\n",
    "        \n",
    "        #print()\n",
    "        #print(\"edge_neighbors\")\n",
    "        #pprint(self.data.edge_neighbors)\n",
    "        #print()\n",
    "        #print(type(self.data.edge_neighbors))\n",
    "        #print()\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "def loadDataset(collection, name=None):\n",
    "    try:\n",
    "        # import datasets\n",
    "        themodule = importlib.import_module(\"torch_geometric.datasets\")\n",
    "        # get the function corresponding to collection\n",
    "        method_to_call = getattr(themodule, collection)\n",
    "        if name:\n",
    "            dataset = method_to_call(root='./data/'+str(collection), name=name)\n",
    "            dataset.filename = name\n",
    "            return dataset\n",
    "        else:\n",
    "            return method_to_call(root='./data/'+str(collection)) \n",
    "    except:\n",
    "        # custom module\n",
    "        method_to_call = globals()[collection]\n",
    "       \n",
    "        if name:\n",
    "            \n",
    "            dataset = method_to_call(root='./data/'+str(collection), name=name)\n",
    "            dataset.filename = name\n",
    "            return dataset\n",
    "        else:\n",
    "            return method_to_call(root='./data/'+str(collection)) \n",
    "        \n",
    "\n",
    "\n",
    "def transformMask(mask):\n",
    "    train_mask = []\n",
    "    i = 0\n",
    "    for pick in mask:\n",
    "        if pick[0]==1:\n",
    "            train_mask.append(i)\n",
    "        i+=1\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def shuffleTrainTestMasks(data, trainpct = 0.7):\n",
    "    ysize = list(data.y.size())[0]\n",
    "    data.train_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "    data.train_mask[int(ysize*trainpct):] = 1\n",
    "    data.train_mask = data.train_mask[torch.randperm(ysize)]\n",
    "    data.test_mask = torch.ones(ysize,1, dtype=torch.long) - data.train_mask\n",
    "    \n",
    "    data.train_mask = transformMask(data.train_mask)\n",
    "    data.test_mask = transformMask(data.test_mask)\n",
    "  \n",
    "\n",
    "def shuffleTrainTestValMasks(data, trainpct = 0.7, valpct = 0.2):\n",
    "\n",
    "    ysize = list(data.y.size())[0]\n",
    "    #print(\"total \", ysize)\n",
    "    #print(\" train \",int(ysize*trainpct)-int(ysize*trainpct*valpct))\n",
    "    #print(\" val \",int(ysize*trainpct*valpct))\n",
    "    #print(\" test \",int(ysize*(1- trainpct) ))\n",
    "    data.train_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "    data.train_mask[:int(ysize*trainpct)] = 1\n",
    "    data.train_mask = data.train_mask[torch.randperm(ysize)]\n",
    "    #print(\" train sum \",data.train_mask.sum())\n",
    "    data.test_mask = torch.ones(ysize,1, dtype=torch.long) - data.train_mask\n",
    "    #print(\" test sum \",data.test_mask.sum())\n",
    "    \n",
    "    # transform to list of indexes\n",
    "    data.train_mask = transformMask(data.train_mask)\n",
    "    data.test_mask = transformMask(data.test_mask)\n",
    "    \n",
    "    data.val_mask = data.train_mask[:int(ysize*trainpct*valpct)]\n",
    "    data.train_mask = data.train_mask[int(ysize*trainpct*valpct):]\n",
    "\n",
    "    \n",
    "    #print(data.train_mask)\n",
    "    #print(data.val_mask)\n",
    "    #print(data.test_mask)\n",
    "    \n",
    "    \n",
    "\n",
    "def trainTestEval(dataset, epochs=1, batch_size=32):\n",
    "    global Net\n",
    "    loader = DataLoader(dataset,  shuffle=False)\n",
    "    i = 0\n",
    "    #print(loader)\n",
    "    #print(dir(loader))\n",
    "    \n",
    "    G = dataset.data\n",
    "    print(G)\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # 1.  prepare model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print(\"using \",device)\n",
    "    model = Net.to(device)  \n",
    "    data = G.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    model.train()\n",
    "\n",
    "    # 2.  create a train_mask, and a test_mask (val_mask for further experiments)\n",
    "    #shuffleTrainTestMasks(data)\n",
    "    #shuffleTrainTestValMasks(data)\n",
    "    shuffleTrainTestMasks(data, trainpct=0.7)\n",
    "\n",
    "    # 3. train some epochs\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        #print(\"out\",out[data.train_mask],out[data.train_mask].size())\n",
    "        #print(\"targetr \",data.y[data.train_mask],data.y[data.train_mask].size())\n",
    "        #print(data.train_mask)\n",
    "        loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 25 == 0 :\n",
    "            print(\"epoch-loss: \",epoch, loss)\n",
    "\n",
    "    # 4. Model evaluation\n",
    "    model.eval()\n",
    "    #  classification in a multiclass setting\n",
    "    #_, pred = model(data).max(dim=1)\n",
    "    #correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
    "    #acc = correct / data.test_mask.sum().item()\n",
    "    #print('Accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "\n",
    "    # regression \n",
    "    pred = model(data)\n",
    "    #print(\"target: \",data.y[data.test_mask])\n",
    "    #print(\"prediction: \",pred[data.test_mask])\n",
    "    #print(pred[data.test_mask].type())\n",
    "    #print(data.y[data.test_mask].type())\n",
    "    \n",
    "    # prepare the normalized mean root squared error\n",
    "    t = data.y[data.test_mask]\n",
    "    y = pred[data.test_mask]\n",
    "    nrmse = torch.sum((t - y) ** 2)/len(data.test_mask)\n",
    "    nrmse = nrmse.sqrt()\n",
    "    print(\"RMSE: \",nrmse)\n",
    "\n",
    "    #m = torch.mean(t)\n",
    "    #print(\"mean\",m)\n",
    "    #tmax = torch.max(t)\n",
    "    #tmin = torch.min(t)\n",
    "    #sd = tmax-tmin\n",
    "    #print(\"sd\",sd)\n",
    "    #nrmse = (nrmse - m)/sd\n",
    "    #print(\"NRMSE:\",nrmse)\n",
    "\n",
    "\n",
    "    endtime = time.time()\n",
    "    print(\"Total train-test time: \"+str(endtime-start))\n",
    "    \n",
    "    with open(\"results.txt\",\"a\") as f:\n",
    "        #print(dir(dataset))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(model)+\" \" \n",
    "                +str(dataset.filename)+\" \"  \n",
    "                +\"nrmse: \"+str(nrmse.item())+\" \" \n",
    "                +\"total time: \"+str(endtime-start) \n",
    "                +\" negative vals?: \"+str(False) \n",
    "                +\"\\n\"\n",
    "               )\n",
    "    \n",
    "    del model\n",
    "\n",
    "    #i+=1\n",
    "    #if i==1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init NetLSAGE1  7\n",
      "Data(edge_index=[2, 7], edge_neighbors=[2, 28], x=[7, 1], y=[7])\n",
      "epoch-loss:  0 tensor(0.1360, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  200 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  225 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  250 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  275 tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "RMSE:  tensor(0.0998, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 1.303556203842163\n"
     ]
    }
   ],
   "source": [
    "#dataset = loadDataset(collection='MyOwnDataset2', name='precomputed/er_10_0_10_nb.pickle')\n",
    "dataset = MyOwnDataset2(\n",
    "    root='', \n",
    "    name='precomputed/er_5_0_45_eb.pickle')\n",
    "#print(dir(dataset.data))\n",
    "#print()\n",
    "global Net\n",
    "Net=NetLSAGE1(dataset.data, d1=30,d2=5, sampling_size=4,k=3)\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "trainTestEval(dataset,  epochs=300)\n",
    "del Net\n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [0, 1, 1, 0]])\n",
      "tensor(2) tensor(2)\n",
      "tensor(1) tensor(1)\n",
      "tensor([1, 3]) tensor([1, 3])\n",
      "tensor([4, 3, 1]) tensor([4, 3, 1])\n",
      "tensor([3, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor([[1,2,3,4],[0,1,1,0]])\n",
    "print(a)\n",
    "print(a[0][1], a[0,1])\n",
    "print(a[1][1], a[1,1])\n",
    "b = torch.ByteTensor([True, False, True, False])\n",
    "print(a[0][b], a[0,b])\n",
    "b = torch.LongTensor([3,2,0])\n",
    "print(a[0][b], a[0,b])\n",
    "d = torch.LongTensor([1,1,1,1,1,1,1,1,1])\n",
    "c = torch.cat([b,d], dim=0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 90], edge_neighbors=[2, 2148], x=[90, 1], y=[90])\n",
      "epoch-loss:  0 tensor(0.1463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  200 tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  225 tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  250 tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  275 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "RMSE:  tensor(0.0521, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 687.5783488750458\n"
     ]
    }
   ],
   "source": [
    "#dataset = loadDataset(collection='MyOwnDataset2', name='precomputed/er_10_0_10_nb.pickle')\n",
    "dataset = MyOwnDataset2(\n",
    "    root='', \n",
    "    name='precomputed/TUDataset_1765_eb.pickle')\n",
    "#print(dir(dataset.data))\n",
    "#print()\n",
    "global Net\n",
    "Net=NetLSAGE1(d1=255,d2=5)\n",
    "trainTestEval(dataset,  epochs=300)\n",
    "del Net\n",
    "# before applying that all is done in GPU: 687s and RMSE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 90], edge_neighbors=[2, 2148], x=[90, 1], y=[90])\n",
      "epoch-loss:  0 tensor(0.1020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  200 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  225 tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  250 tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  275 tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "RMSE:  tensor(0.0288, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 27.81558394432068\n"
     ]
    }
   ],
   "source": [
    "#dataset = loadDataset(collection='MyOwnDataset2', name='precomputed/er_10_0_10_nb.pickle')\n",
    "dataset = MyOwnDataset2(\n",
    "    root='', \n",
    "    name='precomputed/TUDataset_1765_eb.pickle')\n",
    "#print(dir(dataset.data))\n",
    "#print()\n",
    "global Net\n",
    "Net=NetLSAGE1(d1=255,d2=5)\n",
    "trainTestEval(dataset,  epochs=300)\n",
    "del Net\n",
    "# after applying that all is done in GPU: 27s and RMSE 0.03, 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init NetLSAGE1  90\n",
      "Data(edge_index=[2, 90], edge_neighbors=[2, 2148], x=[90, 1], y=[90])\n",
      "epoch-loss:  0 tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(6.5384e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(6.9279e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(5.6953e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(5.6657e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(5.6611e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  200 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  225 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  250 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  275 tensor(5.6606e-06, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "RMSE:  tensor(0.0411, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 26.779576778411865\n"
     ]
    }
   ],
   "source": [
    "#dataset = loadDataset(collection='MyOwnDataset2', name='precomputed/er_10_0_10_nb.pickle')\n",
    "dataset = MyOwnDataset2(\n",
    "    root='', \n",
    "    name='precomputed/TUDataset_1765_eb.pickle')\n",
    "#print(dir(dataset.data))\n",
    "#print()\n",
    "global Net\n",
    "#Net=NetLSAGE1(d1=255,d2=5)\n",
    "Net=NetLSAGE1(dataset.data, d1=30,d2=5, sampling_size=4,k=3)\n",
    "trainTestEval(dataset,  epochs=300)\n",
    "del Net\n",
    "# before applying that all is done in GPU: 687s and RMSE 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init NetLSAGE1  90\n",
      "Data(edge_index=[2, 90], edge_neighbors=[2, 2148], x=[90, 1], y=[90])\n",
      "epoch-loss:  0 tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(5.1887e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(2.4448e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(2.4115e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(2.4110e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(2.4113e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(2.4114e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(2.4113e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  200 tensor(2.4112e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  225 tensor(2.4112e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  250 tensor(2.4111e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  275 tensor(2.4111e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "RMSE:  tensor(0.0326, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 28.606421947479248\n"
     ]
    }
   ],
   "source": [
    "#dataset = loadDataset(collection='MyOwnDataset2', name='precomputed/er_10_0_10_nb.pickle')\n",
    "dataset = MyOwnDataset2(\n",
    "    root='', \n",
    "    name='precomputed/TUDataset_1765_eb.pickle')\n",
    "#print(dir(dataset.data))\n",
    "#print()\n",
    "global Net\n",
    "#Net=NetLSAGE1(d1=255,d2=5)\n",
    "Net=NetLSAGE1(dataset.data, d1=30,d2=5, sampling_size=10,k=7)\n",
    "trainTestEval(dataset,  epochs=300)\n",
    "del Net\n",
    "# before applying that all is done in GPU: 687s and RMSE 0.05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
