{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import time\n",
    "import pickle\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "def writeAdjacencyMatrixToDisk(G, filename='temp_adjacency_matrix.txt'):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    f = open(filename,'w')\n",
    "    _ni=-1\n",
    "    newline = False\n",
    "    theline = []\n",
    "    careturn = \"\"\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        if int(G.edge_index[0,ei].item()) != _ni:\n",
    "            newline=True\n",
    "            _ni=int(G.edge_index[0,ei].item())\n",
    "            \n",
    "        else:\n",
    "            newline=False\n",
    "            \n",
    "            \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if newline:\n",
    "            f.write(''.join(theline))\n",
    "            #print(''.join(theline))\n",
    "            #print(\" --> \"+str(_ni))\n",
    "            theline =[]\n",
    "            theline.append(careturn+ni+\" \")\n",
    "            theline.append(vi+\" \")\n",
    "            careturn = \"\\n\"\n",
    "        else:\n",
    "            theline.append(vi+\" \")\n",
    "        # print(\"({},{})\".format(ni,vi))\n",
    "    \n",
    "    \n",
    "def nx_createNxGraphInMem(G):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    g = nx.MultiGraph()\n",
    "   \n",
    "    for ei in range(G.edge_index.size()[1]):    \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        g.add_edge(ni,vi)\n",
    "    return g\n",
    "    \n",
    "def nx_verifyEdges(G, g):\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if (ni,vi,0) not in list(g.edges):\n",
    "            if (vi,ni,1) not in list(g.edges):\n",
    "                print(\"Error {} not in networkx graph\".format((ni,vi)))\n",
    "            \n",
    "        \n",
    "\n",
    "def nx_compute_edge_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.edge_betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "def nx_compute_node_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "\n",
    "def update_edge_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \n",
    "        G.edge_attr must contain the edge betweenness values \n",
    "        for each edge\n",
    "        \n",
    "        G.y must contain it also.. (it is a copy of the edge betweenness..)\n",
    "        this could help the training phase\n",
    "        \n",
    "        Size restrictions:\n",
    "        - Given the size of the graphs, is it better to just transform the \n",
    "        object instead to write a new one?\n",
    "        - also just use G.y? but for GNN algorithms..not sure\n",
    "        \n",
    "        new_edg_attr will be size [num edges, 1]\n",
    "        and must be sorted in accordance to G.edge_index\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_edg_attr = []\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            #print((ni,vi))\n",
    "            try:\n",
    "                new_edg_attr.append([eb_dict[(ni,vi)]])\n",
    "            except:\n",
    "                try:\n",
    "                    new_edg_attr.append([eb_dict[(vi,ni)]])\n",
    "                except:\n",
    "                    #print(\"ERROR {} and {} not found!\".format((ni,vi),(vi,ni)))\n",
    "                    new_edg_attr.append([0])\n",
    "        else:\n",
    "            new_edg_attr.append([0])\n",
    "\n",
    "    new_edg_attr = torch.FloatTensor(new_edg_attr)\n",
    "    \n",
    "    #newG = Data(\n",
    "    #    x=G.x, \n",
    "    #    edge_index=G.edge_index, \n",
    "    #    edge_attr=new_edg_attr,\n",
    "    #    y=new_edg_attr)\n",
    "    \n",
    "    #G.edge_attr = new_edg_attr\n",
    "    G.y = new_edg_attr\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def update_node_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        Get nodes keys from eb_dict and get their betweenness centrality\n",
    "        G.y will have all centralities of al lnodes following the order\n",
    "        of a list of the nodes sorted by id\n",
    "\n",
    "        add spaces in between!\n",
    "\n",
    "    \"\"\"\n",
    "    betweennesses = []\n",
    "    nodes = sorted([int(k) for k in eb_dict.keys()])\n",
    "    for node in range(nodes[-1]+1):\n",
    "        try:\n",
    "            betweennesses.append(eb_dict[str(node+1)])    \n",
    "        except:\n",
    "            betweennesses.append(0.0)\n",
    "            \n",
    "    G.y = torch.FloatTensor(betweennesses)\n",
    "    return G\n",
    "\n",
    "def get_betweenness_into_dict(G):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \"\"\"\n",
    "    \n",
    "    eb_dict ={}\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            eb_dict[(ni,vi)] = float(G.y[i].item())\n",
    "    return eb_dict\n",
    "\n",
    "\n",
    "def computeBetweenness(G,suffix=0):\n",
    "    \"\"\"\n",
    "        Alternatives:\n",
    "            - to disk, to nx, then dict of betweenness\n",
    "            - transform in memory\n",
    "            - directly pickle a G object with the betweenness\n",
    "    \"\"\"\n",
    "    prefix = 'temp_aj_m'\n",
    "    # 1. PyTorch Geometric graph -> nx -> compute betweenness \n",
    "    #             -> PyTorch Geom with target the betweenness-------\n",
    "    # Transform to networkx graph\n",
    "    # write to adjacency matrix on disk\n",
    "    writeAdjacencyMatrixToDisk(G, filename=prefix+str(suffix)+'.txt')\n",
    "\n",
    "    # load into a networkx graph object\n",
    "    g2 = nx.read_adjlist(prefix+str(suffix)+'.txt')\n",
    "    #g2 = nx_createNxGraphInMem(G)\n",
    "\n",
    "    # compute node betweenness centrality\n",
    "    eb_dict = nx_compute_node_betweenness(g2)\n",
    "    #print(\"eb_dict\",eb_dict)\n",
    "    \n",
    "    # write node betweenness back to PyTorch Geometric graph\n",
    "    update_node_betweenness(G,eb_dict)\n",
    "    #return G\n",
    "    \n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        #print(\"root \", root, \" name \", name)\n",
    "        #print(\"setting a self.name in the object!\")\n",
    "        #self.name = name\n",
    "        #print(dir(self))\n",
    "        #super(MyOwnDataset, self).__init__(root,transform, pre_transform)\n",
    "        #self.data = torch.load(self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['PPI0.pickle']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['PPI0']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        return True\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        \n",
    "        # unpickle the graph\n",
    "        print(\"going to unpickle\")\n",
    "        f = open(self.name, 'rb')\n",
    "        G = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        \n",
    "        #if self.pre_filter is not None:\n",
    "        #    data_list [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        #if self.pre_transform is not None:\n",
    "        #    data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        #data, slices = self.collate(data_list)\n",
    "        torch.save(G, self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "class MyOwnDataset2():\n",
    "    def __init__(self,  name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        f.close()\n",
    "    \n",
    "def loadDataset(collection, name=None, split=None):\n",
    "    # import datasets\n",
    "    themodule = importlib.import_module(\"torch_geometric.datasets\")\n",
    "    # get the function corresponding to collection\n",
    "    method_to_call = getattr(themodule, collection)\n",
    "    if name:\n",
    "        return method_to_call(root='./data/'+str(collection), name=name)\n",
    "    elif split:\n",
    "        return method_to_call(root='./data/'+str(collection), split=split)\n",
    "    else:\n",
    "        return method_to_call(root='./data/'+str(collection)) \n",
    "\n",
    "    \n",
    "def createDataset(x, edge_index):\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "\n",
    "def createDatasetFromNX(g):\n",
    "    # get edge list\n",
    "    edges = g.edges\n",
    "    edge_list_1 = []\n",
    "    edge_list_2 = []\n",
    "    for e in edges:\n",
    "        # node id must be an int\n",
    "        edge_list_1.append(int(e[0])) \n",
    "        edge_list_2.append(int(e[1]))\n",
    "        \n",
    "    edge_index = torch.tensor([ edge_list_1,\n",
    "                                edge_list_2], dtype=torch.long)\n",
    "    \n",
    "    # create single 1 feature for each node\n",
    "    n = len(g.nodes())\n",
    "    x = [[1.0] for i in range(n)]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    return createDataset(x, edge_index)\n",
    "                         \n",
    "def createDatasetFromNXwithTarget(g,y):\n",
    "    dataset =  createDatasetFromNX(g)\n",
    "    y = torch.FloatTensor(y)\n",
    "    dataset.y = y \n",
    "    return dataset\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Graph datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b2372a6dd028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# translate into a PyTorch Geometric dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateDatasetFromNXwithTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# check dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-66dda9d16134>\u001b[0m in \u001b[0;36mcreateDatasetFromNXwithTarget\u001b[0;34m(g, y)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "# generate random graphs\n",
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "red = nx.random_lobster(100, 0.9, 0.9)\n",
    "g=er\n",
    "\n",
    "\n",
    "# compute its node and edge betweenness\n",
    "nx_betweenness = nx.betweenness_centrality(g)\n",
    "nx_edge_betweenness = nx.edge_betweenness_centrality(g)\n",
    "y = [ v for k,v in nx_betweenness.items()]\n",
    "\n",
    "# verify order of betweenness is the same as  order of edge_list\n",
    "#print(nx_betweenness)\n",
    "#print(y)\n",
    "#-> ok!\n",
    "\n",
    "# translate into a PyTorch Geometric dataset \n",
    "dataset = createDatasetFromNXwithTarget(g,y)\n",
    "\n",
    "# check dimensions\n",
    "print(dataset) # check edge_index[1] and y have same length\n",
    "\n",
    "\n",
    "# save as a pickled object\n",
    "dname = \"er_100_0_15\"\n",
    "i=0\n",
    "with open(dname+\"_\"+str(i)+'.pickle','wb') as f:\n",
    "    pickle.dump(dataset,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createRandomGraphDataset(g,dname,betweenness):\n",
    "    \n",
    "    # compute its node and edge betweenness\n",
    "    y =[]\n",
    "    \n",
    "    if betweenness == 'node':\n",
    "        nx_betweenness = nx.betweenness_centrality(g)\n",
    "        y = [ v for k,v in nx_betweenness.items()]\n",
    "    else:\n",
    "        nx_edge_betweenness = nx.edge_betweenness_centrality(g)\n",
    "        y = [ v for k,v in nx_edge_betweenness.items()]\n",
    "        \n",
    "    # verify order of betweenness is the same as  order of edge_list\n",
    "    #print(nx_betweenness)\n",
    "    #print(y)\n",
    "    #-> ok!\n",
    "\n",
    "    # translate into a PyTorch Geometric dataset \n",
    "    dataset = createDatasetFromNXwithTarget(g,y)\n",
    "    print(dataset.num_features)\n",
    "\n",
    "    # check dimensions\n",
    "    #print(dataset) # check edge_index[1] and y have same length\n",
    "\n",
    "    # save as a pickled object\n",
    "    with open(dname+'.pickle','wb') as f:\n",
    "        pickle.dump(dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createRandomGraphDataset(er,'er_100_0_15_nb','node')\n",
    "createRandomGraphDataset(er,'er_100_0_15_eb','edge')\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_30_3_0_1','node')\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "red = nx.random_lobster(100, 0.9, 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[56944], edge_index=[2, 818716], test_mask=[56944], train_mask=[56944], val_mask=[56944], x=[56944, 50], y=[56944, 121])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-759b1515274c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcomputeBetweenness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-66dda9d16134>\u001b[0m in \u001b[0;36mcomputeBetweenness\u001b[0;34m(G, suffix)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# Transform to networkx graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# write to adjacency matrix on disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mwriteAdjacencyMatrixToDisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# load into a networkx graph object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-66dda9d16134>\u001b[0m in \u001b[0;36mwriteAdjacencyMatrixToDisk\u001b[0;34m(G, filename)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#PPI\n",
    "dname='PPI'\n",
    "dataset = loadDataset(dname)\n",
    "#QM7b\n",
    "#dataset = loadDataset('QM7b')\n",
    "#MUTAG\n",
    "#dataset = loadDataset(collection='Entities',name='MUTAG')\n",
    "#ENZYMES FROM TUDataset\n",
    "#dataset = loadDataset(collection='TUDataset',name='ENZYMES')\n",
    "# Cora\n",
    "#dataset = loadDataset(collection='Planetoid',name='Cora')\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, shuffle=False)\n",
    "i = 0\n",
    "for G in loader:\n",
    "    print(G)\n",
    "    computeBetweenness(G,i)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    #if i>10:\n",
    "    #    break\n",
    "    dataset.data.y = G.y\n",
    "        \n",
    "        \n",
    "print(dataset.data.y)\n",
    "\n",
    "# pickle\n",
    "with open(dname+str(i)+'.pickle','wb') as f:\n",
    "    pickle.dump(dataset,f)\n",
    "        \n",
    "        \n",
    "        \n",
    "dataset = MyOwnDataset2(name='PPI0.pickle')\n",
    "print(dataset.data)\n",
    "print(len(dataset.data.y))\n",
    "print(len(set(dataset.data.edge_index[1]) ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
