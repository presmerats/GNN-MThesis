{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import time\n",
    "import pickle\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "def writeAdjacencyMatrixToDisk(G, filename='temp_adjacency_matrix.txt'):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    f = open(filename,'w')\n",
    "    _ni=-1\n",
    "    newline = False\n",
    "    theline = []\n",
    "    careturn = \"\"\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        if int(G.edge_index[0,ei].item()) != _ni:\n",
    "            newline=True\n",
    "            _ni=int(G.edge_index[0,ei].item())\n",
    "            \n",
    "        else:\n",
    "            newline=False\n",
    "            \n",
    "            \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if newline:\n",
    "            f.write(''.join(theline))\n",
    "            #print(''.join(theline))\n",
    "            #print(\" --> \"+str(_ni))\n",
    "            theline =[]\n",
    "            theline.append(careturn+ni+\" \")\n",
    "            theline.append(vi+\" \")\n",
    "            careturn = \"\\n\"\n",
    "        else:\n",
    "            theline.append(vi+\" \")\n",
    "        # print(\"({},{})\".format(ni,vi))\n",
    "    \n",
    "    \n",
    "def nx_createNxGraphInMem(G):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    g = nx.MultiGraph()\n",
    "   \n",
    "    for ei in range(G.edge_index.size()[1]):    \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        g.add_edge(ni,vi)\n",
    "    return g\n",
    "    \n",
    "def nx_verifyEdges(G, g):\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if (ni,vi,0) not in list(g.edges):\n",
    "            if (vi,ni,1) not in list(g.edges):\n",
    "                print(\"Error {} not in networkx graph\".format((ni,vi)))\n",
    "            \n",
    "        \n",
    "\n",
    "def nx_compute_edge_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.edge_betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "def nx_compute_node_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "\n",
    "def update_edge_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \n",
    "        G.edge_attr must contain the edge betweenness values \n",
    "        for each edge\n",
    "        \n",
    "        G.y must contain it also.. (it is a copy of the edge betweenness..)\n",
    "        this could help the training phase\n",
    "        \n",
    "        Size restrictions:\n",
    "        - Given the size of the graphs, is it better to just transform the \n",
    "        object instead to write a new one?\n",
    "        - also just use G.y? but for GNN algorithms..not sure\n",
    "        \n",
    "        new_edg_attr will be size [num edges, 1]\n",
    "        and must be sorted in accordance to G.edge_index\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_edg_attr = []\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            #print((ni,vi))\n",
    "            try:\n",
    "                new_edg_attr.append([eb_dict[(ni,vi)]])\n",
    "            except:\n",
    "                try:\n",
    "                    new_edg_attr.append([eb_dict[(vi,ni)]])\n",
    "                except:\n",
    "                    #print(\"ERROR {} and {} not found!\".format((ni,vi),(vi,ni)))\n",
    "                    new_edg_attr.append([0])\n",
    "        else:\n",
    "            new_edg_attr.append([0])\n",
    "\n",
    "    new_edg_attr = torch.FloatTensor(new_edg_attr)\n",
    "    \n",
    "    #newG = Data(\n",
    "    #    x=G.x, \n",
    "    #    edge_index=G.edge_index, \n",
    "    #    edge_attr=new_edg_attr,\n",
    "    #    y=new_edg_attr)\n",
    "    \n",
    "    #G.edge_attr = new_edg_attr\n",
    "    G.y = new_edg_attr\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def update_node_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        Get nodes keys from eb_dict and get their betweenness centrality\n",
    "        G.y will have all centralities of al lnodes following the order\n",
    "        of a list of the nodes sorted by id\n",
    "\n",
    "        add spaces in between!\n",
    "\n",
    "    \"\"\"\n",
    "    betweennesses = []\n",
    "    nodes = sorted([int(k) for k in eb_dict.keys()])\n",
    "    for node in range(nodes[-1]+1):\n",
    "        try:\n",
    "            betweennesses.append(eb_dict[str(node+1)])    \n",
    "        except:\n",
    "            betweennesses.append(0.0)\n",
    "            \n",
    "    G.y = torch.FloatTensor(betweennesses)\n",
    "    return G\n",
    "\n",
    "def get_betweenness_into_dict(G):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \"\"\"\n",
    "    \n",
    "    eb_dict ={}\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            eb_dict[(ni,vi)] = float(G.y[i].item())\n",
    "    return eb_dict\n",
    "\n",
    "\n",
    "def pyTorchGeometricDatasetToNx(G,suffix=0):\n",
    "    \"\"\"\n",
    "        Alternatives:\n",
    "            - to disk, to nx, then dict of betweenness\n",
    "            - transform in memory\n",
    "            - directly pickle a G object with the betweenness\n",
    "    \"\"\"\n",
    "    prefix = 'temp_aj_m'\n",
    "    # 1. PyTorch Geometric graph -> nx -> compute betweenness \n",
    "    #             -> PyTorch Geom with target the betweenness-------\n",
    "    # Transform to networkx graph\n",
    "    # write to adjacency matrix on disk\n",
    "    writeAdjacencyMatrixToDisk(G, filename=prefix+str(suffix)+'.txt')\n",
    "\n",
    "    # load into a networkx graph object\n",
    "    g2 = nx.read_adjlist(prefix+str(suffix)+'.txt')\n",
    "    #g2 = nx_createNxGraphInMem(G)\n",
    "    \n",
    "    return g2\n",
    "\n",
    "def computeBetweenness(G,suffix=0):\n",
    "    \"\"\"\n",
    "        Alternatives:\n",
    "            - to disk, to nx, then dict of betweenness\n",
    "            - transform in memory\n",
    "            - directly pickle a G object with the betweenness\n",
    "    \"\"\"\n",
    "    prefix = 'temp_aj_m'\n",
    "    # 1. PyTorch Geometric graph -> nx -> compute betweenness \n",
    "    #             -> PyTorch Geom with target the betweenness-------\n",
    "    # Transform to networkx graph\n",
    "    # write to adjacency matrix on disk\n",
    "    writeAdjacencyMatrixToDisk(G, filename=prefix+str(suffix)+'.txt')\n",
    "\n",
    "    # load into a networkx graph object\n",
    "    g2 = nx.read_adjlist(prefix+str(suffix)+'.txt')\n",
    "    #g2 = nx_createNxGraphInMem(G)\n",
    "\n",
    "    # compute node betweenness centrality\n",
    "    eb_dict = nx_compute_node_betweenness(g2)\n",
    "    #print(\"eb_dict\",eb_dict)\n",
    "    \n",
    "    # write node betweenness back to PyTorch Geometric graph\n",
    "    update_node_betweenness(G,eb_dict)\n",
    "    #return G\n",
    "    \n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        #print(\"root \", root, \" name \", name)\n",
    "        #print(\"setting a self.name in the object!\")\n",
    "        #self.name = name\n",
    "        #print(dir(self))\n",
    "        #super(MyOwnDataset, self).__init__(root,transform, pre_transform)\n",
    "        #self.data = torch.load(self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['PPI0.pickle']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['PPI0']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        return True\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        \n",
    "        # unpickle the graph\n",
    "        print(\"going to unpickle\")\n",
    "        f = open(self.name, 'rb')\n",
    "        G = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        \n",
    "        #if self.pre_filter is not None:\n",
    "        #    data_list [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        #if self.pre_transform is not None:\n",
    "        #    data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        #data, slices = self.collate(data_list)\n",
    "        torch.save(G, self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "class MyOwnDataset2():\n",
    "    def __init__(self,  name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        f.close()\n",
    "    \n",
    "def loadDataset(collection, name=None, split=None):\n",
    "    # import datasets\n",
    "    themodule = importlib.import_module(\"torch_geometric.datasets\")\n",
    "    # get the function corresponding to collection\n",
    "    method_to_call = getattr(themodule, collection)\n",
    "    if name:\n",
    "        return method_to_call(root='./data/'+str(collection), name=name)\n",
    "    elif split:\n",
    "        return method_to_call(root='./data/'+str(collection), split=split)\n",
    "    else:\n",
    "        return method_to_call(root='./data/'+str(collection)) \n",
    "\n",
    "    \n",
    "def createDataset(x, edge_index):\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "\n",
    "def createDatasetFromNX(g, undirected=True):\n",
    "    # get edge list\n",
    "    edges = g.edges\n",
    "    edge_list_1 = []\n",
    "    edge_list_2 = []\n",
    "    for e in edges:\n",
    "        # node id must be an int\n",
    "        edge_list_1.append(int(e[0])) \n",
    "        edge_list_2.append(int(e[1]))\n",
    "        if undirected:\n",
    "            edge_list_1.append(int(e[1])) \n",
    "            edge_list_2.append(int(e[0]))\n",
    "            \n",
    "        \n",
    "    edge_index = torch.tensor([ edge_list_1,\n",
    "                                edge_list_2], dtype=torch.long)\n",
    "    \n",
    "    # create single 1 feature for each node\n",
    "    n = len(g.nodes())\n",
    "    x = [[1.0] for i in range(n)]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    return createDataset(x, edge_index)\n",
    "                         \n",
    "def createDatasetFromNXwithTarget(g,y, undirected=True):\n",
    "    dataset =  createDatasetFromNX(g,undirected)\n",
    "    y = torch.FloatTensor(y)\n",
    "    dataset.y = y \n",
    "    return dataset\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Graph datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 740], x=[100, 1], y=[100])\n"
     ]
    }
   ],
   "source": [
    "# generate random graphs\n",
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "red = nx.random_lobster(100, 0.9, 0.9)\n",
    "g=er\n",
    "\n",
    "\n",
    "# compute its node and edge betweenness\n",
    "nx_betweenness = nx.betweenness_centrality(g)\n",
    "nx_edge_betweenness = nx.edge_betweenness_centrality(g)\n",
    "y = [ v for k,v in nx_betweenness.items()]\n",
    "\n",
    "# verify order of betweenness is the same as  order of edge_list\n",
    "#print(nx_betweenness)\n",
    "#print(y)\n",
    "#-> ok!\n",
    "\n",
    "# translate into a PyTorch Geometric dataset \n",
    "dataset = createDatasetFromNXwithTarget(g,y)\n",
    "\n",
    "# check dimensions\n",
    "print(dataset) # check edge_index[1] and y have same length\n",
    "\n",
    "\n",
    "# save as a pickled object\n",
    "dname = \"er_100_0_15\"\n",
    "i=0\n",
    "with open(dname+\"_\"+str(i)+'.pickle','wb') as f:\n",
    "    pickle.dump(dataset,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createRandomGraphDataset(g,dname,betweenness):\n",
    "    \n",
    "    # compute its node and edge betweenness\n",
    "    y =[]\n",
    "    \n",
    "    if betweenness == 'node':\n",
    "        nx_betweenness = nx.betweenness_centrality(g)\n",
    "        y = [ v for k,v in nx_betweenness.items()]\n",
    "    else:\n",
    "        nx_edge_betweenness = nx.edge_betweenness_centrality(g)\n",
    "        #y = [ v for k,v in nx_edge_betweenness.items()]\n",
    "        # we need to double it? because undirected graph show 2 edge each in one direction?\n",
    "        y = []\n",
    "        for k,v in nx_edge_betweenness.items():\n",
    "            y.append(v)\n",
    "            y.append(v)\n",
    "        \n",
    "        \n",
    "    # verify order of betweenness is the same as  order of edge_list\n",
    "    #print(nx_betweenness)\n",
    "    #print(y)\n",
    "    #-> ok!\n",
    "\n",
    "    # translate into a PyTorch Geometric dataset \n",
    "    dataset = createDatasetFromNXwithTarget(g,y)\n",
    "    print(dataset.num_features)\n",
    "\n",
    "    # check dimensions\n",
    "    print(dataset) # check edge_index[1] and y have same length\n",
    "    print(\" directed graph: \",dataset.is_directed())\n",
    "    print(\"isolated nodes: \",dataset.contains_isolated_nodes())\n",
    "    print(\"self loops: \",dataset.contains_self_loops())\n",
    "    \n",
    "    # save as a pickled object\n",
    "    with open(dname+'.pickle','wb') as f:\n",
    "        pickle.dump(dataset,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data(edge_index=[2, 1516], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n"
     ]
    }
   ],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "red = nx.random_lobster(100, 0.9, 0.9)\n",
    "\n",
    "\n",
    "createRandomGraphDataset(er,'er_100_0_15_nb','node')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data(edge_index=[2, 1542], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 4432], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 149596], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 449110], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 60], x=[30, 1], y=[30])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 200], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 2000], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 10000], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 950], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 9950], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-46f7f2fc2703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merdos_renyi_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcreateRandomGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'er_4000_0_15_nb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'node'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#er = nx.erdos_renyi_graph(4000, 0.35)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#createRandomGraphDataset(er,'er_4000_0_35_nb','node')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-033ff6ee5e78>\u001b[0m in \u001b[0;36mcreateRandomGraphDataset\u001b[0;34m(g, dname, betweenness)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbetweenness\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'node'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnx_betweenness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetweenness_centrality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnx_betweenness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/pau/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/decorator.py:decorator-gen-229>\u001b[0m in \u001b[0;36mbetweenness_centrality\u001b[0;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_random_state\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mnew_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_state_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/algorithms/centrality/betweenness.py\u001b[0m in \u001b[0;36mbetweenness_centrality\u001b[0;34m(G, k, normalized, weight, endpoints, seed)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# single source shortest paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use BFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_source_shortest_path_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use Dijkstra's algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_source_dijkstra_path_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/algorithms/centrality/betweenness.py\u001b[0m in \u001b[0;36m_single_source_shortest_path_basic\u001b[0;34m(G, s)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# this is a shortest path, count paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msigmav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predecessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createRandomGraphDataset(er,'er_100_0_15_nb','node')\n",
    "er = nx.erdos_renyi_graph(100, 0.45)\n",
    "createRandomGraphDataset(er,'er_100_0_45_nb','node')\n",
    "er = nx.erdos_renyi_graph(1000, 0.15)\n",
    "createRandomGraphDataset(er,'er_1000_0_15_nb','node')\n",
    "er = nx.erdos_renyi_graph(1000, 0.45)\n",
    "createRandomGraphDataset(er,'er_1000_0_45_nb','node')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_30_3_0_1_nb','node')\n",
    "ws = nx.watts_strogatz_graph(100, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_100_3_0_1_nb','node')\n",
    "ws = nx.watts_strogatz_graph(1000, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_1000_3_0_1_nb','node')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(1000, 10, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_1000_10_0_1_nb','node')\n",
    "\n",
    "\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "createRandomGraphDataset(ba,'ba_100_5_nb','node')\n",
    "ba = nx.barabasi_albert_graph(1000, 5)\n",
    "createRandomGraphDataset(ba,'ba_1000_5_nb','node')\n",
    "\n",
    "\n",
    "\n",
    "#er = nx.erdos_renyi_graph(4000, 0.15)\n",
    "#createRandomGraphDataset(er,'er_4000_0_15_nb','node')\n",
    "#er = nx.erdos_renyi_graph(4000, 0.35)\n",
    "#createRandomGraphDataset(er,'er_4000_0_35_nb','node')\n",
    "#ws = nx.watts_strogatz_graph(4000, 3, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_3_0_1_nb','node')\n",
    "#ws = nx.watts_strogatz_graph(4000, 20, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_20_0_1_nb','node')\n",
    "#ba = nx.barabasi_albert_graph(4000, 5)\n",
    "#createRandomGraphDataset(ba,'ba_4000_5_nb','node')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data(edge_index=[2, 1386], x=[100, 1], y=[693])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 4534], x=[100, 1], y=[2267])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 149652], x=[1000, 1], y=[74826])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 449736], x=[1000, 1], y=[224868])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 60], x=[30, 1], y=[30])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 200], x=[100, 1], y=[100])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 2000], x=[1000, 1], y=[1000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 10000], x=[1000, 1], y=[5000])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 950], x=[100, 1], y=[475])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n",
      "1\n",
      "Data(edge_index=[2, 9950], x=[1000, 1], y=[4975])\n",
      " directed graph:  False\n",
      "isolated nodes:  False\n",
      "self loops:  False\n"
     ]
    }
   ],
   "source": [
    "# repeat with edge betweenness\n",
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createRandomGraphDataset(er,'er_100_0_15_eb','edge')\n",
    "er = nx.erdos_renyi_graph(100, 0.45)\n",
    "createRandomGraphDataset(er,'er_100_0_45_eb','edge')\n",
    "er = nx.erdos_renyi_graph(1000, 0.15)\n",
    "createRandomGraphDataset(er,'er_1000_0_15_eb','edge')\n",
    "er = nx.erdos_renyi_graph(1000, 0.45)\n",
    "createRandomGraphDataset(er,'er_1000_0_45_eb','edge')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_30_3_0_1_eb','edge')\n",
    "ws = nx.watts_strogatz_graph(100, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_100_3_0_1_eb','edge')\n",
    "ws = nx.watts_strogatz_graph(1000, 3, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_1000_3_0_1_eb','edge')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(1000, 10, 0.1)\n",
    "createRandomGraphDataset(ws,'ws_1000_10_0_1_eb','edge')\n",
    "\n",
    "\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "createRandomGraphDataset(ba,'ba_100_5_eb','edge')\n",
    "ba = nx.barabasi_albert_graph(1000, 5)\n",
    "createRandomGraphDataset(ba,'ba_1000_5_eb','edge')\n",
    "\n",
    "\n",
    "#er = nx.erdos_renyi_graph(4000, 0.15)\n",
    "#createRandomGraphDataset(er,'er_4000_0_15_eb','edge')\n",
    "#er = nx.erdos_renyi_graph(4000, 0.35)\n",
    "#createRandomGraphDataset(er,'er_4000_0_35_eb','edge')\n",
    "#ws = nx.watts_strogatz_graph(4000, 3, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_3_0_1_eb','edge')\n",
    "#ws = nx.watts_strogatz_graph(4000, 20, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_20_0_1_eb','edge')\n",
    "#ba = nx.barabasi_albert_graph(4000, 5)\n",
    "#createRandomGraphDataset(ba,'ba_4000_5_eb','edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[56944], edge_index=[2, 818716], test_mask=[56944], train_mask=[56944], val_mask=[56944], x=[56944, 50], y=[56944, 121])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0b9af8711e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcomputeBetweenness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-04812b6de545>\u001b[0m in \u001b[0;36mcomputeBetweenness\u001b[0;34m(G, suffix)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;31m# compute node betweenness centrality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0meb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx_compute_node_betweenness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;31m#print(\"eb_dict\",eb_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-04812b6de545>\u001b[0m in \u001b[0;36mnx_compute_node_betweenness\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m#print(list(G.edges)[:10])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mG_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected_component_subgraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mG_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0meb_dict_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0meb_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetweenness_centrality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/algorithms/components/connected.py\u001b[0m in \u001b[0;36mconnected_component_subgraphs\u001b[0;34m(G, copy)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconnected_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         G.add_edges_from((u, v, datadict.copy())\n\u001b[0;32m-> 1532\u001b[0;31m                          \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m                          for v, datadict in nbrs.items())\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/envs/gnn-pytorch/lib/python3.6/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# plan\n",
    "# 1. get graph from PyTorch Geom\n",
    "# 2. transform to nx (pyTorchGeometricDatasetToNx)\n",
    "# 3. call createRandomGraphDataset(ba,'ba_1000_5','edge') \n",
    "#      or createRandomGraphDataset(ba,'ba_1000_5','node')\n",
    "\n",
    "\n",
    "def processDatasetsSingle(dname, dataset):\n",
    "    G = dataset.data\n",
    "    print(G)\n",
    "    g = pyTorchGeometricDatasetToNx(G,i)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    createRandomGraphDataset(g,dname+'_'+str(i)+'_eb','edge') \n",
    "    createRandomGraphDataset(g,dname+'_'+str(i)+'_nd','node') \n",
    "\n",
    "        \n",
    "def processDatasets(dname, dataset):\n",
    "    # set size of batch to total size of graph here\n",
    "    loader = DataLoader(dataset, shuffle=False)\n",
    "    i = 0\n",
    "    for G in loader:\n",
    "        print(G)\n",
    "        g = pyTorchGeometricDatasetToNx(G,i)\n",
    "        i+=1\n",
    "        print(i)\n",
    "        createRandomGraphDataset(g,dname+'_'+str(i)+'_eb','edge') \n",
    "        createRandomGraphDataset(g,dname+'_'+str(i)+'_nd','node') \n",
    "\n",
    "\n",
    "#KarateClub\n",
    "dname='KarateClub'\n",
    "dataset = loadDataset(dname)\n",
    "processDatasets(dname, dataset)\n",
    "\n",
    "#ENZYMES FROM TUDataset\n",
    "dname='TUDataset'\n",
    "name='ENZYMES'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname, dataset)\n",
    "\n",
    "#PROTEINS FROM TUDataset\n",
    "dname='TUDataset'\n",
    "name='PROTEINS'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname, dataset)\n",
    "\n",
    "#PPI\n",
    "dname='PPI'\n",
    "dataset = loadDataset(dname)\n",
    "processDatasets(dname,dataset)\n",
    "\n",
    "#QM7b\n",
    "dataset = loadDataset('QM7b')\n",
    "processDatasets(dname,dataset)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#MUTAG\n",
    "#dataset = loadDataset(collection='Entities',name='MUTAG')\n",
    "\n",
    "# Cora\n",
    "#dataset = loadDataset(collection='Planetoid',name='Cora')\n",
    "#processDatasets(dname,dataset)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
