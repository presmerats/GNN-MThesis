{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook:\n",
    "\n",
    "- reads graphs from the PyTorch Geometric dataset, into Networkx versions of the graphs, then computes the node betweenness centrality (and edge betweenness centrality).\n",
    "- then transforms the Networkx graph into a json graph object\n",
    "- it also creates id_map, which assign every node identifier to a consecutive integer\n",
    "- it also creates class_maps, which will save the betweeness centrality of each node\n",
    "- finally an edge version, converts graph into  node representation of vertices  and saves the edge betweenees in the class_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import time\n",
    "import pickle\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "      \n",
    "    \n",
    "def writeAdjacencyMatrixToDisk(G, filename='temp_adjacency_matrix.txt'):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    f = open(filename,'w')\n",
    "    _ni=-1\n",
    "    newline = False\n",
    "    theline = []\n",
    "    careturn = \"\"\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        if int(G.edge_index[0,ei].item()) != _ni:\n",
    "            newline=True\n",
    "            _ni=int(G.edge_index[0,ei].item())\n",
    "            \n",
    "        else:\n",
    "            newline=False\n",
    "            \n",
    "            \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if newline:\n",
    "            f.write(''.join(theline))\n",
    "            #print(''.join(theline))\n",
    "            #print(\" --> \"+str(_ni))\n",
    "            theline =[]\n",
    "            theline.append(careturn+ni+\" \")\n",
    "            theline.append(vi+\" \")\n",
    "            careturn = \"\\n\"\n",
    "        else:\n",
    "            theline.append(vi+\" \")\n",
    "        # print(\"({},{})\".format(ni,vi))\n",
    "    \n",
    "    \n",
    "def nx_createNxGraphInMem(G):\n",
    "    \"\"\"\n",
    "        Transform to networkx dataset\n",
    "\n",
    "        possible formats: GML, Adjacency matrix, ..\n",
    "        start by Adjcency list \n",
    "             --> (ignoring edge/node features)\n",
    "             --> line format: source target target2 target3 ... \n",
    "        later we can improve this...\n",
    "    \"\"\"\n",
    "    g = nx.MultiGraph()\n",
    "   \n",
    "    for ei in range(G.edge_index.size()[1]):    \n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        g.add_edge(ni,vi)\n",
    "    return g\n",
    "    \n",
    "def nx_verifyEdges(G, g):\n",
    "    for ei in range(G.edge_index.size()[1]):\n",
    "        ni = str(G.edge_index[0,ei].item())\n",
    "        vi = str(G.edge_index[1,ei].item())\n",
    "        if (ni,vi,0) not in list(g.edges):\n",
    "            if (vi,ni,1) not in list(g.edges):\n",
    "                print(\"Error {} not in networkx graph\".format((ni,vi)))\n",
    "            \n",
    "        \n",
    "\n",
    "def nx_compute_edge_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.edge_betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "def nx_compute_node_betweenness(G):\n",
    "    \n",
    "    #print(list(G.edges)[:10])\n",
    "    G_components = nx.connected_component_subgraphs(G)\n",
    "    G_mc = list(G_components)[0]  \n",
    "    eb_dict_res = {}\n",
    "    eb_dict = nx.betweenness_centrality(G_mc)\n",
    "    \n",
    "    # if there are more connected components...\n",
    "    if len(list(G_components))>1:\n",
    "        print(\"WARNING connected components: \",len(list(G_components)))\n",
    "    \n",
    "    eb_dict_res.update(eb_dict)\n",
    "    \n",
    "        \n",
    "    return eb_dict_res\n",
    "\n",
    "\n",
    "def update_edge_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \n",
    "        G.edge_attr must contain the edge betweenness values \n",
    "        for each edge\n",
    "        \n",
    "        G.y must contain it also.. (it is a copy of the edge betweenness..)\n",
    "        this could help the training phase\n",
    "        \n",
    "        Size restrictions:\n",
    "        - Given the size of the graphs, is it better to just transform the \n",
    "        object instead to write a new one?\n",
    "        - also just use G.y? but for GNN algorithms..not sure\n",
    "        \n",
    "        new_edg_attr will be size [num edges, 1]\n",
    "        and must be sorted in accordance to G.edge_index\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    new_edg_attr = []\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            #print((ni,vi))\n",
    "            try:\n",
    "                new_edg_attr.append([eb_dict[(ni,vi)]])\n",
    "            except:\n",
    "                try:\n",
    "                    new_edg_attr.append([eb_dict[(vi,ni)]])\n",
    "                except:\n",
    "                    #print(\"ERROR {} and {} not found!\".format((ni,vi),(vi,ni)))\n",
    "                    new_edg_attr.append([0])\n",
    "        else:\n",
    "            new_edg_attr.append([0])\n",
    "\n",
    "    new_edg_attr = torch.FloatTensor(new_edg_attr)\n",
    "    \n",
    "    #newG = Data(\n",
    "    #    x=G.x, \n",
    "    #    edge_index=G.edge_index, \n",
    "    #    edge_attr=new_edg_attr,\n",
    "    #    y=new_edg_attr)\n",
    "    \n",
    "    #G.edge_attr = new_edg_attr\n",
    "    G.y = new_edg_attr\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def update_node_betweenness(G, eb_dict):\n",
    "    \"\"\"\n",
    "        Get nodes keys from eb_dict and get their betweenness centrality\n",
    "        G.y will have all centralities of al lnodes following the order\n",
    "        of a list of the nodes sorted by id\n",
    "\n",
    "        add spaces in between!\n",
    "\n",
    "    \"\"\"\n",
    "    betweennesses = []\n",
    "    nodes = sorted([int(k) for k in eb_dict.keys()])\n",
    "    for node in range(nodes[-1]+1):\n",
    "        try:\n",
    "            betweennesses.append(eb_dict[str(node+1)])    \n",
    "        except:\n",
    "            betweennesses.append(0.0)\n",
    "            \n",
    "    G.y = torch.FloatTensor(betweennesses)\n",
    "    return G\n",
    "\n",
    "def get_betweenness_into_dict(G):\n",
    "    \"\"\"\n",
    "        FOR UNDIRECTED GRAPHS\n",
    "    \"\"\"\n",
    "    \n",
    "    eb_dict ={}\n",
    "    for i in range(len(G.edge_index[0])):\n",
    "        ni = G.edge_index[0][i]\n",
    "        vi = G.edge_index[1][i]\n",
    "        \n",
    "        if ni and vi:\n",
    "            ni=str(ni.item())\n",
    "            vi=str(vi.item())\n",
    "            eb_dict[(ni,vi)] = float(G.y[i].item())\n",
    "    return eb_dict\n",
    "\n",
    "\n",
    "def pyTorchGeometricDatasetToNx(G,suffix=0):\n",
    "    \"\"\"\n",
    "        Alternatives:\n",
    "            - to disk, to nx, then dict of betweenness\n",
    "            - transform in memory\n",
    "            - directly pickle a G object with the betweenness\n",
    "    \"\"\"\n",
    "    prefix = 'temp_aj_m'\n",
    "    # 1. PyTorch Geometric graph -> nx -> compute betweenness \n",
    "    #             -> PyTorch Geom with target the betweenness-------\n",
    "    # Transform to networkx graph\n",
    "    # write to adjacency matrix on disk\n",
    "    writeAdjacencyMatrixToDisk(G, filename=prefix+str(suffix)+'.txt')\n",
    "\n",
    "    # load into a networkx graph object\n",
    "    g2 = nx.read_adjlist(prefix+str(suffix)+'.txt')\n",
    "    #g2 = nx_createNxGraphInMem(G)\n",
    "    \n",
    "    return g2\n",
    "\n",
    "def computeBetweenness(G,suffix=0):\n",
    "    \"\"\"\n",
    "        Alternatives:\n",
    "            - to disk, to nx, then dict of betweenness\n",
    "            - transform in memory\n",
    "            - directly pickle a G object with the betweenness\n",
    "    \"\"\"\n",
    "    prefix = 'temp_aj_m'\n",
    "    # 1. PyTorch Geometric graph -> nx -> compute betweenness \n",
    "    #             -> PyTorch Geom with target the betweenness-------\n",
    "    # Transform to networkx graph\n",
    "    # write to adjacency matrix on disk\n",
    "    writeAdjacencyMatrixToDisk(G, filename=prefix+str(suffix)+'.txt')\n",
    "\n",
    "    # load into a networkx graph object\n",
    "    g2 = nx.read_adjlist(prefix+str(suffix)+'.txt')\n",
    "    #g2 = nx_createNxGraphInMem(G)\n",
    "\n",
    "    # compute node betweenness centrality\n",
    "    eb_dict = nx_compute_node_betweenness(g2)\n",
    "    #print(\"eb_dict\",eb_dict)\n",
    "    \n",
    "    # write node betweenness back to PyTorch Geometric graph\n",
    "    update_node_betweenness(G,eb_dict)\n",
    "    #return G\n",
    "    \n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        #print(\"root \", root, \" name \", name)\n",
    "        #print(\"setting a self.name in the object!\")\n",
    "        #self.name = name\n",
    "        #print(dir(self))\n",
    "        #super(MyOwnDataset, self).__init__(root,transform, pre_transform)\n",
    "        #self.data = torch.load(self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['PPI0.pickle']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['PPI0']\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        return True\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        \n",
    "        # unpickle the graph\n",
    "        print(\"going to unpickle\")\n",
    "        f = open(self.name, 'rb')\n",
    "        G = pickle.load(f) \n",
    "        #data_list = [G.x, G.edge_index, G.test_mask, G.train_mask, G.val_mask, G.y, G.batch]\n",
    "        f.close()\n",
    "        \n",
    "        #if self.pre_filter is not None:\n",
    "        #    data_list [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        #if self.pre_transform is not None:\n",
    "        #    data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        #data, slices = self.collate(data_list)\n",
    "        torch.save(G, self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "class MyOwnDataset2():\n",
    "    def __init__(self,  name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        f.close()\n",
    "    \n",
    "def loadDataset(collection, name=None, split=None):\n",
    "    # import datasets\n",
    "    themodule = importlib.import_module(\"torch_geometric.datasets\")\n",
    "    # get the function corresponding to collection\n",
    "    method_to_call = getattr(themodule, collection)\n",
    "    try:\n",
    "        if name:\n",
    "            return method_to_call(root='./data/'+str(collection), name=name)\n",
    "        elif split:\n",
    "            return method_to_call(root='./data/'+str(collection), split=split)\n",
    "        else:\n",
    "            return method_to_call(root='./data/'+str(collection)) \n",
    "    except:\n",
    "        traceback.print_tb()\n",
    "        if name:\n",
    "            return method_to_call( name=name)\n",
    "        elif split:\n",
    "            return method_to_call(split=split)\n",
    "        else:\n",
    "            return method_to_call()         \n",
    "        \n",
    "    \n",
    "def createDataset(x, edge_index):\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "\n",
    "def createDatasetFromNX(g, xlen, undirected=True):\n",
    "    # get edge list\n",
    "    edges = g.edges\n",
    "    edge_list_1 = []\n",
    "    edge_list_2 = []\n",
    "    for e in edges:\n",
    "        # node id must be an int\n",
    "        edge_list_1.append(int(e[0])) \n",
    "        edge_list_2.append(int(e[1]))\n",
    "        if undirected:\n",
    "            edge_list_1.append(int(e[1])) \n",
    "            edge_list_2.append(int(e[0]))\n",
    "            \n",
    "        \n",
    "    edge_index = torch.tensor([ edge_list_1,\n",
    "                                edge_list_2], dtype=torch.long)\n",
    "    \n",
    "    # create single 1 feature for each node\n",
    "    n = xlen\n",
    "    x = [[1.0] for i in range(n)]\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    return createDataset(x, edge_index)\n",
    "                         \n",
    "def createDatasetFromNXwithTarget(g,y,xlen, undirected=True):\n",
    "    dataset =  createDatasetFromNX(g,xlen, undirected)\n",
    "    y = torch.FloatTensor(y)\n",
    "    dataset.y = y \n",
    "    return dataset\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GraphSAGE TF Graph datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "import random\n",
    "\n",
    "\n",
    "def  edgeToNodeTransform(g,verif=False):\n",
    "    \"\"\"\n",
    "    # transform:\n",
    "        #  1-every edge is a node\n",
    "        #  2.every edge that share a node creates a link between them\n",
    "        #ยบ 3- id_map with an ide for each \"edge_node\"\n",
    "        #  3- class_map with edge-betwweennes and id\n",
    "        \n",
    "        \n",
    "    1) create a new graph g2\n",
    "    2) iter edge of g \n",
    "    2.1) and add as node in g2\n",
    "    2.2) iter edges of g looking for node in original edge, \n",
    "         add them as nodes in g2 and add links in g2\n",
    "        \n",
    "    \"\"\"\n",
    "    g2 = nx.Graph()\n",
    "    i=0\n",
    "    for e in g.edges_iter():\n",
    "        g2.add_node(i)\n",
    "        g2.node[i]['old_edge']=e\n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "    for n in g2.nodes():\n",
    "        # now for each node in the old_edge\n",
    "        # find other edges that are neighbors\n",
    "        # and add the corresponding new edge in g2\n",
    "\n",
    "        e = g2.node[n]['old_edge']\n",
    "        for n2 in g2.nodes():\n",
    "            if n==n2:\n",
    "                continue\n",
    "            e2 = g2.node[n2]['old_edge']\n",
    "            if e2[0] == e[0] or e2[1] == e[0] \\\n",
    "               or e2[0] == e[1] or e2[1] == e[1]:\n",
    "                g2.add_edge(n,n2)\n",
    "                # this will add 2 times n,n2 and n2,n edges...\n",
    "                # should we remove it?\n",
    "        \n",
    "    return g2\n",
    "\n",
    "def createGraphSAGEDataset(g,dname,betweenness, undirected=True):\n",
    "    \n",
    "    # compute its node and edge betweenness\n",
    "    y =[]\n",
    "    x=[]\n",
    "    xlen=len(g.nodes())\n",
    "    \n",
    "    \n",
    "    class_map = {}\n",
    "    id_map ={}\n",
    "    i = 0\n",
    "    \n",
    "    if betweenness == 'node':\n",
    "        nx_betweenness = nx.betweenness_centrality(g)\n",
    "        #y = [ v for k,v in nx_betweenness.items()]\n",
    "        y = []\n",
    "        for k,v in nx_betweenness.items():\n",
    "            y.append(v)\n",
    "            id_map[str(k)] = i\n",
    "            class_map[str(k)] = v\n",
    "            i+=1\n",
    "    elif betweenness == 'classes':\n",
    "        nx_betweenness = nx.betweenness_centrality(g)\n",
    "        \n",
    "        # discretize in 10 classes and assign \n",
    "        \n",
    "        y = []\n",
    "        for k,v in nx_betweenness.items():\n",
    "            y.append(v)\n",
    "            theclass = [0]*10\n",
    "            #print(theclass)\n",
    "            if v*1000 > 9.0:\n",
    "                theclass[9]=1\n",
    "            elif v*1000 > 8.0:\n",
    "                theclass[8]=1\n",
    "            elif v*1000 > 7.0:\n",
    "                theclass[7]=1\n",
    "            elif v*1000 > 6.0:\n",
    "                theclass[6]=1\n",
    "            elif v*1000 > 5.0:\n",
    "                theclass[5]=1\n",
    "            elif v*1000 > 4.0:\n",
    "                theclass[4]=1\n",
    "            elif v*1000 > 3.0:\n",
    "                theclass[3]=1\n",
    "            elif v*1000 > 2.0:\n",
    "                theclass[2]=1\n",
    "            elif v*1000 > 1.0:\n",
    "                theclass[1]=1\n",
    "            id_map[str(k)] = i\n",
    "            class_map[str(k)] = theclass\n",
    "            i+=1\n",
    "        \n",
    "    else:\n",
    "        nx_edge_betweenness = nx.edge_betweenness_centrality(g)\n",
    "        \n",
    "        # this part needs a rethinking, edge_betweenness goest to edges\n",
    "        #class_map[str(k)] = v\n",
    "        # we would need to TRANSFORM the graph and then save the betweenness \n",
    "        # as a node betweenness\n",
    "        g2 = edgeToNodeTransform(g)\n",
    "        g = g2\n",
    "        \n",
    "        y = []\n",
    "        for n in g2.nodes():\n",
    "            id_map[str(n)] = n\n",
    "            class_map[str(n)] = nx_edge_betweenness[g2.node[n]['old_edge']]\n",
    "            y.append(class_map[str(n)])\n",
    "            \n",
    "        xlen=len(y)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # add 'val' and 'test' attributes to all nodes\n",
    "    for nd in g.nodes():\n",
    "        # random uniform with 3 values\n",
    "        node = g.node[nd]\n",
    "        node['val']=0\n",
    "        node['test']=0\n",
    "        node['train']=0\n",
    "        rand_elem = random.choice(['val','test','train'])\n",
    "        node[rand_elem]=1\n",
    "    \n",
    "    # translate into a Networkx graph json object\n",
    "    jg = json_graph.node_link_data(g)\n",
    "    \n",
    "    # and write to disk as a json object\n",
    "    with open(dname+\"-G.json\", \"w+\") as write_file:\n",
    "        json.dump(jg, write_file)\n",
    "    \n",
    "    # id_map\n",
    "    with open(dname+\"-id_map.json\", \"w+\") as write_file:\n",
    "        json.dump(id_map, write_file)\n",
    "    \n",
    "    # class_map\n",
    "    with open(dname+\"-class_map.json\", \"w+\") as write_file:\n",
    "        json.dump(class_map, write_file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test1\n",
      "[(0, 1), (1, 2), (2, 3)]\n",
      "[(0, 1), (1, 2)]\n",
      "\n",
      "test2\n",
      "[(0, 1), (0, 4), (1, 2), (2, 3), (3, 6), (4, 5), (5, 6)]\n",
      "[(0, 1), (0, 2), (1, 5), (2, 3), (3, 4), (4, 6), (5, 6)]\n",
      "\n",
      "test3\n",
      "[(0, 1), (0, 4), (0, 7), (1, 3), (3, 6), (3, 9), (4, 5), (5, 6), (7, 8), (8, 9)]\n",
      "nodes [(0, {'old_edge': (0, 1)}), (1, {'old_edge': (0, 4)}), (2, {'old_edge': (0, 7)}), (3, {'old_edge': (1, 3)}), (4, {'old_edge': (3, 6)}), (5, {'old_edge': (3, 9)}), (6, {'old_edge': (4, 5)}), (7, {'old_edge': (5, 6)}), (8, {'old_edge': (7, 8)}), (9, {'old_edge': (8, 9)})]\n",
      "[(0, 1), (0, 2), (0, 3), (1, 2), (1, 6), (2, 8), (3, 4), (3, 5), (4, 5), (4, 7), (5, 9), (6, 7), (8, 9)]\n"
     ]
    }
   ],
   "source": [
    "# test graph transform\n",
    "print(\"\\ntest1\")\n",
    "G = nx.Graph()\n",
    "G.add_path([0,1,2,3])\n",
    "print(list([e for e in G.edges_iter()]))\n",
    "G = edgeToNodeTransform(G)\n",
    "print(list([e for e in G.edges_iter()]))\n",
    "\n",
    "print(\"\\ntest2\")\n",
    "G = nx.Graph()\n",
    "G.add_path([0,1,2,3])\n",
    "\n",
    "G.add_path([4,5,6])\n",
    "G.add_edge(0,4)\n",
    "G.add_edge(3,6)\n",
    "print(list([e for e in G.edges_iter()]))\n",
    "G = edgeToNodeTransform(G)\n",
    "print(list([e for e in G.edges_iter()]))\n",
    "\n",
    "\n",
    "print(\"\\ntest3\")\n",
    "G = nx.Graph()\n",
    "G.add_path([0,1,3])\n",
    "\n",
    "G.add_path([4,5,6])\n",
    "G.add_edge(0,4)\n",
    "G.add_edge(3,6)\n",
    "\n",
    "G.add_path([7,8,9])\n",
    "G.add_edge(0,7)\n",
    "G.add_edge(3,9)\n",
    "\n",
    "print(list([e for e in G.edges_iter()]))\n",
    "G = edgeToNodeTransform(G)\n",
    "print(\"nodes\",G.nodes(data=True))\n",
    "print(list([e for e in G.edges_iter()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "red = nx.random_lobster(100, 0.9, 0.9)\n",
    "\n",
    "createGraphSAGEDataset(er,'./precomputed-graphSAGE-TF/er_100_0_15_eb','edge', undirected=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createGraphSAGEDataset(er,'./precomputed-graphSAGE-TF/er_100_0_15_nb_discrete','classes', undirected=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createGraphSAGEDataset(er,'er_100_0_15_nb','node')\n",
    "er = nx.erdos_renyi_graph(100, 0.45)\n",
    "createGraphSAGEDataset(er,'er_100_0_45_nb','node')\n",
    "er = nx.erdos_renyi_graph(1000, 0.15)\n",
    "createGraphSAGEDataset(er,'er_1000_0_15_nb','node')\n",
    "er = nx.erdos_renyi_graph(1000, 0.45)\n",
    "createGraphSAGEDataset(er,'er_1000_0_45_nb','node')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_30_3_0_1_nb','node')\n",
    "ws = nx.watts_strogatz_graph(100, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_100_3_0_1_nb','node')\n",
    "ws = nx.watts_strogatz_graph(1000, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_1000_3_0_1_nb','node')\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(1000, 10, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_1000_10_0_1_nb','node')\n",
    "\n",
    "\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "createGraphSAGEDataset(ba,'ba_100_5_nb','node')\n",
    "ba = nx.barabasi_albert_graph(1000, 5)\n",
    "createGraphSAGEDataset(ba,'ba_1000_5_nb','node')\n",
    "\n",
    "\n",
    "\n",
    "#er = nx.erdos_renyi_graph(4000, 0.15)\n",
    "#createGraphSAGEDataset(er,'er_4000_0_15_nb','node')\n",
    "#er = nx.erdos_renyi_graph(4000, 0.35)\n",
    "#createGraphSAGEDataset(er,'er_4000_0_35_nb','node')\n",
    "#ws = nx.watts_strogatz_graph(4000, 3, 0.1)\n",
    "#createGraphSAGEDataset(ws,'ws_4000_3_0_1_nb','node')\n",
    "#ws = nx.watts_strogatz_graph(4000, 20, 0.1)\n",
    "#createGraphSAGEDataset(ws,'ws_4000_20_0_1_nb','node')\n",
    "#ba = nx.barabasi_albert_graph(4000, 5)\n",
    "#createGraphSAGEDataset(ba,'ba_4000_5_nb','node')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# repeat with edge betweenness\n",
    "er = nx.erdos_renyi_graph(100, 0.15)\n",
    "createGraphSAGEDataset(er,'er_100_0_15_eb','edge', undirected=False)\n",
    "er = nx.erdos_renyi_graph(100, 0.45)\n",
    "createGraphSAGEDataset(er,'er_100_0_45_eb','edge', undirected=False)\n",
    "er = nx.erdos_renyi_graph(1000, 0.15)\n",
    "createGraphSAGEDataset(er,'er_1000_0_15_eb','edge', undirected=False)\n",
    "er = nx.erdos_renyi_graph(1000, 0.45)\n",
    "createGraphSAGEDataset(er,'er_1000_0_45_eb','edge', undirected=False)\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_30_3_0_1_eb','edge', undirected=False)\n",
    "ws = nx.watts_strogatz_graph(100, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_100_3_0_1_eb','edge', undirected=False)\n",
    "ws = nx.watts_strogatz_graph(1000, 3, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_1000_3_0_1_eb','edge', undirected=False)\n",
    "\n",
    "\n",
    "ws = nx.watts_strogatz_graph(1000, 10, 0.1)\n",
    "createGraphSAGEDataset(ws,'ws_1000_10_0_1_eb','edge', undirected=False)\n",
    "\n",
    "\n",
    "ba = nx.barabasi_albert_graph(100, 5)\n",
    "createGraphSAGEDataset(ba,'ba_100_5_eb','edge', undirected=False)\n",
    "ba = nx.barabasi_albert_graph(1000, 5)\n",
    "createGraphSAGEDataset(ba,'ba_1000_5_eb','edge', undirected=False)\n",
    "\n",
    "\n",
    "#er = nx.erdos_renyi_graph(4000, 0.15)\n",
    "#createRandomGraphDataset(er,'er_4000_0_15_eb','edge')\n",
    "#er = nx.erdos_renyi_graph(4000, 0.35)\n",
    "#createRandomGraphDataset(er,'er_4000_0_35_eb','edge')\n",
    "#ws = nx.watts_strogatz_graph(4000, 3, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_3_0_1_eb','edge')\n",
    "#ws = nx.watts_strogatz_graph(4000, 20, 0.1)\n",
    "#createRandomGraphDataset(ws,'ws_4000_20_0_1_eb','edge')\n",
    "#ba = nx.barabasi_albert_graph(4000, 5)\n",
    "#createRandomGraphDataset(ba,'ba_4000_5_eb','edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs for the inductive setting\n",
    "\n",
    "for i in range(20):\n",
    "    er = nx.erdos_renyi_graph(100, 0.15)\n",
    "    createGraphSAGEDataset(er,'ind_er_100_0_15_eb_i_'+str(i),'edge', undirected=False)\n",
    "    er = nx.erdos_renyi_graph(100, 0.45)\n",
    "    createGraphSAGEDataset(er,'ind_er_100_0_45_eb_i_'+str(i),'edge', undirected=False)\n",
    "    #er = nx.erdos_renyi_graph(1000, 0.35)\n",
    "    #createGraphSAGEDataset(er,'ind_er_1000_0_35_eb_i_'+str(i),'edge', undirected=False)\n",
    "\n",
    "    ws = nx.watts_strogatz_graph(30, 3, 0.1)\n",
    "    createGraphSAGEDataset(ws,'ws_30_3_0_1_eb_i_'+str(i),'edge', undirected=False)\n",
    "    ws = nx.watts_strogatz_graph(100, 3, 0.1)\n",
    "    createGraphSAGEDataset(ws,'ws_100_3_0_1_eb_i_'+str(i),'edge', undirected=False)\n",
    "    #ws = nx.watts_strogatz_graph(1000, 3, 0.1)\n",
    "    #createGraphSAGEDataset(ws,'ws_1000_3_0_1_eb','edge', undirected=False)\n",
    "    #ws = nx.watts_strogatz_graph(1000, 10, 0.1)\n",
    "    #createGraphSAGEDataset(ws,'ws_1000_10_0_1_eb_i_'+str(i),'edge', undirected=False)\n",
    "\n",
    "    ba = nx.barabasi_albert_graph(100, 5)\n",
    "    createGraphSAGEDataset(ba,'ba_100_5_eb_i_'+str(i),'edge', undirected=False)\n",
    "    #ba = nx.barabasi_albert_graph(1000, 5)\n",
    "    #createGraphSAGEDataset(ba,'ba_1000_5_eb','edge', undirected=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# plan\n",
    "# 1. get graph from PyTorch Geom\n",
    "# 2. transform to nx (pyTorchGeometricDatasetToNx)\n",
    "# 3. call createGraphSAGEDataset(ba,'ba_1000_5','edge') \n",
    "#      or createGraphSAGEDataset(ba,'ba_1000_5','node')\n",
    "\n",
    "\n",
    "def processDatasetsSingle(dname, dataset):\n",
    "    G = dataset.data\n",
    "    print(G)\n",
    "    g = pyTorchGeometricDatasetToNx(G,i)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    createGraphSAGEDataset(g,'./precomputed-graphSAGE-TF/'+dname+'_'+str(i)+'_eb','edge', undirected=False) \n",
    "    createGraphSAGEDataset(g,'./precomputed-graphSAGE-TF/'+dname+'_'+str(i)+'_nd','node') \n",
    "\n",
    "        \n",
    "def processDatasets(dname, dataset):\n",
    "    # set size of batch to total size of graph here\n",
    "    loader = DataLoader(dataset, shuffle=False)\n",
    "    i = 0\n",
    "    for G in loader:\n",
    "        print(G)\n",
    "        g = pyTorchGeometricDatasetToNx(G,i)\n",
    "        i+=1\n",
    "        print(i)\n",
    "        createGraphSAGEDataset(g,'./precomputed-graphSAGE-TF/'+dname+'_'+str(i)+'_eb','edge', undirected=False) \n",
    "        createGraphSAGEDataset(g,'./precomputed-graphSAGE-TF/'+dname+'_'+str(i)+'_nd','node') \n",
    "    \n",
    "\n",
    "#KarateClub\n",
    "print(\"\\nKarateClub\")\n",
    "dname='KarateClub'\n",
    "dataset = loadDataset(dname)\n",
    "processDatasets(dname, dataset)\n",
    "\n",
    "#ENZYMES FROM TUDataset\n",
    "print(\"\\nTUDataset EnZYMES\")\n",
    "dname='TUDataset'\n",
    "name='ENZYMES'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname+'_'+name, dataset)\n",
    "\n",
    "#PROTEINS FROM TUDataset\n",
    "print(\"\\nTUDataset PROTEINS\")\n",
    "dname='TUDataset'\n",
    "name='PROTEINS'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname+'_'+name, dataset)\n",
    "\n",
    "#QM7b\n",
    "print(\"\\QM7B QM7B\")\n",
    "dataset = loadDataset('QM7b')\n",
    "processDatasets(dname,dataset)\n",
    "\n",
    "# Planetoid Cora\n",
    "print(\"\\n Planetoid Cora\")\n",
    "dname='Planetoid'\n",
    "name='Cora'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname+'_'+name,dataset)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#MUTAG\n",
    "#print(\"\\n MUTAG\")\n",
    "#dname='Entities'\n",
    "#name='MUTAG'\n",
    "#dataset = loadDataset(dname,name)\n",
    "#processDatasets(dname+'_'+name,dataset)\n",
    "\n",
    "\n",
    "\n",
    "#PPI\n",
    "#print(\"\\PPI PPI\")\n",
    "#dname='PPI'\n",
    "#dataset = loadDataset(dname)\n",
    "#processDatasets(dname,dataset)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#PROTEINS FROM TUDataset\n",
    "print(\"\\nTUDataset PROTEINS\")\n",
    "dname='TUDataset'\n",
    "name='PROTEINS'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname+'_'+name, dataset)\n",
    "\n",
    "#QM7b\n",
    "print(\"\\QM7B QM7B\")\n",
    "dataset = loadDataset('QM7b')\n",
    "processDatasets(dname,dataset)\n",
    "\n",
    "# Planetoid Cora\n",
    "print(\"\\n Planetoid Cora\")\n",
    "dname='Planetoid'\n",
    "name='Cora'\n",
    "dataset = loadDataset(dname,name)\n",
    "processDatasets(dname+'_'+name,dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-tf",
   "language": "python",
   "name": "gnn-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
