{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFM-girvan-newman-linksage.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Lvx9wjvl0U25",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ]
    },
    {
      "metadata": {
        "id": "35WB7fDo0Miw",
        "colab_type": "code",
        "outputId": "460d48b3-3d31-4e58-b99e-3fb43e3a0cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "user = getpass('Github user')\n",
        "password = getpass('Github password')\n",
        "os.environ['GIT_AUTH'] = user + ':' + password\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Github user··········\n",
            "Github password··········\n",
            "Cloning into 'GNN-MThesis'...\n",
            "remote: Enumerating objects: 31008, done.\u001b[K\n",
            "remote: Counting objects: 100% (31008/31008), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2950/2950), done.\u001b[K\n",
            "remote: Total 31008 (delta 29105), reused 29931 (delta 28032), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (31008/31008), 89.78 MiB | 30.14 MiB/s, done.\n",
            "Resolving deltas: 100% (29105/29105), done.\n",
            "Checking out files: 100% (32398/32398), done.\n",
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Mar  7 11:03 .\n",
            "drwxr-xr-x 1 root root 4096 Mar  7 11:02 ..\n",
            "drwxr-xr-x 1 root root 4096 Feb 26 17:33 .config\n",
            "drwxr-xr-x 5 root root 4096 Mar  7 11:03 GNN-MThesis\n",
            "drwxr-xr-x 1 root root 4096 Feb 26 17:33 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "deqBaxts1H-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r GNN-MThesis\n",
        "!git clone https://$GIT_AUTH@github.com/presmerats/GNN-MThesis.git\n",
        "#!ls -la\n",
        "#!ls -la GNN-MThesis/src/girvan-newman\n",
        "!ls -la GNN-MThesis/src/girvan-newman/graphSAGE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62ua_WmW3j7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba152156-3288-4dea-f1db-9608e4b8e04b"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RThCxjD-CrmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a22fcad-7bba-44f8-cde6-910c00c33c4d"
      },
      "cell_type": "code",
      "source": [
        "#!ls -lah drive/My\\ Drive/MIRI-DS/TFM/Data/precomputed-graphSAGE-TF\n",
        "!ln -s ./drive/My\\ Drive/MIRI-DS/TFM/Data ./data\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ln: failed to create symbolic link './data/Data': Function not implemented\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nv8PRs84DP-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "88068ec4-3e43-425d-91e5-a7b77fafc7a8"
      },
      "cell_type": "code",
      "source": [
        "#!ls data/precomputed\n",
        "!ls data/precomputed-graphSAGE-TF"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ba_1000_5_nb-class_map.json\t\ter_100_0_15_nb-id.json\n",
            "ba_1000_5_nb-id.json\t\t\ter_100_0_15_nb.json\n",
            "ba_1000_5_nb.json\t\t\ter_100_0_45_eb-class_map.json\n",
            "ba_100_5_nb-class_map.json\t\ter_100_0_45_eb-id.json\n",
            "ba_100_5_nb-id.json\t\t\ter_100_0_45_nb-class_map.json\n",
            "ba_100_5_nb.json\t\t\ter_100_0_45_nb-id.json\n",
            "er_1000_0_15_eb-id.json\t\t\ter_100_0_45_nb.json\n",
            "er_1000_0_15_eb.json\t\t\tws_1000_10_0_1_nb-class_map.json\n",
            "er_1000_0_15_nb-class_map.json\t\tws_1000_10_0_1_nb-id.json\n",
            "er_1000_0_15_nb-id.json\t\t\tws_1000_10_0_1_nb.json\n",
            "er_1000_0_45_nb-class_map.json\t\tws_1000_3_0_1_nb-class_map.json\n",
            "er_1000_0_45_nb-id.json\t\t\tws_1000_3_0_1_nb-id.json\n",
            "er_100_0_15_eb-class_map.json\t\tws_1000_3_0_1_nb.json\n",
            "er_100_0_15_eb-id.json\t\t\tws_100_3_0_1_nb-class_map.json\n",
            "er_100_0_15_eb.json\t\t\tws_100_3_0_1_nb-id.json\n",
            "er_100_0_15_nb-class_map.json\t\tws_100_3_0_1_nb.json\n",
            "er_100_0_15_nb_discrete-class_map.json\tws_30_3_0_1_nb-class_map.json\n",
            "er_100_0_15_nb_discrete-id.json\t\tws_30_3_0_1_nb-id.json\n",
            "er_100_0_15_nb_discrete.json\t\tws_30_3_0_1_nb.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wfelWg9sKB6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "dceb6865-1c9f-4549-8c35-802687646aaa"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/williamleif/GraphSAGE.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GraphSAGE'...\n",
            "remote: Enumerating objects: 253, done.\u001b[K\n",
            "remote: Total 253 (delta 0), reused 0 (delta 0), pack-reused 253\u001b[K\n",
            "Receiving objects: 100% (253/253), 6.43 MiB | 3.89 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tlW6nwEyLSY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv ./GraphSAGE/graphsage ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LtKYmXepMZqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv ./GraphSAGE/example_data .\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8oSRxSJHLeiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a71458d2-bbed-4f30-f811-7af0a9fb1dfb"
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall networkx"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling networkx-2.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/networkx-2.2-py3.6.egg-info\n",
            "    /usr/local/lib/python3.6/dist-packages/networkx/*\n",
            "    /usr/local/share/doc/networkx-2.2/LICENSE.txt\n",
            "    /usr/local/share/doc/networkx-2.2/examples\n",
            "    /usr/local/share/doc/networkx-2.2/pip-delete-this-directory.txt\n",
            "    /usr/local/share/doc/networkx-2.2/requirements.txt\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled networkx-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gtfuZyO_Li3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "c6fa89d0-f599-489a-a04c-16efece04fd6"
      },
      "cell_type": "code",
      "source": [
        "!pip install networkx==1.11"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting networkx==1.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2c/e473e54afc9fae58dfa97066ef6709a7e35a1dd1c28c5a3842989322be00/networkx-1.11-py2.py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K    0% |▎                               | 10kB 10.8MB/s eta 0:00:01\r\u001b[K    1% |▌                               | 20kB 1.6MB/s eta 0:00:01\r\u001b[K    2% |▊                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    3% |█▎                              | 51kB 2.0MB/s eta 0:00:01\r\u001b[K    4% |█▌                              | 61kB 2.4MB/s eta 0:00:01\r\u001b[K    5% |█▊                              | 71kB 2.7MB/s eta 0:00:01\r\u001b[K    6% |██                              | 81kB 3.1MB/s eta 0:00:01\r\u001b[K    6% |██▎                             | 92kB 3.5MB/s eta 0:00:01\r\u001b[K    7% |██▌                             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K    8% |██▊                             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 122kB 4.1MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 133kB 4.0MB/s eta 0:00:01\r\u001b[K    10% |███▌                            | 143kB 7.6MB/s eta 0:00:01\r\u001b[K    11% |███▊                            | 153kB 7.6MB/s eta 0:00:01\r\u001b[K    12% |████                            | 163kB 7.6MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    13% |████▌                           | 184kB 7.7MB/s eta 0:00:01\r\u001b[K    14% |████▊                           | 194kB 7.7MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 204kB 36.6MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 215kB 8.4MB/s eta 0:00:01\r\u001b[K    17% |█████▌                          | 225kB 8.4MB/s eta 0:00:01\r\u001b[K    17% |█████▊                          | 235kB 8.5MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 245kB 8.5MB/s eta 0:00:01\r\u001b[K    19% |██████▏                         | 256kB 8.5MB/s eta 0:00:01\r\u001b[K    20% |██████▌                         | 266kB 8.3MB/s eta 0:00:01\r\u001b[K    20% |██████▊                         | 276kB 8.4MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 286kB 8.4MB/s eta 0:00:01\r\u001b[K    22% |███████▏                        | 296kB 8.4MB/s eta 0:00:01\r\u001b[K    23% |███████▌                        | 307kB 8.6MB/s eta 0:00:01\r\u001b[K    24% |███████▊                        | 317kB 41.2MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 327kB 41.4MB/s eta 0:00:01\r\u001b[K    25% |████████▏                       | 337kB 43.2MB/s eta 0:00:01\r\u001b[K    26% |████████▌                       | 348kB 39.2MB/s eta 0:00:01\r\u001b[K    27% |████████▊                       | 358kB 39.0MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 368kB 44.3MB/s eta 0:00:01\r\u001b[K    28% |█████████▏                      | 378kB 43.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▌                      | 389kB 44.1MB/s eta 0:00:01\r\u001b[K    30% |█████████▊                      | 399kB 44.4MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 409kB 44.0MB/s eta 0:00:01\r\u001b[K    31% |██████████▏                     | 419kB 45.1MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 430kB 44.7MB/s eta 0:00:01\r\u001b[K    33% |██████████▊                     | 440kB 44.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 450kB 11.2MB/s eta 0:00:01\r\u001b[K    34% |███████████▏                    | 460kB 11.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▍                    | 471kB 11.1MB/s eta 0:00:01\r\u001b[K    36% |███████████▊                    | 481kB 11.0MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 491kB 10.9MB/s eta 0:00:01\r\u001b[K    38% |████████████▏                   | 501kB 11.0MB/s eta 0:00:01\r\u001b[K    38% |████████████▍                   | 512kB 10.8MB/s eta 0:00:01\r\u001b[K    39% |████████████▊                   | 522kB 10.8MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 532kB 10.8MB/s eta 0:00:01\r\u001b[K    41% |█████████████▏                  | 542kB 10.8MB/s eta 0:00:01\r\u001b[K    41% |█████████████▍                  | 552kB 43.1MB/s eta 0:00:01\r\u001b[K    42% |█████████████▊                  | 563kB 43.9MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 573kB 44.8MB/s eta 0:00:01\r\u001b[K    44% |██████████████▏                 | 583kB 46.0MB/s eta 0:00:01\r\u001b[K    45% |██████████████▍                 | 593kB 46.9MB/s eta 0:00:01\r\u001b[K    45% |██████████████▊                 | 604kB 47.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 614kB 50.4MB/s eta 0:00:01\r\u001b[K    47% |███████████████▏                | 624kB 49.8MB/s eta 0:00:01\r\u001b[K    48% |███████████████▍                | 634kB 50.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 645kB 50.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 655kB 49.3MB/s eta 0:00:01\r\u001b[K    50% |████████████████▏               | 665kB 40.2MB/s eta 0:00:01\r\u001b[K    51% |████████████████▍               | 675kB 40.4MB/s eta 0:00:01\r\u001b[K    52% |████████████████▋               | 686kB 40.7MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 696kB 40.7MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▏              | 706kB 40.2MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 716kB 40.7MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▋              | 727kB 41.0MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 737kB 40.6MB/s eta 0:00:01\r\u001b[K    56% |██████████████████▏             | 747kB 41.0MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▍             | 757kB 40.8MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 768kB 51.0MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 778kB 48.9MB/s eta 0:00:01\r\u001b[K    59% |███████████████████▏            | 788kB 15.3MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▍            | 798kB 15.1MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 808kB 15.1MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 819kB 14.8MB/s eta 0:00:01\r\u001b[K    62% |████████████████████▏           | 829kB 14.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 839kB 14.6MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 849kB 14.6MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 860kB 13.9MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▏          | 870kB 13.8MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▍          | 880kB 14.0MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 890kB 37.3MB/s eta 0:00:01\r\u001b[K    68% |█████████████████████▉          | 901kB 38.3MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▏         | 911kB 39.2MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▍         | 921kB 40.2MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 931kB 40.9MB/s eta 0:00:01\r\u001b[K    71% |██████████████████████▉         | 942kB 41.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▏        | 952kB 41.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▍        | 962kB 48.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 972kB 49.1MB/s eta 0:00:01\r\u001b[K    74% |███████████████████████▉        | 983kB 47.8MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▏       | 993kB 48.3MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▍       | 1.0MB 48.8MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 1.0MB 49.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 1.0MB 49.4MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 1.0MB 48.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▍      | 1.0MB 50.4MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▋      | 1.1MB 50.6MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 1.1MB 49.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 1.1MB 50.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▍     | 1.1MB 51.7MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▋     | 1.1MB 44.4MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 1.1MB 43.8MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 1.1MB 43.3MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▍    | 1.1MB 43.2MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▋    | 1.1MB 43.9MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 1.1MB 43.4MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 1.2MB 43.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▍   | 1.2MB 43.1MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▋   | 1.2MB 42.8MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 1.2MB 42.9MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 1.2MB 48.5MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▍  | 1.2MB 49.5MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▋  | 1.2MB 49.1MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 1.2MB 49.7MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████  | 1.2MB 49.8MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▎ | 1.2MB 49.2MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 1.3MB 50.0MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 1.3MB 49.9MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████ | 1.3MB 49.4MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▎| 1.3MB 49.8MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▋| 1.3MB 50.0MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 1.3MB 49.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 1.3MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from networkx==1.11) (4.3.2)\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: networkx\n",
            "Successfully installed networkx-1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CwxX3JxjKFXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE testing"
      ]
    },
    {
      "metadata": {
        "id": "LXLd07dkQKdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test 0**\n",
        "- run graphSAGE from command line"
      ]
    },
    {
      "metadata": {
        "id": "fZAf6FKZKKFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "35474f4d-b26d-40c7-a25a-db3b285a70a5"
      },
      "cell_type": "code",
      "source": [
        "!ls ./example_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ppi-class_map.json  ppi-walks.txt\t    toy-ppi-id_map.json\n",
            "ppi-feats.npy\t    toy-ppi-class_map.json  toy-ppi-walks.txt\n",
            "ppi-G.json\t    toy-ppi-feats.npy\n",
            "ppi-id_map.json     toy-ppi-G.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "neRRI-JYMryL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp ./example_data/toy-ppi-class_map.json ./example_data/ppi-class_map.json\n",
        "!cp ./example_data/toy-ppi-feats.npy ./example_data/ppi-feats.npy\n",
        "!cp ./example_data/toy-ppi-G.json ./example_data/ppi-G.json\n",
        "!cp ./example_data/toy-ppi-id_map.json ./example_data/ppi-id_map.json\n",
        "!cp ./example_data/toy-ppi-walks.txt ./example_data/ppi-walks.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aqWo-G1gMUsb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        },
        "outputId": "e23eac1e-e51f-4b97-a6ee-5138318ee8bb"
      },
      "cell_type": "code",
      "source": [
        "!python -m graphsage.supervised_train --train_prefix ./example_data/ppi --model graphsage_mean --sigmoid"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data..\n",
            "Removed 0 nodes that lacked proper annotations due to networkx versioning issues\n",
            "Loaded data.. now preprocessing..\n",
            "Done loading training data..\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/graphsage/aggregators.py:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019-03-07 12:58:32.800990: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-03-07 12:58:32.801399: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ba8aa0 executing computations on platform Host. Devices:\n",
            "2019-03-07 12:58:32.801440: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-03-07 12:58:32.838302: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2019-03-07 12:58:32.838371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: a9315e0d9123\n",
            "2019-03-07 12:58:32.838397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: a9315e0d9123\n",
            "2019-03-07 12:58:32.838461: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 410.79.0\n",
            "2019-03-07 12:58:32.838519: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 410.79.0\n",
            "2019-03-07 12:58:32.838562: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version seems to match DSO: 410.79.0\n",
            "Epoch: 0001\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "Iter: 0000 train_loss= 0.69505 train_f1_mic= 0.35924 train_f1_mac= 0.30955 val_loss= 0.66970 val_f1_mic= 0.35150 val_f1_mac= 0.13677 time= 0.60760\n",
            "Iter: 0005 train_loss= 0.58748 train_f1_mic= 0.37589 train_f1_mac= 0.09386 val_loss= 0.66970 val_f1_mic= 0.35150 val_f1_mac= 0.13677 time= 0.19961\n",
            "Iter: 0010 train_loss= 0.54748 train_f1_mic= 0.39185 train_f1_mac= 0.09869 val_loss= 0.66970 val_f1_mic= 0.35150 val_f1_mac= 0.13677 time= 0.16144\n",
            "Iter: 0015 train_loss= 0.54677 train_f1_mic= 0.37365 train_f1_mac= 0.08938 val_loss= 0.66970 val_f1_mic= 0.35150 val_f1_mac= 0.13677 time= 0.14658\n",
            "Epoch: 0002\n",
            "Iter: 0001 train_loss= 0.54168 train_f1_mic= 0.38883 train_f1_mac= 0.09746 val_loss= 0.58059 val_f1_mic= 0.39054 val_f1_mac= 0.10407 time= 0.14262\n",
            "Iter: 0006 train_loss= 0.54341 train_f1_mic= 0.39475 train_f1_mac= 0.10454 val_loss= 0.58059 val_f1_mic= 0.39054 val_f1_mac= 0.10407 time= 0.13742\n",
            "Iter: 0011 train_loss= 0.53761 train_f1_mic= 0.38597 train_f1_mac= 0.10029 val_loss= 0.58059 val_f1_mic= 0.39054 val_f1_mac= 0.10407 time= 0.13360\n",
            "Iter: 0016 train_loss= 0.52666 train_f1_mic= 0.40698 train_f1_mac= 0.11349 val_loss= 0.58059 val_f1_mic= 0.39054 val_f1_mac= 0.10407 time= 0.13108\n",
            "Epoch: 0003\n",
            "Iter: 0002 train_loss= 0.53438 train_f1_mic= 0.41041 train_f1_mac= 0.12432 val_loss= 0.54478 val_f1_mic= 0.40932 val_f1_mac= 0.12697 time= 0.13055\n",
            "Iter: 0007 train_loss= 0.53133 train_f1_mic= 0.41272 train_f1_mac= 0.12816 val_loss= 0.54478 val_f1_mic= 0.40932 val_f1_mac= 0.12697 time= 0.12898\n",
            "Iter: 0012 train_loss= 0.53462 train_f1_mic= 0.42017 train_f1_mac= 0.14297 val_loss= 0.54478 val_f1_mic= 0.40932 val_f1_mac= 0.12697 time= 0.12765\n",
            "Iter: 0017 train_loss= 0.52235 train_f1_mic= 0.42469 train_f1_mac= 0.14349 val_loss= 0.54478 val_f1_mic= 0.40932 val_f1_mac= 0.12697 time= 0.12662\n",
            "Epoch: 0004\n",
            "Iter: 0003 train_loss= 0.51977 train_f1_mic= 0.43740 train_f1_mac= 0.15662 val_loss= 0.55725 val_f1_mic= 0.42770 val_f1_mac= 0.16322 time= 0.12684\n",
            "Iter: 0008 train_loss= 0.52090 train_f1_mic= 0.43213 train_f1_mac= 0.16903 val_loss= 0.55725 val_f1_mic= 0.42770 val_f1_mac= 0.16322 time= 0.12591\n",
            "Iter: 0013 train_loss= 0.51613 train_f1_mic= 0.44391 train_f1_mac= 0.17873 val_loss= 0.55725 val_f1_mic= 0.42770 val_f1_mac= 0.16322 time= 0.12502\n",
            "Iter: 0018 train_loss= 0.52349 train_f1_mic= 0.43889 train_f1_mac= 0.16564 val_loss= 0.55725 val_f1_mic= 0.42770 val_f1_mac= 0.16322 time= 0.12442\n",
            "Epoch: 0005\n",
            "Iter: 0004 train_loss= 0.51203 train_f1_mic= 0.48311 train_f1_mac= 0.22768 val_loss= 0.53414 val_f1_mic= 0.48168 val_f1_mac= 0.22674 time= 0.12458\n",
            "Iter: 0009 train_loss= 0.51590 train_f1_mic= 0.47224 train_f1_mac= 0.23102 val_loss= 0.53414 val_f1_mic= 0.48168 val_f1_mac= 0.22674 time= 0.12394\n",
            "Iter: 0014 train_loss= 0.51557 train_f1_mic= 0.44917 train_f1_mac= 0.19437 val_loss= 0.53414 val_f1_mic= 0.48168 val_f1_mac= 0.22674 time= 0.12346\n",
            "Epoch: 0006\n",
            "Iter: 0000 train_loss= 0.50891 train_f1_mic= 0.46809 train_f1_mac= 0.20837 val_loss= 0.52231 val_f1_mic= 0.46453 val_f1_mac= 0.23418 time= 0.12374\n",
            "Iter: 0005 train_loss= 0.50703 train_f1_mic= 0.47151 train_f1_mac= 0.23248 val_loss= 0.52231 val_f1_mic= 0.46453 val_f1_mac= 0.23418 time= 0.12338\n",
            "Iter: 0010 train_loss= 0.51063 train_f1_mic= 0.45370 train_f1_mac= 0.20222 val_loss= 0.52231 val_f1_mic= 0.46453 val_f1_mac= 0.23418 time= 0.12308\n",
            "Iter: 0015 train_loss= 0.52465 train_f1_mic= 0.47463 train_f1_mac= 0.23418 val_loss= 0.52231 val_f1_mic= 0.46453 val_f1_mac= 0.23418 time= 0.12261\n",
            "Epoch: 0007\n",
            "Iter: 0001 train_loss= 0.49844 train_f1_mic= 0.47226 train_f1_mac= 0.24063 val_loss= 0.51414 val_f1_mic= 0.50532 val_f1_mac= 0.27702 time= 0.12273\n",
            "Iter: 0006 train_loss= 0.50913 train_f1_mic= 0.47191 train_f1_mac= 0.24830 val_loss= 0.51414 val_f1_mic= 0.50532 val_f1_mac= 0.27702 time= 0.12238\n",
            "Iter: 0011 train_loss= 0.49969 train_f1_mic= 0.51874 train_f1_mac= 0.29666 val_loss= 0.51414 val_f1_mic= 0.50532 val_f1_mac= 0.27702 time= 0.12206\n",
            "Iter: 0016 train_loss= 0.50814 train_f1_mic= 0.48167 train_f1_mac= 0.25207 val_loss= 0.51414 val_f1_mic= 0.50532 val_f1_mac= 0.27702 time= 0.12177\n",
            "Epoch: 0008\n",
            "Iter: 0002 train_loss= 0.50639 train_f1_mic= 0.46762 train_f1_mac= 0.24206 val_loss= 0.54051 val_f1_mic= 0.48374 val_f1_mac= 0.26101 time= 0.12207\n",
            "Iter: 0007 train_loss= 0.50764 train_f1_mic= 0.49488 train_f1_mac= 0.27238 val_loss= 0.54051 val_f1_mic= 0.48374 val_f1_mac= 0.26101 time= 0.12181\n",
            "Iter: 0012 train_loss= 0.50105 train_f1_mic= 0.48016 train_f1_mac= 0.24353 val_loss= 0.54051 val_f1_mic= 0.48374 val_f1_mac= 0.26101 time= 0.12157\n",
            "Iter: 0017 train_loss= 0.51075 train_f1_mic= 0.47074 train_f1_mac= 0.26088 val_loss= 0.54051 val_f1_mic= 0.48374 val_f1_mac= 0.26101 time= 0.12134\n",
            "Epoch: 0009\n",
            "Iter: 0003 train_loss= 0.51198 train_f1_mic= 0.50176 train_f1_mac= 0.27354 val_loss= 0.53167 val_f1_mic= 0.53975 val_f1_mac= 0.33509 time= 0.12153\n",
            "Iter: 0008 train_loss= 0.49942 train_f1_mic= 0.48930 train_f1_mac= 0.26802 val_loss= 0.53167 val_f1_mic= 0.53975 val_f1_mac= 0.33509 time= 0.12137\n",
            "Iter: 0013 train_loss= 0.50791 train_f1_mic= 0.48092 train_f1_mac= 0.26192 val_loss= 0.53167 val_f1_mic= 0.53975 val_f1_mac= 0.33509 time= 0.12117\n",
            "Iter: 0018 train_loss= 0.50043 train_f1_mic= 0.50132 train_f1_mac= 0.26338 val_loss= 0.53167 val_f1_mic= 0.53975 val_f1_mac= 0.33509 time= 0.12102\n",
            "Epoch: 0010\n",
            "Iter: 0004 train_loss= 0.48597 train_f1_mic= 0.50729 train_f1_mac= 0.28763 val_loss= 0.51065 val_f1_mic= 0.52000 val_f1_mac= 0.30406 time= 0.12122\n",
            "Iter: 0009 train_loss= 0.49198 train_f1_mic= 0.49179 train_f1_mac= 0.27731 val_loss= 0.51065 val_f1_mic= 0.52000 val_f1_mac= 0.30406 time= 0.12103\n",
            "Iter: 0014 train_loss= 0.48661 train_f1_mic= 0.49220 train_f1_mac= 0.27013 val_loss= 0.51065 val_f1_mic= 0.52000 val_f1_mac= 0.30406 time= 0.12085\n",
            "Optimization Finished!\n",
            "Full validation stats: loss= 0.50954 f1_micro= 0.52899 f1_macro= 0.33321 time= 0.40714\n",
            "Writing test set stats to file (don't peak!)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "drpWhxxYKM08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test 1:**\n",
        "- import graphsage modules\n",
        "- run a training"
      ]
    },
    {
      "metadata": {
        "id": "vWHOL8XSKT1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from graphsage import supervised_train as train\n",
        "\n",
        "# replicate : python -m graphsage.supervised_train --train_prefix ./example_data/ppi --model graphsage_mean --sigmoid\n",
        "# but from code\n",
        "import sys\n",
        "\n",
        "#print(sys.argv)\n",
        "#sys.argv = [\"prog\", \"-f\", \"/home/fenton/project/setup.py\"]\n",
        "sys.argv = [\"python\" ,\"graphsage.supervised_train\", \"--train_prefix\", \"./example_data/ppi\", \"--model\" ,\"graphsage_mean\" ,\"--sigmoid\"]\n",
        "#print(sys.argv)\n",
        "\n",
        "train.main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RK4sL0xsQZzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**test 3**\n",
        "- train graphSAGE from a different dataset"
      ]
    },
    {
      "metadata": {
        "id": "JcY1fDB9XOky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "7542b84c-aff0-46e1-8732-23d36b2ebded"
      },
      "cell_type": "code",
      "source": [
        "!ls ./data/precomputed-graphSAGE-TF"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ba_1000_5_nb-class_map.json\t\ter_100_0_15_nb_discrete-G.json\n",
            "ba_1000_5_nb-id.json\t\t\ter_100_0_15_nb_discrete-id_map.json\n",
            "ba_1000_5_nb.json\t\t\ter_100_0_15_nb-id.json\n",
            "ba_100_5_nb-class_map.json\t\ter_100_0_15_nb.json\n",
            "ba_100_5_nb-id.json\t\t\ter_100_0_45_eb-class_map.json\n",
            "ba_100_5_nb.json\t\t\ter_100_0_45_eb-id.json\n",
            "er_1000_0_15_eb-class_map.json\t\ter_100_0_45_eb.json\n",
            "er_1000_0_15_eb-id.json\t\t\ter_100_0_45_nb-class_map.json\n",
            "er_1000_0_15_eb.json\t\t\ter_100_0_45_nb-id.json\n",
            "er_1000_0_15_nb-class_map.json\t\ter_100_0_45_nb.json\n",
            "er_1000_0_15_nb-id.json\t\t\tws_1000_10_0_1_nb-class_map.json\n",
            "er_1000_0_15_nb.json\t\t\tws_1000_10_0_1_nb-id.json\n",
            "er_1000_0_45_nb-class_map.json\t\tws_1000_10_0_1_nb.json\n",
            "er_1000_0_45_nb-id.json\t\t\tws_1000_3_0_1_nb-class_map.json\n",
            "er_1000_0_45_nb.json\t\t\tws_1000_3_0_1_nb-id.json\n",
            "er_100_0_15_eb-class_map.json\t\tws_1000_3_0_1_nb.json\n",
            "er_100_0_15_eb-G.json\t\t\tws_100_3_0_1_nb-class_map.json\n",
            "er_100_0_15_eb-id.json\t\t\tws_100_3_0_1_nb-id.json\n",
            "er_100_0_15_eb-id_map.json\t\tws_100_3_0_1_nb.json\n",
            "er_100_0_15_eb.json\t\t\tws_30_3_0_1_nb-class_map.json\n",
            "er_100_0_15_nb-class_map.json\t\tws_30_3_0_1_nb-id.json\n",
            "er_100_0_15_nb_discrete-class_map.json\tws_30_3_0_1_nb.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Op81wYWZ4aJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv ./data/precomputed-graphSAGE-TF/er_100_0_15_nb_discrete-id.json ./data/precomputed-graphSAGE-TF/er_100_0_15_nb_discrete-id_map.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amhHiGj2Zmea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv ./data/precomputed-graphSAGE-TF/er_100_0_15_nb_discrete.json ./data/precomputed-graphSAGE-TF/er_100_0_15_nb_discrete-G.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpuAKwJ5QeLQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "#from graphsage import supervised_train as train\n",
        "#sys.argv = [\"python\" ,\"graphsage.supervised_train\", \"--train_prefix\", \"./example_data/ppi\", \"--model\" ,\"graphsage_mean\" ,\"--sigmoid\"]\n",
        "sys.argv = [\"python\" ,\"graphsage.supervised_train\", \"--train_prefix\", \"./data/precomputed-graphSAGE-TF/er_100_0_15_nb_discrete\", \n",
        "            \"--model\" ,\"graphsage_mean\",\n",
        "           \"--identity_dim\",\"1\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gh-fHpPYROI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "\n",
        "from graphsage.supervised_models import SupervisedGraphsage\n",
        "from graphsage.models import SAGEInfo\n",
        "from graphsage.minibatch import NodeMinibatchIterator\n",
        "from graphsage.neigh_samplers import UniformNeighborSampler\n",
        "from graphsage.utils import load_data\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
        "                            \"\"\"Whether to log device placement.\"\"\")\n",
        "\n",
        "#core params..\n",
        "flags.DEFINE_string('model', 'graphsage_mean', 'model names. See README for possible values.')  \n",
        "flags.DEFINE_float('learning_rate', 0.01, 'initial learning rate.')\n",
        "flags.DEFINE_string(\"model_size\", \"small\", \"Can be big or small; model specific def'ns\")\n",
        "flags.DEFINE_string('train_prefix', '', 'prefix identifying training data. must be specified.')\n",
        "\n",
        "# left to default values in main experiments \n",
        "flags.DEFINE_integer('epochs', 10, 'number of epochs to train.')\n",
        "flags.DEFINE_float('dropout', 0.0, 'dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 0.0, 'weight for l2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('max_degree', 128, 'maximum node degree.')\n",
        "flags.DEFINE_integer('samples_1', 25, 'number of samples in layer 1')\n",
        "flags.DEFINE_integer('samples_2', 10, 'number of samples in layer 2')\n",
        "flags.DEFINE_integer('samples_3', 0, 'number of users samples in layer 3. (Only for mean model)')\n",
        "flags.DEFINE_integer('dim_1', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
        "flags.DEFINE_integer('dim_2', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
        "flags.DEFINE_boolean('random_context', True, 'Whether to use random context or direct edges')\n",
        "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
        "flags.DEFINE_boolean('sigmoid', False, 'whether to use sigmoid loss')\n",
        "flags.DEFINE_integer('identity_dim', 0, 'Set to positive value to use identity embedding features of that dimension. Default 0.')\n",
        "\n",
        "#logging, saving, validation settings etc.\n",
        "flags.DEFINE_string('base_log_dir', '.', 'base directory for logging and saving embeddings')\n",
        "flags.DEFINE_integer('validate_iter', 5000, \"how often to run a validation minibatch.\")\n",
        "flags.DEFINE_integer('validate_batch_size', 256, \"how many nodes per validation sample.\")\n",
        "flags.DEFINE_integer('gpu', 1, \"which gpu to use.\")\n",
        "flags.DEFINE_integer('print_every', 5, \"How often to print training info.\")\n",
        "flags.DEFINE_integer('max_total_steps', 10**10, \"Maximum total number of iterations\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(FLAGS.gpu)\n",
        "\n",
        "GPU_MEM_FRACTION = 0.8\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sA5QMO9ZOWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1135
        },
        "outputId": "ea9b9176-2364-4baa-cfb5-4b5cd4694773"
      },
      "cell_type": "code",
      "source": [
        "def calc_f1(y_true, y_pred):\n",
        "    if not FLAGS.sigmoid:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    else:\n",
        "        y_pred[y_pred > 0.5] = 1\n",
        "        y_pred[y_pred <= 0.5] = 0\n",
        "    return metrics.f1_score(y_true, y_pred, average=\"micro\"), metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "# Define model evaluation function\n",
        "def evaluate(sess, model, minibatch_iter, size=None):\n",
        "    \"\"\"\n",
        "      This function will be changed into regreession evaluation\n",
        "      (mean squared error, or Root mean squared error)\n",
        "    \"\"\"\n",
        "    t_test = time.time()\n",
        "    feed_dict_val, labels = minibatch_iter.node_val_feed_dict(size)\n",
        "    node_outs_val = sess.run([model.preds, model.loss], \n",
        "                        feed_dict=feed_dict_val)\n",
        "    mic, mac = calc_f1(labels, node_outs_val[0])\n",
        "    return node_outs_val[1], mic, mac, (time.time() - t_test)\n",
        "  \n",
        "  \n",
        "\n",
        "def log_dir():\n",
        "    log_dir = FLAGS.base_log_dir + \"/sup-\" + FLAGS.train_prefix.split(\"/\")[-2]\n",
        "    log_dir += \"/{model:s}_{model_size:s}_{lr:0.4f}/\".format(\n",
        "            model=FLAGS.model,\n",
        "            model_size=FLAGS.model_size,\n",
        "            lr=FLAGS.learning_rate)\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def incremental_evaluate(sess, model, minibatch_iter, size, test=False):\n",
        "    t_test = time.time()\n",
        "    finished = False\n",
        "    val_losses = []\n",
        "    val_preds = []\n",
        "    labels = []\n",
        "    iter_num = 0\n",
        "    finished = False\n",
        "    while not finished:\n",
        "        feed_dict_val, batch_labels, finished, _  = minibatch_iter.incremental_node_val_feed_dict(size, iter_num, test=test)\n",
        "        node_outs_val = sess.run([model.preds, model.loss], \n",
        "                         feed_dict=feed_dict_val)\n",
        "        val_preds.append(node_outs_val[0])\n",
        "        labels.append(batch_labels)\n",
        "        val_losses.append(node_outs_val[1])\n",
        "        iter_num += 1\n",
        "    val_preds = np.vstack(val_preds)\n",
        "    labels = np.vstack(labels)\n",
        "    f1_scores = calc_f1(labels, val_preds)\n",
        "    return np.mean(val_losses), f1_scores[0], f1_scores[1], (time.time() - t_test)\n",
        "\n",
        "def construct_placeholders(num_classes):\n",
        "    # Define placeholders\n",
        "    placeholders = {\n",
        "        'labels' : tf.placeholder(tf.float32, shape=(None, num_classes), name='labels'),\n",
        "        'batch' : tf.placeholder(tf.int32, shape=(None), name='batch1'),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),\n",
        "        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),\n",
        "    }\n",
        "    return placeholders\n",
        "\n",
        "def train(train_data, test_data=None):\n",
        "\n",
        "    G = train_data[0]\n",
        "    features = train_data[1]\n",
        "    id_map = train_data[2]\n",
        "    class_map  = train_data[4]\n",
        "    #if isinstance(list(class_map.values())[0], list):\n",
        "    #    num_classes = len(list(class_map.values())[0])\n",
        "    #else:\n",
        "    #    num_classes = len(set(class_map.values()))\n",
        "    num_classes = 1\n",
        "\n",
        "    if not features is None:\n",
        "        # pad with dummy zero vector\n",
        "        features = np.vstack([features, np.zeros((features.shape[1],))])\n",
        "\n",
        "    context_pairs = train_data[3] if FLAGS.random_context else None\n",
        "    placeholders = construct_placeholders(num_classes)\n",
        "    minibatch = NodeMinibatchIterator(G, \n",
        "            id_map,\n",
        "            placeholders, \n",
        "            class_map,\n",
        "            num_classes,\n",
        "            batch_size=FLAGS.batch_size,\n",
        "            max_degree=FLAGS.max_degree, \n",
        "            context_pairs = context_pairs)\n",
        "    print(\"\\n\\n\\n**************mini batch created*****************\")\n",
        "    adj_info_ph = tf.placeholder(tf.int32, shape=minibatch.adj.shape)\n",
        "    adj_info = tf.Variable(adj_info_ph, trainable=False, name=\"adj_info\")\n",
        "    print(\"\\n\\n\\n**************adj_info created*****************\")\n",
        "    \n",
        "\n",
        "    if FLAGS.model == 'graphsage_mean':\n",
        "        # Create model\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        if FLAGS.samples_3 != 0:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_3, FLAGS.dim_2)]\n",
        "        elif FLAGS.samples_2 != 0:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "        else:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos, \n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "        print(\"\\n\\n\\n**************model created*****************\")\n",
        "    elif FLAGS.model == 'gcn':\n",
        "        # Create model\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, 2*FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, 2*FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"gcn\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     concat=False,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_seq':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"seq\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_maxpool':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                    features,\n",
        "                                    adj_info,\n",
        "                                    minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"maxpool\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_meanpool':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                    features,\n",
        "                                    adj_info,\n",
        "                                    minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"meanpool\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Error: model name unrecognized.')\n",
        "\n",
        "    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n",
        "    config.gpu_options.allow_growth = True\n",
        "    #config.gpu_options.per_process_gpu_memory_fraction = GPU_MEM_FRACTION\n",
        "    config.allow_soft_placement = True\n",
        "    \n",
        "    # Initialize session\n",
        "    sess = tf.Session(config=config)\n",
        "    merged = tf.summary.merge_all()\n",
        "    summary_writer = tf.summary.FileWriter(log_dir(), sess.graph)\n",
        "     \n",
        "    # Init variables\n",
        "    sess.run(tf.global_variables_initializer(), feed_dict={adj_info_ph: minibatch.adj})\n",
        "    \n",
        "    # Train model\n",
        "    \n",
        "    total_steps = 0\n",
        "    avg_time = 0.0\n",
        "    epoch_val_costs = []\n",
        "\n",
        "    train_adj_info = tf.assign(adj_info, minibatch.adj)\n",
        "    val_adj_info = tf.assign(adj_info, minibatch.test_adj)\n",
        "    for epoch in range(FLAGS.epochs): \n",
        "        minibatch.shuffle() \n",
        "\n",
        "        iter = 0\n",
        "        print('Epoch: %04d' % (epoch + 1))\n",
        "        epoch_val_costs.append(0)\n",
        "        while not minibatch.end():\n",
        "            # Construct feed dictionary\n",
        "            feed_dict, labels = minibatch.next_minibatch_feed_dict()\n",
        "            feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "            t = time.time()\n",
        "            # Training step\n",
        "            outs = sess.run([merged, model.opt_op, model.loss, model.preds], feed_dict=feed_dict)\n",
        "            train_cost = outs[2]\n",
        "\n",
        "            if iter % FLAGS.validate_iter == 0:\n",
        "                # Validation\n",
        "                sess.run(val_adj_info.op)\n",
        "                if FLAGS.validate_batch_size == -1:\n",
        "                    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
        "                else:\n",
        "                    val_cost, val_f1_mic, val_f1_mac, duration = evaluate(sess, model, minibatch, FLAGS.validate_batch_size)\n",
        "                sess.run(train_adj_info.op)\n",
        "                epoch_val_costs[-1] += val_cost\n",
        "\n",
        "            if total_steps % FLAGS.print_every == 0:\n",
        "                summary_writer.add_summary(outs[0], total_steps)\n",
        "    \n",
        "            # Print results\n",
        "            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
        "\n",
        "            if total_steps % FLAGS.print_every == 0:\n",
        "                train_f1_mic, train_f1_mac = calc_f1(labels, outs[-1])\n",
        "                print(\"Iter:\", '%04d' % iter, \n",
        "                      \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                      \"train_f1_mic=\", \"{:.5f}\".format(train_f1_mic), \n",
        "                      \"train_f1_mac=\", \"{:.5f}\".format(train_f1_mac), \n",
        "                      \"val_loss=\", \"{:.5f}\".format(val_cost),\n",
        "                      \"val_f1_mic=\", \"{:.5f}\".format(val_f1_mic), \n",
        "                      \"val_f1_mac=\", \"{:.5f}\".format(val_f1_mac), \n",
        "                      \"time=\", \"{:.5f}\".format(avg_time))\n",
        " \n",
        "            iter += 1\n",
        "            total_steps += 1\n",
        "\n",
        "            if total_steps > FLAGS.max_total_steps:\n",
        "                break\n",
        "\n",
        "        if total_steps > FLAGS.max_total_steps:\n",
        "                break\n",
        "    \n",
        "    print(\"Optimization Finished!\")\n",
        "    sess.run(val_adj_info.op)\n",
        "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
        "    print(\"Full validation stats:\",\n",
        "                  \"loss=\", \"{:.5f}\".format(val_cost),\n",
        "                  \"f1_micro=\", \"{:.5f}\".format(val_f1_mic),\n",
        "                  \"f1_macro=\", \"{:.5f}\".format(val_f1_mac),\n",
        "                  \"time=\", \"{:.5f}\".format(duration))\n",
        "    with open(log_dir() + \"val_stats.txt\", \"w\") as fp:\n",
        "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f} time={:.5f}\".\n",
        "                format(val_cost, val_f1_mic, val_f1_mac, duration))\n",
        "\n",
        "    print(\"Writing test set stats to file (don't peak!)\")\n",
        "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size, test=True)\n",
        "    with open(log_dir() + \"test_stats.txt\", \"w\") as fp:\n",
        "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f}\".\n",
        "                format(val_cost, val_f1_mic, val_f1_mac))\n",
        "\n",
        "        \n",
        "FLAGS.identity_dim = 1\n",
        "print(\"Loading training data..\")\n",
        "train_data = load_data(FLAGS.train_prefix)\n",
        "print(\"Done loading training data..\")\n",
        "train(train_data)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data..\n",
            "No features present.. Only identity features will be used.\n",
            "Removed 0 nodes that lacked proper annotations due to networkx versioning issues\n",
            "Loaded data.. now preprocessing..\n",
            "Done loading training data..\n",
            "\n",
            "\n",
            "\n",
            "**************mini batch created*****************\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "\n",
            "\n",
            "**************adj_info created*****************\n",
            "WARNING:tensorflow:From /content/graphsage/aggregators.py:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /content/graphsage/supervised_models.py:118: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "\n",
            "\n",
            "**************model created*****************\n",
            "Epoch: 0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter: 0000 train_loss= 2.27310 train_f1_mic= 0.29412 train_f1_mac= 0.05556 val_loss= 1.94367 val_f1_mic= 0.49219 val_f1_mac= 0.09424 time= 0.33363\n",
            "Epoch: 0002\n",
            "Epoch: 0003\n",
            "Epoch: 0004\n",
            "Epoch: 0005\n",
            "Epoch: 0006\n",
            "Iter: 0000 train_loss= 1.49931 train_f1_mic= 0.55882 train_f1_mac= 0.10243 val_loss= 1.82771 val_f1_mic= 0.50781 val_f1_mac= 0.09623 time= 0.07667\n",
            "Epoch: 0007\n",
            "Epoch: 0008\n",
            "Epoch: 0009\n",
            "Epoch: 0010\n",
            "Optimization Finished!\n",
            "Full validation stats: loss= 1.75045 f1_micro= 0.52381 f1_macro= 0.09821 time= 0.00526\n",
            "Writing test set stats to file (don't peak!)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dPm4ZTa4acrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "416f4533-e8ba-4050-e08a-1601f681b246"
      },
      "cell_type": "code",
      "source": [
        "print(dir(flags))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ArgumentParser', 'ArgumentSerializer', 'BaseListParser', 'BooleanFlag', 'BooleanParser', 'CantOpenFlagFileError', 'CsvListSerializer', 'DEFINE', 'DEFINE_alias', 'DEFINE_bool', 'DEFINE_boolean', 'DEFINE_enum', 'DEFINE_enum_class', 'DEFINE_flag', 'DEFINE_float', 'DEFINE_integer', 'DEFINE_list', 'DEFINE_multi', 'DEFINE_multi_enum', 'DEFINE_multi_enum_class', 'DEFINE_multi_float', 'DEFINE_multi_integer', 'DEFINE_multi_string', 'DEFINE_spaceseplist', 'DEFINE_string', 'DuplicateFlagError', 'EnumClassFlag', 'EnumClassParser', 'EnumFlag', 'EnumParser', 'Error', 'FLAGS', 'Flag', 'FlagNameConflictsWithMethodError', 'FlagValues', 'FloatParser', 'IllegalFlagValueError', 'IntegerParser', 'ListParser', 'ListSerializer', 'MultiEnumClassFlag', 'MultiFlag', 'UnparsedFlagAccessError', 'UnrecognizedFlagError', 'ValidationError', 'WhitespaceSeparatedListParser', '_FlagValuesWrapper', '_RENAMED_ARGUMENTS', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_logging', '_six', '_sys', '_wrap_define_function', 'absolute_import', 'adopt_module_key_flags', 'declare_key_flag', 'disclaim_key_flags', 'division', 'doc_to_help', 'flag_dict_to_args', 'get_help_width', 'getopt', 'mark_flag_as_required', 'mark_flags_as_mutual_exclusive', 'mark_flags_as_required', 'multi_flags_validator', 'os', 'print_function', 're', 'register_multi_flags_validator', 'register_validator', 'six', 'sys', 'text_wrap', 'tf_decorator', 'types', 'validator', 'warnings']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QhjA7-j7DggH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE customization"
      ]
    },
    {
      "metadata": {
        "id": "1Ab-45gKtG53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code modifications"
      ]
    },
    {
      "metadata": {
        "id": "tgbs6PK0IEpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### supervised_models.py"
      ]
    },
    {
      "metadata": {
        "id": "L61-egH9IHh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import graphsage.models as models\n",
        "import graphsage.layers as layers\n",
        "from graphsage.aggregators import MeanAggregator, MaxPoolingAggregator, MeanPoolingAggregator, SeqAggregator, GCNAggregator\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "class SupervisedGraphsage2(models.SampleAndAggregate):\n",
        "    \"\"\"Implementation of supervised GraphSAGE.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes,\n",
        "            placeholders, features, adj, degrees,\n",
        "            layer_infos, concat=True, aggregator_type=\"mean\", \n",
        "            model_size=\"small\", sigmoid_loss=False, identity_dim=0,\n",
        "                **kwargs):\n",
        "        '''\n",
        "        Args:\n",
        "            - placeholders: Stanford TensorFlow placeholder object.\n",
        "            - features: Numpy array with node features.\n",
        "            - adj: Numpy array with adjacency lists (padded with random re-samples)\n",
        "            - degrees: Numpy array with node degrees. \n",
        "            - layer_infos: List of SAGEInfo namedtuples that describe the parameters of all \n",
        "                   the recursive layers. See SAGEInfo definition above.\n",
        "            - concat: whether to concatenate during recursive iterations\n",
        "            - aggregator_type: how to aggregate neighbor information\n",
        "            - model_size: one of \"small\" and \"big\"\n",
        "            - sigmoid_loss: Set to true if nodes can belong to multiple classes\n",
        "        '''\n",
        "\n",
        "        models.GeneralizedModel.__init__(self, **kwargs)\n",
        "\n",
        "        if aggregator_type == \"mean\":\n",
        "            self.aggregator_cls = MeanAggregator\n",
        "        elif aggregator_type == \"seq\":\n",
        "            self.aggregator_cls = SeqAggregator\n",
        "        elif aggregator_type == \"meanpool\":\n",
        "            self.aggregator_cls = MeanPoolingAggregator\n",
        "        elif aggregator_type == \"maxpool\":\n",
        "            self.aggregator_cls = MaxPoolingAggregator\n",
        "        elif aggregator_type == \"gcn\":\n",
        "            self.aggregator_cls = GCNAggregator\n",
        "        else:\n",
        "            raise Exception(\"Unknown aggregator: \", self.aggregator_cls)\n",
        "\n",
        "        # get info from placeholders...\n",
        "        self.inputs1 = placeholders[\"batch\"]\n",
        "        self.model_size = model_size\n",
        "        self.adj_info = adj\n",
        "        if identity_dim > 0:\n",
        "           self.embeds = tf.get_variable(\"node_embeddings\", [adj.get_shape().as_list()[0], identity_dim])\n",
        "        else:\n",
        "           self.embeds = None\n",
        "        if features is None: \n",
        "            if identity_dim == 0:\n",
        "                raise Exception(\"Must have a positive value for identity feature dimension if no input features given.\")\n",
        "            self.features = self.embeds\n",
        "        else:\n",
        "            self.features = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False)\n",
        "            if not self.embeds is None:\n",
        "                self.features = tf.concat([self.embeds, self.features], axis=1)\n",
        "        self.degrees = degrees\n",
        "        self.concat = concat\n",
        "        self.num_classes = num_classes\n",
        "        self.sigmoid_loss = sigmoid_loss\n",
        "        self.dims = [(0 if features is None else features.shape[1]) + identity_dim]\n",
        "        self.dims.extend([layer_infos[i].output_dim for i in range(len(layer_infos))])\n",
        "        self.batch_size = placeholders[\"batch_size\"]\n",
        "        self.placeholders = placeholders\n",
        "        print(placeholders['labels'])\n",
        "        self.layer_infos = layer_infos\n",
        "        self.loss2= 0.0\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        samples1, support_sizes1 = self.sample(self.inputs1, self.layer_infos)\n",
        "        num_samples = [layer_info.num_samples for layer_info in self.layer_infos]\n",
        "        self.outputs1, self.aggregators = self.aggregate(samples1, [self.features], self.dims, num_samples,\n",
        "                support_sizes1, concat=self.concat, model_size=self.model_size)\n",
        "        dim_mult = 2 if self.concat else 1\n",
        "\n",
        "        self.outputs1 = tf.nn.l2_normalize(self.outputs1, 1)\n",
        "\n",
        "        dim_mult = 2 if self.concat else 1\n",
        "        self.node_pred = layers.Dense(dim_mult*self.dims[-1], self.num_classes, \n",
        "                dropout=self.placeholders['dropout'],\n",
        "                act=lambda x : x)\n",
        "        # TF graph management\n",
        "        self.node_preds = self.node_pred(self.outputs1)\n",
        "\n",
        "        self._loss()\n",
        "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "        clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var) \n",
        "                for grad, var in grads_and_vars]\n",
        "        self.grad, _ = clipped_grads_and_vars[0]\n",
        "        self.opt_op = self.optimizer.apply_gradients(clipped_grads_and_vars)\n",
        "        self.preds = self.predict()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for aggregator in self.aggregators:\n",
        "            for var in aggregator.vars.values():\n",
        "                self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "        for var in self.node_pred.vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "       \n",
        "        \n",
        "        # transform into a regression loss\n",
        "        self.loss += tf.reduce_mean(\n",
        "                      tf.squared_difference(\n",
        "                        self.node_preds,\n",
        "                        self.placeholders['labels']))\n",
        "        \n",
        "        self.loss2 += tf.losses.mean_squared_error(\n",
        "                        predictions=self.node_preds,\n",
        "                        labels=self.placeholders['labels'])\n",
        "        \n",
        "        # classification loss\n",
        "        #if self.sigmoid_loss:\n",
        "        #    self.loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        #            logits=self.node_preds,\n",
        "        #            labels=self.placeholders['labels']))\n",
        "        #else:\n",
        "        #    self.loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "        #            logits=self.node_preds,\n",
        "        #            labels=self.placeholders['labels']))\n",
        "\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "\n",
        "    def predict(self):\n",
        "        return self.node_preds\n",
        "        #if self.sigmoid_loss:\n",
        "        #    return tf.nn.sigmoid(self.node_preds)\n",
        "        #else:\n",
        "        #    return tf.nn.softmax(self.node_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImaROpWlU6_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### load_data"
      ]
    },
    {
      "metadata": {
        "id": "tXsU01f_U9aB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import networkx as nx\n",
        "from networkx.readwrite import json_graph\n",
        "version_info = list(map(int, nx.__version__.split('.')))\n",
        "major = version_info[0]\n",
        "minor = version_info[1]\n",
        "assert (major <= 1) and (minor <= 11), \"networkx major version > 1.11\"\n",
        "\n",
        "WALK_LEN=5\n",
        "N_WALKS=50\n",
        "\n",
        "def load_data(prefix, normalize=True, load_walks=False):\n",
        "    G_data = json.load(open(prefix + \"-G.json\"))\n",
        "    G = json_graph.node_link_graph(G_data)\n",
        "    if isinstance(G.nodes()[0], int):\n",
        "        conversion = lambda n : int(n)\n",
        "    else:\n",
        "        conversion = lambda n : n\n",
        "\n",
        "    if os.path.exists(prefix + \"-feats.npy\"):\n",
        "        feats = np.load(prefix + \"-feats.npy\")\n",
        "    else:\n",
        "        print(\"No features present.. Only identity features will be used.\")\n",
        "        feats = None\n",
        "    id_map = json.load(open(prefix + \"-id_map.json\"))\n",
        "    id_map = {conversion(k):int(v) for k,v in id_map.items()}\n",
        "    walks = []\n",
        "    class_map = json.load(open(prefix + \"-class_map.json\"))\n",
        "    if isinstance(list(class_map.values())[0], list):\n",
        "        lab_conversion = lambda n : n\n",
        "    else:\n",
        "        lab_conversion = lambda n : float(n)\n",
        "\n",
        "    class_map = {conversion(k):lab_conversion(v) for k,v in class_map.items()}\n",
        "\n",
        "    ## Remove all nodes that do not have val/test annotations\n",
        "    ## (necessary because of networkx weirdness with the Reddit data)\n",
        "    broken_count = 0\n",
        "    for node in G.nodes():\n",
        "        if not 'val' in G.node[node] or not 'test' in G.node[node]:\n",
        "            G.remove_node(node)\n",
        "            broken_count += 1\n",
        "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(broken_count))\n",
        "\n",
        "    ## Make sure the graph has edge train_removed annotations\n",
        "    ## (some datasets might already have this..)\n",
        "    print(\"Loaded data.. now preprocessing..\")\n",
        "    for edge in G.edges():\n",
        "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
        "            G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
        "            G[edge[0]][edge[1]]['train_removed'] = True\n",
        "        else:\n",
        "            G[edge[0]][edge[1]]['train_removed'] = False\n",
        "\n",
        "    if normalize and not feats is None:\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n",
        "        train_feats = feats[train_ids]\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(train_feats)\n",
        "        feats = scaler.transform(feats)\n",
        "    \n",
        "    if load_walks:\n",
        "        with open(prefix + \"-walks.txt\") as fp:\n",
        "            for line in fp:\n",
        "                walks.append(map(conversion, line.split()))\n",
        "\n",
        "    return G, feats, id_map, walks, class_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgHpUYkUVhGD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### minibatch.py"
      ]
    },
    {
      "metadata": {
        "id": "_RjzVlt3Vjou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "\n",
        "class NodeMinibatchIterator(object):\n",
        "    \n",
        "    \"\"\" \n",
        "    This minibatch iterator iterates over nodes for supervised learning.\n",
        "    G -- networkx graph\n",
        "    id2idx -- dict mapping node ids to integer values indexing feature tensor\n",
        "    placeholders -- standard tensorflow placeholders object for feeding\n",
        "    label_map -- map from node ids to class values (integer or list)\n",
        "    num_classes -- number of output classes\n",
        "    batch_size -- size of the minibatches\n",
        "    max_degree -- maximum size of the downsampled adjacency lists\n",
        "    \"\"\"\n",
        "    def __init__(self, G, id2idx, \n",
        "            placeholders, label_map, num_classes, \n",
        "            batch_size=100, max_degree=25,\n",
        "            **kwargs):\n",
        "\n",
        "        self.G = G\n",
        "        self.nodes = G.nodes()\n",
        "        self.id2idx = id2idx\n",
        "        self.placeholders = placeholders\n",
        "        self.batch_size = batch_size\n",
        "        self.max_degree = max_degree\n",
        "        self.batch_num = 0\n",
        "        self.label_map = label_map\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.adj, self.deg = self.construct_adj()\n",
        "        self.test_adj = self.construct_test_adj()\n",
        "\n",
        "        self.val_nodes = [n for n in self.G.nodes() if self.G.node[n]['val']]\n",
        "        self.test_nodes = [n for n in self.G.nodes() if self.G.node[n]['test']]\n",
        "\n",
        "        self.no_train_nodes_set = set(self.val_nodes + self.test_nodes)\n",
        "        self.train_nodes = set(G.nodes()).difference(self.no_train_nodes_set)\n",
        "        # don't train on nodes that only have edges to test set\n",
        "        self.train_nodes = [n for n in self.train_nodes if self.deg[id2idx[n]] > 0]\n",
        "\n",
        "    def _make_label_vec(self, node):\n",
        "        label = self.label_map[node]\n",
        "        if isinstance(label, list):\n",
        "            label_vec = np.array(label)\n",
        "        else:\n",
        "            label_vec = np.zeros((self.num_classes))\n",
        "            # small workaround to save floats as targets\n",
        "            class_ind = self.label_map[node]\n",
        "            label_vec[0] = class_ind\n",
        "        return label_vec\n",
        "\n",
        "    def construct_adj(self):\n",
        "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
        "        deg = np.zeros((len(self.id2idx),))\n",
        "\n",
        "        for nodeid in self.G.nodes():\n",
        "            if self.G.node[nodeid]['test'] or self.G.node[nodeid]['val']:\n",
        "                continue\n",
        "            neighbors = np.array([self.id2idx[neighbor] \n",
        "                for neighbor in self.G.neighbors(nodeid)\n",
        "                if (not self.G[nodeid][neighbor]['train_removed'])])\n",
        "            deg[self.id2idx[nodeid]] = len(neighbors)\n",
        "            if len(neighbors) == 0:\n",
        "                continue\n",
        "            if len(neighbors) > self.max_degree:\n",
        "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
        "            elif len(neighbors) < self.max_degree:\n",
        "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
        "            adj[self.id2idx[nodeid], :] = neighbors\n",
        "        return adj, deg\n",
        "\n",
        "    def construct_test_adj(self):\n",
        "        adj = len(self.id2idx)*np.ones((len(self.id2idx)+1, self.max_degree))\n",
        "        for nodeid in self.G.nodes():\n",
        "            neighbors = np.array([self.id2idx[neighbor] \n",
        "                for neighbor in self.G.neighbors(nodeid)])\n",
        "            if len(neighbors) == 0:\n",
        "                continue\n",
        "            if len(neighbors) > self.max_degree:\n",
        "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
        "            elif len(neighbors) < self.max_degree:\n",
        "                neighbors = np.random.choice(neighbors, self.max_degree, replace=True)\n",
        "            adj[self.id2idx[nodeid], :] = neighbors\n",
        "        return adj\n",
        "\n",
        "    def end(self):\n",
        "        return self.batch_num * self.batch_size >= len(self.train_nodes)\n",
        "\n",
        "    def batch_feed_dict(self, batch_nodes, val=False):\n",
        "        batch1id = batch_nodes\n",
        "        batch1 = [self.id2idx[n] for n in batch1id]\n",
        "              \n",
        "        labels = np.vstack([self._make_label_vec(node) for node in batch1id])\n",
        "        #print(\"labels\", labels)\n",
        "        feed_dict = dict()\n",
        "        feed_dict.update({self.placeholders['batch_size'] : len(batch1)})\n",
        "        feed_dict.update({self.placeholders['batch']: batch1})\n",
        "        feed_dict.update({self.placeholders['labels']: labels})\n",
        "\n",
        "        return feed_dict, labels\n",
        "\n",
        "    def node_val_feed_dict(self, size=None, test=False):\n",
        "        if test:\n",
        "            val_nodes = self.test_nodes\n",
        "        else:\n",
        "            val_nodes = self.val_nodes\n",
        "        if not size is None:\n",
        "            val_nodes = np.random.choice(val_nodes, size, replace=True)\n",
        "        # add a dummy neighbor\n",
        "        ret_val = self.batch_feed_dict(val_nodes)\n",
        "        return ret_val[0], ret_val[1]\n",
        "\n",
        "    def incremental_node_val_feed_dict(self, size, iter_num, test=False):\n",
        "        if test:\n",
        "            val_nodes = self.test_nodes\n",
        "        else:\n",
        "            val_nodes = self.val_nodes\n",
        "        val_node_subset = val_nodes[iter_num*size:min((iter_num+1)*size, \n",
        "            len(val_nodes))]\n",
        "\n",
        "        # add a dummy neighbor\n",
        "        ret_val = self.batch_feed_dict(val_node_subset)\n",
        "        return ret_val[0], ret_val[1], (iter_num+1)*size >= len(val_nodes), val_node_subset\n",
        "\n",
        "    def num_training_batches(self):\n",
        "        return len(self.train_nodes) // self.batch_size + 1\n",
        "\n",
        "    def next_minibatch_feed_dict(self):\n",
        "        start_idx = self.batch_num * self.batch_size\n",
        "        self.batch_num += 1\n",
        "        end_idx = min(start_idx + self.batch_size, len(self.train_nodes))\n",
        "        batch_nodes = self.train_nodes[start_idx : end_idx]\n",
        "        return self.batch_feed_dict(batch_nodes)\n",
        "\n",
        "    def incremental_embed_feed_dict(self, size, iter_num):\n",
        "        node_list = self.nodes\n",
        "        val_nodes = node_list[iter_num*size:min((iter_num+1)*size, \n",
        "            len(node_list))]\n",
        "        return self.batch_feed_dict(val_nodes), (iter_num+1)*size >= len(node_list), val_nodes\n",
        "\n",
        "    def shuffle(self):\n",
        "        \"\"\" Re-shuffle the training set.\n",
        "            Also reset the batch number.\n",
        "        \"\"\"\n",
        "        self.train_nodes = np.random.permutation(self.train_nodes)\n",
        "        self.batch_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HW11noONtgS0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### supervised_train.py"
      ]
    },
    {
      "metadata": {
        "id": "lM2kg-wqs_yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "\n",
        "#from graphsage.supervised_models import SupervisedGraphsage\n",
        "from graphsage.models import SAGEInfo\n",
        "#from graphsage.minibatch import NodeMinibatchIterator\n",
        "from graphsage.neigh_samplers import UniformNeighborSampler\n",
        "#from graphsage.utils import load_data\n",
        "\n",
        "\n",
        "def calc_f1(y_true, y_pred):\n",
        "    if not FLAGS.sigmoid:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    else:\n",
        "        y_pred[y_pred > 0.5] = 1\n",
        "        y_pred[y_pred <= 0.5] = 0\n",
        "    return metrics.f1_score(y_true, y_pred, average=\"micro\"), metrics.f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "  \n",
        "# Define model evaluation function\n",
        "def evaluate(sess, model, minibatch_iter, size=None):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val, labels = minibatch_iter.node_val_feed_dict(size)\n",
        "    node_outs_val = sess.run([model.preds, model.loss, model.loss2], \n",
        "                        feed_dict=feed_dict_val)\n",
        "    #mic, mac = calc_f1(labels, node_outs_val[0])\n",
        "    mic = 0\n",
        "    mac = 0\n",
        "    #print(node_outs_val)\n",
        "    return node_outs_val[1], mic, mac, (time.time() - t_test)\n",
        "\n",
        "def log_dir():\n",
        "    log_dir = FLAGS.base_log_dir + \"/sup-\" + FLAGS.train_prefix.split(\"/\")[-2]\n",
        "    log_dir += \"/{model:s}_{model_size:s}_{lr:0.4f}/\".format(\n",
        "            model=FLAGS.model,\n",
        "            model_size=FLAGS.model_size,\n",
        "            lr=FLAGS.learning_rate)\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def incremental_evaluate(sess, model, minibatch_iter, size, test=False):\n",
        "    t_test = time.time()\n",
        "    finished = False\n",
        "    val_losses = []\n",
        "    val_preds = []\n",
        "    labels = []\n",
        "    iter_num = 0\n",
        "    finished = False\n",
        "    while not finished:\n",
        "        feed_dict_val, batch_labels, finished, _  = minibatch_iter.incremental_node_val_feed_dict(size, iter_num, test=test)\n",
        "        node_outs_val = sess.run([model.preds, model.loss, model.loss2], \n",
        "                         feed_dict=feed_dict_val)\n",
        "        #print(node_outs_val)\n",
        "        val_preds.append(node_outs_val[0])\n",
        "        labels.append(batch_labels)\n",
        "        val_losses.append(node_outs_val[1])\n",
        "        iter_num += 1\n",
        "    val_preds = np.vstack(val_preds)\n",
        "    labels = np.vstack(labels)\n",
        "    #f1_scores = calc_f1(labels, val_preds)\n",
        "    #return np.mean(val_losses), f1_scores[0], f1_scores[1], (time.time() - t_test)\n",
        "    return np.mean(val_losses), 0.0, 0.0,  (time.time() - t_test)\n",
        "\n",
        "  \n",
        "def construct_placeholders(num_classes):\n",
        "    # Define placeholders\n",
        "    placeholders = {\n",
        "        'labels' : tf.placeholder(tf.float32, shape=(None, num_classes), name='labels'),\n",
        "        'batch' : tf.placeholder(tf.int32, shape=(None), name='batch1'),\n",
        "        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),\n",
        "        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),\n",
        "    }\n",
        "    return placeholders\n",
        "\n",
        "def train(train_data, test_data=None):\n",
        "\n",
        "    G = train_data[0]\n",
        "    features = train_data[1]\n",
        "    id_map = train_data[2]\n",
        "    class_map  = train_data[4]\n",
        "    #if isinstance(list(class_map.values())[0], list):\n",
        "    #    num_classes = len(list(class_map.values())[0])\n",
        "    #else:\n",
        "    #    num_classes = len(set(class_map.values()))\n",
        "    num_classes = 1\n",
        "\n",
        "    if not features is None:\n",
        "        # pad with dummy zero vector\n",
        "        features = np.vstack([features, np.zeros((features.shape[1],))])\n",
        "\n",
        "    context_pairs = train_data[3] if FLAGS.random_context else None\n",
        "    placeholders = construct_placeholders(num_classes)\n",
        "    print(class_map) # this one is already flawed with all 0's and should be floats! like 0.002323\n",
        "    # now class_map contains regression values\n",
        "    minibatch = NodeMinibatchIterator(G, \n",
        "            id_map,\n",
        "            placeholders, \n",
        "            class_map,\n",
        "            num_classes,\n",
        "            batch_size=FLAGS.batch_size,\n",
        "            max_degree=FLAGS.max_degree, \n",
        "            context_pairs = context_pairs)\n",
        "    print(\"\\n\\n\\n**************mini batch created*****************\")\n",
        "    print(minibatch)\n",
        "    adj_info_ph = tf.placeholder(tf.int32, shape=minibatch.adj.shape)\n",
        "    adj_info = tf.Variable(adj_info_ph, trainable=False, name=\"adj_info\")\n",
        "    print(\"\\n\\n\\n**************adj_info created*****************\")\n",
        "    \n",
        "\n",
        "    if FLAGS.model == 'graphsage_mean':\n",
        "        # Create model\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        if FLAGS.samples_3 != 0:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_3, FLAGS.dim_2)]\n",
        "        elif FLAGS.samples_2 != 0:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                                SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "        else:\n",
        "            layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1)]\n",
        "\n",
        "        model = SupervisedGraphsage2(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos, \n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "        print(dir(model))\n",
        "        print(\"\\n\\n\\n**************model created*****************\")\n",
        "    elif FLAGS.model == 'gcn':\n",
        "        # Create model\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, 2*FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, 2*FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"gcn\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     concat=False,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_seq':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                     features,\n",
        "                                     adj_info,\n",
        "                                     minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"seq\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_maxpool':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                    features,\n",
        "                                    adj_info,\n",
        "                                    minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"maxpool\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    elif FLAGS.model == 'graphsage_meanpool':\n",
        "        sampler = UniformNeighborSampler(adj_info)\n",
        "        layer_infos = [SAGEInfo(\"node\", sampler, FLAGS.samples_1, FLAGS.dim_1),\n",
        "                            SAGEInfo(\"node\", sampler, FLAGS.samples_2, FLAGS.dim_2)]\n",
        "\n",
        "        model = SupervisedGraphsage(num_classes, placeholders, \n",
        "                                    features,\n",
        "                                    adj_info,\n",
        "                                    minibatch.deg,\n",
        "                                     layer_infos=layer_infos, \n",
        "                                     aggregator_type=\"meanpool\",\n",
        "                                     model_size=FLAGS.model_size,\n",
        "                                     sigmoid_loss = FLAGS.sigmoid,\n",
        "                                     identity_dim = FLAGS.identity_dim,\n",
        "                                     logging=True)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Error: model name unrecognized.')\n",
        "\n",
        "    config = tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)\n",
        "    config.gpu_options.allow_growth = True\n",
        "    #config.gpu_options.per_process_gpu_memory_fraction = GPU_MEM_FRACTION\n",
        "    config.allow_soft_placement = True\n",
        "    \n",
        "    # Initialize session\n",
        "    sess = tf.Session(config=config)\n",
        "    merged = tf.summary.merge_all()\n",
        "    summary_writer = tf.summary.FileWriter(log_dir(), sess.graph)\n",
        "     \n",
        "    # Init variables\n",
        "    sess.run(tf.global_variables_initializer(), feed_dict={adj_info_ph: minibatch.adj})\n",
        "    \n",
        "    # Train model\n",
        "    \n",
        "    total_steps = 0\n",
        "    avg_time = 0.0\n",
        "    epoch_val_costs = []\n",
        "\n",
        "    train_adj_info = tf.assign(adj_info, minibatch.adj)\n",
        "    val_adj_info = tf.assign(adj_info, minibatch.test_adj)\n",
        "    for epoch in range(FLAGS.epochs): \n",
        "        minibatch.shuffle() \n",
        "\n",
        "        iter = 0\n",
        "        print('Epoch: %04d' % (epoch + 1))\n",
        "        epoch_val_costs.append(0)\n",
        "        while not minibatch.end():\n",
        "            # Construct feed dictionary\n",
        "            feed_dict, labels = minibatch.next_minibatch_feed_dict()\n",
        "            feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "            t = time.time()\n",
        "            # Training step\n",
        "            outs = sess.run([merged, model.opt_op, model.loss, model.preds], feed_dict=feed_dict)\n",
        "            train_cost = outs[2]\n",
        "\n",
        "            if iter % FLAGS.validate_iter == 0:\n",
        "                # Validation\n",
        "                sess.run(val_adj_info.op)\n",
        "                if FLAGS.validate_batch_size == -1:\n",
        "                    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
        "                else:\n",
        "                    val_cost, val_f1_mic, val_f1_mac, duration = evaluate(sess, model, minibatch, FLAGS.validate_batch_size)\n",
        "                sess.run(train_adj_info.op)\n",
        "                epoch_val_costs[-1] += val_cost\n",
        "\n",
        "            if total_steps % FLAGS.print_every == 0:\n",
        "                summary_writer.add_summary(outs[0], total_steps)\n",
        "    \n",
        "            # Print results\n",
        "            avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
        "\n",
        "            if total_steps % FLAGS.print_every == 0:\n",
        "                train_f1_mic, train_f1_mac = calc_f1(labels, outs[-1])\n",
        "                print(\"Iter:\", '%04d' % iter, \n",
        "                      \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
        "                      \"train_f1_mic=\", \"{:.5f}\".format(train_f1_mic), \n",
        "                      \"train_f1_mac=\", \"{:.5f}\".format(train_f1_mac), \n",
        "                      \"val_loss=\", \"{:.5f}\".format(val_cost),\n",
        "                      \"val_f1_mic=\", \"{:.5f}\".format(val_f1_mic), \n",
        "                      \"val_f1_mac=\", \"{:.5f}\".format(val_f1_mac), \n",
        "                      \"time=\", \"{:.5f}\".format(avg_time))\n",
        " \n",
        "            iter += 1\n",
        "            total_steps += 1\n",
        "\n",
        "            if total_steps > FLAGS.max_total_steps:\n",
        "                break\n",
        "\n",
        "        if total_steps > FLAGS.max_total_steps:\n",
        "                break\n",
        "    \n",
        "    print(\"Optimization Finished!\")\n",
        "    sess.run(val_adj_info.op)\n",
        "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size)\n",
        "    print(\"Full validation stats:\",\n",
        "                  \"loss=\", \"{:.5f}\".format(val_cost),\n",
        "                  \"f1_micro=\", \"{:.5f}\".format(val_f1_mic),\n",
        "                  \"f1_macro=\", \"{:.5f}\".format(val_f1_mac),\n",
        "                  \"time=\", \"{:.5f}\".format(duration))\n",
        "    with open(log_dir() + \"val_stats.txt\", \"w\") as fp:\n",
        "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f} time={:.5f}\".\n",
        "                format(val_cost, val_f1_mic, val_f1_mac, duration))\n",
        "\n",
        "    print(\"Writing test set stats to file (don't peak!)\")\n",
        "    val_cost, val_f1_mic, val_f1_mac, duration = incremental_evaluate(sess, model, minibatch, FLAGS.batch_size, test=True)\n",
        "    with open(log_dir() + \"test_stats.txt\", \"w\") as fp:\n",
        "        fp.write(\"loss={:.5f} f1_micro={:.5f} f1_macro={:.5f}\".\n",
        "                format(val_cost, val_f1_mic, val_f1_mac))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZa6q-2xtNVR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ]
    },
    {
      "metadata": {
        "id": "jOXpzvHKDnBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1045
        },
        "outputId": "d4b4accf-a0cd-4cda-b26e-31833a0b1057"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "#from graphsage import supervised_train as train\n",
        "#sys.argv = [\"python\" ,\"graphsage.supervised_train\", \"--train_prefix\", \"./example_data/ppi\", \"--model\" ,\"graphsage_mean\" ,\"--sigmoid\"]\n",
        "sys.argv = [\"python\" ,\"graphsage.supervised_train\", \"--train_prefix\", \"./data/precomputed-graphSAGE-TF/er_100_0_15_eb\", \n",
        "            \"--model\" ,\"graphsage_mean\",\n",
        "           \"--identity_dim\",\"1\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "\n",
        "# Set random seed\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Settings\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
        "                            \"\"\"Whether to log device placement.\"\"\")\n",
        "\n",
        "#core params..\n",
        "flags.DEFINE_string('model', 'graphsage_mean', 'model names. See README for possible values.')  \n",
        "flags.DEFINE_float('learning_rate', 0.01, 'initial learning rate.')\n",
        "flags.DEFINE_string(\"model_size\", \"small\", \"Can be big or small; model specific def'ns\")\n",
        "flags.DEFINE_string('train_prefix', '', 'prefix identifying training data. must be specified.')\n",
        "\n",
        "# left to default values in main experiments \n",
        "flags.DEFINE_integer('epochs', 10, 'number of epochs to train.')\n",
        "flags.DEFINE_float('dropout', 0.0, 'dropout rate (1 - keep probability).')\n",
        "flags.DEFINE_float('weight_decay', 0.0, 'weight for l2 loss on embedding matrix.')\n",
        "flags.DEFINE_integer('max_degree', 128, 'maximum node degree.')\n",
        "flags.DEFINE_integer('samples_1', 25, 'number of samples in layer 1')\n",
        "flags.DEFINE_integer('samples_2', 10, 'number of samples in layer 2')\n",
        "flags.DEFINE_integer('samples_3', 0, 'number of users samples in layer 3. (Only for mean model)')\n",
        "flags.DEFINE_integer('dim_1', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
        "flags.DEFINE_integer('dim_2', 128, 'Size of output dim (final is 2x this, if using concat)')\n",
        "flags.DEFINE_boolean('random_context', True, 'Whether to use random context or direct edges')\n",
        "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
        "flags.DEFINE_boolean('sigmoid', False, 'whether to use sigmoid loss')\n",
        "flags.DEFINE_integer('identity_dim', 0, 'Set to positive value to use identity embedding features of that dimension. Default 0.')\n",
        "\n",
        "#logging, saving, validation settings etc.\n",
        "flags.DEFINE_string('base_log_dir', '.', 'base directory for logging and saving embeddings')\n",
        "flags.DEFINE_integer('validate_iter', 5000, \"how often to run a validation minibatch.\")\n",
        "flags.DEFINE_integer('validate_batch_size', 256, \"how many nodes per validation sample.\")\n",
        "flags.DEFINE_integer('gpu', 1, \"which gpu to use.\")\n",
        "flags.DEFINE_integer('print_every', 5, \"How often to print training info.\")\n",
        "flags.DEFINE_integer('max_total_steps', 10**10, \"Maximum total number of iterations\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(FLAGS.gpu)\n",
        "\n",
        "GPU_MEM_FRACTION = 0.8\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FLAGS.identity_dim = 1\n",
        "print(\"Loading training data..\")\n",
        "train_data = load_data(FLAGS.train_prefix)\n",
        "print(\"Done loading training data..\")\n",
        "train(train_data)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data..\n",
            "No features present.. Only identity features will be used.\n",
            "Removed 0 nodes that lacked proper annotations due to networkx versioning issues\n",
            "Loaded data.. now preprocessing..\n",
            "Done loading training data..\n",
            "{0: 0.00293450850813729, 1: 0.0024638317654371005, 2: 0.002442520255804091, 3: 0.002492287718725189, 4: 0.0028810761378967346, 5: 0.0023797124730655446, 6: 0.0022796596051368203, 7: 0.002587409461213603, 8: 0.002940673808618742, 9: 0.002553149176521342, 10: 0.0024161822901593967, 11: 0.0027858972861737824, 12: 0.0022352724704013047, 13: 0.002535108565727603, 14: 0.002334299610777262, 15: 0.003414151549466331, 16: 0.0027759414910120492, 17: 0.002646476569518387, 18: 0.002478857459874291, 19: 0.0031545371944696485, 20: 0.0027005935868813697, 21: 0.0025012162909600036, 22: 0.00268720934384496, 23: 0.002396673799257646, 24: 0.002892114635581358, 25: 0.002164091830479805, 26: 0.0024478732936242284, 27: 0.002871244577193392, 28: 0.0021682545368117236, 29: 0.00254338117647795, 30: 0.0030509064148617412, 31: 0.002436163123340614, 32: 0.002619952588249722, 33: 0.0023988841625610397, 34: 0.0027006975097279805, 35: 0.00232621331526197, 36: 0.002604629228701701, 37: 0.0030480539229629836, 38: 0.0021789507451789067, 39: 0.002602706555213879, 40: 0.002812244707042956, 41: 0.0017829652720420983, 42: 0.0021646672763607775, 43: 0.001979994457576318, 44: 0.002551037940651066, 45: 0.0026601724353177195, 46: 0.0024997968669400283, 47: 0.0022400398867277764, 48: 0.002552701125794745, 49: 0.0024349913576486065, 50: 0.0028368837769569477, 51: 0.002472595137734062, 52: 0.003994488860103708, 53: 0.002724178467239037, 54: 0.0021780987655311917, 55: 0.0034522924631865404, 56: 0.003261212572888248, 57: 0.002991493924446379, 58: 0.0030209195052359546, 59: 0.002936928919036206, 60: 0.0023725765606265356, 61: 0.0036040491412457905, 62: 0.0024506305645287216, 63: 0.003179608796364348, 64: 0.0030210623002397924, 65: 0.0030828356750483886, 66: 0.0026728152039197095, 67: 0.0030480074759715622, 68: 0.0021218969364656414, 69: 0.002281982167658838, 70: 0.0043397957472545105, 71: 0.0028282420595909506, 72: 0.0027452033317481787, 73: 0.005048268690693754, 74: 0.0029431752804877248, 75: 0.003288505607623257, 76: 0.0022526209240791885, 77: 0.0025031534136607117, 78: 0.0024696859522351667, 79: 0.0020830656397154597, 80: 0.0020271253529512757, 81: 0.003036782262779067, 82: 0.0036866750785011534, 83: 0.001868569446573746, 84: 0.0031210124334901516, 85: 0.0035779617899903645, 86: 0.0035402072512826952, 87: 0.002351002440649483, 88: 0.0021525984283581284, 89: 0.002354865880972507, 90: 0.002817827379376468, 91: 0.0029777104346492412, 92: 0.0028705438684372996, 93: 0.003312991934533295, 94: 0.003313902484095878, 95: 0.0031151000789187812, 96: 0.0030165501512546293, 97: 0.00307910781536029, 98: 0.003264772508432419, 99: 0.0030199965252790404, 100: 0.0029761835762158557, 101: 0.0027234077078066785, 102: 0.0030288077664604248, 103: 0.0026465609380654947, 104: 0.0024776049721673766, 105: 0.002409441557658951, 106: 0.0022274228605789477, 107: 0.0024129003534657236, 108: 0.002742478178512435, 109: 0.0025522102546221056, 110: 0.0023990820883357036, 111: 0.003339661896990289, 112: 0.00198076616004811, 113: 0.0022027904749970613, 114: 0.0025016413302531995, 115: 0.0026717447321669904, 116: 0.002710883478797917, 117: 0.0028615308139087734, 118: 0.002837823195358259, 119: 0.002166905776316243, 120: 0.003046070073151941, 121: 0.0036926524966305286, 122: 0.0026384714286764805, 123: 0.0023836946105912716, 124: 0.0022807537597870505, 125: 0.0029859245152948776, 126: 0.002423830171808962, 127: 0.0026700966245753028, 128: 0.0021994198791824866, 129: 0.002164517304940416, 130: 0.002168403573656433, 131: 0.0023923469536598087, 132: 0.0026901025140478397, 133: 0.0018415878898726608, 134: 0.0019250635798906074, 135: 0.0025649812966683063, 136: 0.002540767985256386, 137: 0.0031275009532654506, 138: 0.0028134697779582485, 139: 0.0028851955640454897, 140: 0.0026666489187438876, 141: 0.002636351597568215, 142: 0.0018657529321105778, 143: 0.00236361988005672, 144: 0.0026498891256060467, 145: 0.0020510506349704525, 146: 0.0021780946086399503, 147: 0.0021444736828567395, 148: 0.0029759542676903034, 149: 0.0019549456348749595, 150: 0.002478149690108521, 151: 0.0021893802806091274, 152: 0.00220129321311297, 153: 0.00260476601174258, 154: 0.0027119084311176787, 155: 0.0037064526541469483, 156: 0.0021286679449661113, 157: 0.002655379763310026, 158: 0.0020270330248299113, 159: 0.0029788647727901486, 160: 0.00250600119077702, 161: 0.002098406866714798, 162: 0.002191991819341628, 163: 0.002586569427537626, 164: 0.0021204080216806415, 165: 0.002578179548654749, 166: 0.002245327329573321, 167: 0.0026994379547306087, 168: 0.0030778958165768706, 169: 0.0023797161494531604, 170: 0.0031001631451025437, 171: 0.002182190976974626, 172: 0.002519155228963816, 173: 0.003143482949555689, 174: 0.002468363172487087, 175: 0.00212834018074928, 176: 0.002405891019511112, 177: 0.0037282439304958674, 178: 0.002862601999686845, 179: 0.0025291893357893666, 180: 0.0022749276125337897, 181: 0.002568750127517341, 182: 0.002194586008587898, 183: 0.002060758726276027, 184: 0.0031914526516720254, 185: 0.002974407180143656, 186: 0.0023009044865805587, 187: 0.003592671272438155, 188: 0.002154275358409855, 189: 0.002459547755049956, 190: 0.002518646965240122, 191: 0.002219580826128737, 192: 0.0023834042020862314, 193: 0.002230415285946785, 194: 0.002084453071134278, 195: 0.003605745536686552, 196: 0.003476426567878168, 197: 0.0028443644186984213, 198: 0.003735513525285054, 199: 0.0024415493510591933, 200: 0.0019832968818719794, 201: 0.002744356097953741, 202: 0.002736439317385887, 203: 0.002736297644737098, 204: 0.0026480202681149014, 205: 0.002949595595327833, 206: 0.0015027088814254348, 207: 0.0023745493525477094, 208: 0.003182465344980636, 209: 0.003083629342616875, 210: 0.0024541046178920456, 211: 0.0031130047363523935, 212: 0.0021683819105425993, 213: 0.0022917911380848973, 214: 0.0025642179696280677, 215: 0.0026311674317993383, 216: 0.0032285686847874635, 217: 0.002366340422733876, 218: 0.002961809201402562, 219: 0.0029981432654120795, 220: 0.002666685934938946, 221: 0.002091732520458508, 222: 0.00274627989842838, 223: 0.0025857914888103286, 224: 0.002946523853212877, 225: 0.0024734941819134744, 226: 0.0031679884467510555, 227: 0.0027759548820468806, 228: 0.002397673354019064, 229: 0.002383244300122088, 230: 0.0032128001406127463, 231: 0.0022645628081392607, 232: 0.0026507471731730876, 233: 0.002777843890391636, 234: 0.002958411590035721, 235: 0.0022003803415614684, 236: 0.002683686023315924, 237: 0.0024446621523175954, 238: 0.0022914391355290603, 239: 0.002379072664194589, 240: 0.002739717255136323, 241: 0.0031034524870805565, 242: 0.0018736400343994854, 243: 0.0026984117993962002, 244: 0.0024453190943123956, 245: 0.0028844954406091693, 246: 0.003002161698172592, 247: 0.002682541281106569, 248: 0.0024944641260441973, 249: 0.003328257124016038, 250: 0.0021720574159672834, 251: 0.003541637754162694, 252: 0.002507177497915357, 253: 0.002627811871955304, 254: 0.0025638559276588437, 255: 0.004062599059492298, 256: 0.002258907755203021, 257: 0.00233069315033983, 258: 0.003458128716063019, 259: 0.0027258243661907764, 260: 0.0023148928155658347, 261: 0.0025635065667064744, 262: 0.001798980346811503, 263: 0.0036703494423550144, 264: 0.0035442396687458094, 265: 0.003073969056426587, 266: 0.0030376774748347717, 267: 0.002963717933228482, 268: 0.0019527783360442387, 269: 0.0024109106147687646, 270: 0.002695319939831072, 271: 0.0029878532611944597, 272: 0.0033510938313661876, 273: 0.002071212315825789, 274: 0.0029435816175380227, 275: 0.002659930628096289, 276: 0.0019662801885725377, 277: 0.00275928950696333, 278: 0.003031642119579467, 279: 0.0018666741480711355, 280: 0.00198531885100449, 281: 0.0030742673097656817, 282: 0.0027087173036985553, 283: 0.002356256313382163, 284: 0.0033391495552406634, 285: 0.0033962294427462183, 286: 0.0023663273562978065, 287: 0.0028804309600165973, 288: 0.002617420658173319, 289: 0.002803012453074526, 290: 0.002876453481556508, 291: 0.0020694055790514915, 292: 0.002425609255564975, 293: 0.003197561271213617, 294: 0.0023472306281746764, 295: 0.0020591415760071374, 296: 0.0025577931380206574, 297: 0.0026151009106295912, 298: 0.0026743442255365533, 299: 0.002599564426246157, 300: 0.0024267394715375957, 301: 0.002700731563972443, 302: 0.002725613316324294, 303: 0.0025519844416829977, 304: 0.002401447347198552, 305: 0.0028735896392972733, 306: 0.0033386311236298367, 307: 0.0030900905098279904, 308: 0.0022200994782208914, 309: 0.0031230810040170573, 310: 0.00248425148360971, 311: 0.00223480725650895, 312: 0.0023914175840311054, 313: 0.002199992398554711, 314: 0.002213562022580641, 315: 0.0024694141859266557, 316: 0.0030255215208314064, 317: 0.0026172049215644113, 318: 0.002652227160773725, 319: 0.0022855744503324396, 320: 0.002569377137220484, 321: 0.0021874580605424515, 322: 0.00332417959698796, 323: 0.00235285043238388, 324: 0.002912913984696051, 325: 0.002996531228358062, 326: 0.003314878713046396, 327: 0.002710591771588017, 328: 0.0034567874466720533, 329: 0.003102077677123248, 330: 0.002978102039412978, 331: 0.0028622149121181792, 332: 0.002668353253519139, 333: 0.0025262664043078993, 334: 0.0025798081703030896, 335: 0.0024626599280594464, 336: 0.0023970396374760465, 337: 0.0022758006420912063, 338: 0.0035924536576150017, 339: 0.0023470344976073503, 340: 0.0029183491864291033, 341: 0.002049986143529443, 342: 0.0027568083215297875, 343: 0.0021593150505321247, 344: 0.0022436952014738267, 345: 0.002574942548766713, 346: 0.0021155632461844864, 347: 0.0029549171024459337, 348: 0.0022039145417350045, 349: 0.0029777317886983856, 350: 0.0035654825075253902, 351: 0.002027245950191702, 352: 0.0032716376755820378, 353: 0.002164161725905507, 354: 0.002687442992389116, 355: 0.003170276236045454, 356: 0.0027915917268384266, 357: 0.003025214764147975, 358: 0.0030458621963988493, 359: 0.0027384289794013155, 360: 0.002360067478454735, 361: 0.0029885381181452616, 362: 0.002466643633143821, 363: 0.001909613420941168, 364: 0.0018121273347791098, 365: 0.0024361751898373894, 366: 0.0022307865993563005, 367: 0.00201756770982728, 368: 0.0019378511937530667, 369: 0.002066652503931377, 370: 0.003229506654445558, 371: 0.0019241261669597858, 372: 0.002032566762514761, 373: 0.0023413106714819058, 374: 0.003908065034271576, 375: 0.002529370739717919, 376: 0.0025796105056984835, 377: 0.0019400265691068457, 378: 0.0026196862704185872, 379: 0.003711220961971743, 380: 0.0028578061382639425, 381: 0.002989642706822388, 382: 0.003003348074452187, 383: 0.002393482243733367, 384: 0.002554112014776572, 385: 0.002713832357449147, 386: 0.003117269945928383, 387: 0.003491653233107708, 388: 0.003169162647122607, 389: 0.0031210309531739837, 390: 0.0019503410265625286, 391: 0.002787949856017954, 392: 0.00326134362589339, 393: 0.0023872364678251875, 394: 0.002754658228593678, 395: 0.0026926187541501984, 396: 0.0026423418697644257, 397: 0.002886103220327037, 398: 0.0023788001714243525, 399: 0.0024573330749497286, 400: 0.001924245332948357, 401: 0.002116313391107193, 402: 0.002543017466233515, 403: 0.003076387439975676, 404: 0.0023727776627277763, 405: 0.0032277635699200784, 406: 0.0030916063023487604, 407: 0.00311432530170253, 408: 0.00307013128378424, 409: 0.002858522218687786, 410: 0.00332526416462222, 411: 0.00322700759241221, 412: 0.002571673147076737, 413: 0.002751526827646221, 414: 0.002532797094837163, 415: 0.002396247658323389, 416: 0.0029924499899477383, 417: 0.0024519791947589077, 418: 0.0021218117381477247, 419: 0.0023552648484582175, 420: 0.0031931096322030147, 421: 0.0019803836166423323, 422: 0.0019281380624239116, 423: 0.001949340904424219, 424: 0.0024307311171411624, 425: 0.002456969946561092, 426: 0.0026097804730937676, 427: 0.002688425734165694, 428: 0.0026381780782718943, 429: 0.002161050461205782, 430: 0.0027715080530857803, 431: 0.0023615853470989392, 432: 0.003041823240085611, 433: 0.0024995040279573136, 434: 0.0034486300145440078, 435: 0.0030031708066636954, 436: 0.003205607362098516, 437: 0.0029117872058435344, 438: 0.0021600763092983126, 439: 0.002467541992698872, 440: 0.0028058452873605485, 441: 0.003245838984604995, 442: 0.002592228634264704, 443: 0.0029122861376146537, 444: 0.0024150876896727605, 445: 0.002142009435131191, 446: 0.0029149382134791347, 447: 0.0028011240952369883, 448: 0.002941762707937566, 449: 0.002429200097330904, 450: 0.0036750049096118827, 451: 0.0025437656718368365, 452: 0.002234911760622995, 453: 0.0023548581980355073, 454: 0.001961065525357143, 455: 0.0019455912637014237, 456: 0.0018619303034940156, 457: 0.0020324228138650646, 458: 0.002393955429572752, 459: 0.0027781767700053805, 460: 0.001934857320142248, 461: 0.002450287658283396, 462: 0.002939774607462661, 463: 0.0021282421285508807, 464: 0.0031911410298594354, 465: 0.0024013146261938974, 466: 0.00179331109664459, 467: 0.0028625323310673334, 468: 0.003742154300493418, 469: 0.002451920977830579, 470: 0.00303427937758262, 471: 0.0029109553115139195, 472: 0.0027948462395859944, 473: 0.002876071450524264, 474: 0.0021497480927614174, 475: 0.0029224157211434304, 476: 0.0024850435651825524, 477: 0.0031302192694867186, 478: 0.002895042949463132, 479: 0.002156350435273095, 480: 0.0028994691850378175, 481: 0.00290915380591096, 482: 0.0025701003571987043, 483: 0.0023974140835422318, 484: 0.0025704046062245555, 485: 0.002075717668634634, 486: 0.0027585663157368456, 487: 0.002271515304788903, 488: 0.0028912042208680392, 489: 0.0021070457321853003, 490: 0.002630882767754839, 491: 0.0029705664949798566, 492: 0.00232264267571933, 493: 0.0055801328186240754, 494: 0.004103882300721746, 495: 0.00248431932166335, 496: 0.00309616295124263, 497: 0.0028981836093606647, 498: 0.003836095551518416, 499: 0.0038671328612677512, 500: 0.00280355206259493, 501: 0.0029699260669417723, 502: 0.0029869800173605975, 503: 0.0031474176864075666, 504: 0.0034001966997883153, 505: 0.002808349379282262, 506: 0.0030197259958163404, 507: 0.0034125768097769313, 508: 0.0032897880654708883, 509: 0.0029958336223975903, 510: 0.0025573659872660712, 511: 0.003140131339166636, 512: 0.002965599790182821, 513: 0.00320941127209819, 514: 0.0026662395208621216, 515: 0.002688670990432465, 516: 0.0022765516754089756, 517: 0.0029133896274703016, 518: 0.003298167573723923, 519: 0.002464318760910181, 520: 0.003078054283707808, 521: 0.003677047739847344, 522: 0.003972120655889697, 523: 0.0025912522771445504, 524: 0.002902570323185834, 525: 0.0029896214029158513, 526: 0.0028283132644684205, 527: 0.0029410619005144447, 528: 0.002874981184848188, 529: 0.002424994456626655, 530: 0.002199832459839559, 531: 0.0031275454092366765, 532: 0.0033043850938187717, 533: 0.0027512563675876982, 534: 0.0028722905868243344, 535: 0.0029395241789152856, 536: 0.0031737564042531876, 537: 0.003292901083630134, 538: 0.0027192175861846263, 539: 0.0027932031354614725, 540: 0.0025326752139014715, 541: 0.0025167621824110542, 542: 0.003038144031147228, 543: 0.0034296709755110718, 544: 0.0029221801704822914, 545: 0.002369338635247258, 546: 0.002246750026879822, 547: 0.0025845401638568146, 548: 0.0029821753119239486, 549: 0.0019079521216902584, 550: 0.003016536005019309, 551: 0.0026683995319367695, 552: 0.001711204797113797, 553: 0.002169209827385881, 554: 0.002180490545952894, 555: 0.0023155324290789377, 556: 0.00179515755928626, 557: 0.0028755835037689854, 558: 0.0019306840087800268, 559: 0.0020255492226861874, 560: 0.0019927772183751004, 561: 0.0022108298446882678, 562: 0.0030584994301431348, 563: 0.002293473172783447, 564: 0.002658416735483207, 565: 0.0029176140260833046, 566: 0.0020657531450920686, 567: 0.003105584260234198, 568: 0.0033227958991856633, 569: 0.002166445279436589, 570: 0.003282011452619156, 571: 0.0033925660098023985, 572: 0.002888176926738776, 573: 0.004603985841758421, 574: 0.0023476546473294507, 575: 0.0023116237953921936, 576: 0.0024407967354335433, 577: 0.003024853198669627, 578: 0.0029386311984154908, 579: 0.0032593941806933627, 580: 0.004022361799909328, 581: 0.002934911145808791, 582: 0.003311028254673715, 583: 0.003645570885484634, 584: 0.0032879849604072037, 585: 0.0034818156018354223, 586: 0.0031773417901619526, 587: 0.0031358666763933135, 588: 0.0033214375375239078, 589: 0.0031328787565816727, 590: 0.0037857398191537997, 591: 0.0029289073818787323, 592: 0.0019932483751485958, 593: 0.0027912199382003956, 594: 0.00244977516244421, 595: 0.002135019051756622, 596: 0.0032304277739594453, 597: 0.0032478689337183, 598: 0.003093636680840485, 599: 0.0030675045347678294, 600: 0.002178820455304206, 601: 0.002448842126274877, 602: 0.0025847601537064818, 603: 0.0022938804321445596, 604: 0.002577879711379641, 605: 0.0024902105159927586, 606: 0.002472739049156406, 607: 0.0026287575573872126, 608: 0.003280642880744536, 609: 0.002373152701194637, 610: 0.0029892766871822654, 611: 0.0024425426243796225, 612: 0.0028939578054542137, 613: 0.0022008511467270053, 614: 0.0029371402573876754, 615: 0.0031955386624422906, 616: 0.0032474514385205855, 617: 0.0023078720135997423, 618: 0.002096958640711588, 619: 0.001645895580081696, 620: 0.002289420948535474, 621: 0.0032165701319451087, 622: 0.002594925170751142, 623: 0.002716098113069657, 624: 0.0023332790856774744, 625: 0.002101360843756852, 626: 0.0031708768762076247, 627: 0.0031781610230002468, 628: 0.0026162595616482194, 629: 0.002329488228066597, 630: 0.0024220772198044927, 631: 0.0021892026746647827, 632: 0.0022179541732785337, 633: 0.002314144090753148, 634: 0.0027012188443549063, 635: 0.0028087956644612276, 636: 0.002440246230039977, 637: 0.0024602880494410634, 638: 0.002723689544744361, 639: 0.003215401336368514, 640: 0.0021073475297720013, 641: 0.003937875848581979, 642: 0.003157803961280174, 643: 0.0030076849543689823, 644: 0.0022874552594064753, 645: 0.0024526518415270548, 646: 0.0028971014181911263, 647: 0.0021989200276193127, 648: 0.002146668171082594, 649: 0.0026312364440352135, 650: 0.0033078478401642, 651: 0.0017734968244433843, 652: 0.0020681439229460878, 653: 0.0033482617172280035, 654: 0.002566731217245743, 655: 0.002113265729627953, 656: 0.002525906078515661, 657: 0.0027651386500342065, 658: 0.0024557116335249983, 659: 0.0023946175791582725, 660: 0.002282368028406321, 661: 0.0031424764106470424, 662: 0.002035748331736993, 663: 0.0026183897945913546, 664: 0.0022118077699084387, 665: 0.0025267109092426865, 666: 0.0020373764140337917, 667: 0.002528305524663435, 668: 0.0030142981848311616, 669: 0.0024276894971614602, 670: 0.0020522310089113763, 671: 0.002291260058043096, 672: 0.0018196488826923611, 673: 0.002454574718874769, 674: 0.0024760523971583954, 675: 0.003075597477264356, 676: 0.0024569210578836344, 677: 0.002070924199419879, 678: 0.003280181110021353, 679: 0.0025481175766151683, 680: 0.003932660354522863, 681: 0.0031904968279174327, 682: 0.0016826910027614589, 683: 0.003719261613247765, 684: 0.0029213212728416544, 685: 0.002805810404099258, 686: 0.0036823304025509138, 687: 0.0028964552937995993, 688: 0.002602469212103723, 689: 0.0026970933217800113, 690: 0.002579726457533257, 691: 0.002610602187540464, 692: 0.002539464795138444, 693: 0.0033530070983827185, 694: 0.0026535240696290547, 695: 0.00276103113175582, 696: 0.002491849444999234, 697: 0.003036999864060478, 698: 0.002655645301291384, 699: 0.002655852879444717, 700: 0.0031632287050042222, 701: 0.003357105176366533, 702: 0.0026937678988051745, 703: 0.0018211533806860407, 704: 0.003466456279395328, 705: 0.0020411265983490377, 706: 0.002178074368780288, 707: 0.002967724960985373, 708: 0.0031428755844504705, 709: 0.0032106198538877913, 710: 0.0023403202767683486, 711: 0.002656234313844335, 712: 0.003828509560020641, 713: 0.0024987446071955645, 714: 0.002503325621563482, 715: 0.002264845264669377, 716: 0.0022341764823841496, 717: 0.0031704235893302677, 718: 0.002442328640476649, 719: 0.0023454145320587027, 720: 0.002620640326040679, 721: 0.0027762328762609067, 722: 0.0022649485269616364, 723: 0.0027015066004130012, 724: 0.002553281900537715, 725: 0.00247540705293296}\n",
            "\n",
            "\n",
            "\n",
            "**************mini batch created*****************\n",
            "<__main__.NodeMinibatchIterator object at 0x7fa5e4dc0978>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "\n",
            "\n",
            "**************adj_info created*****************\n",
            "Tensor(\"labels:0\", shape=(?, 1), dtype=float32)\n",
            "WARNING:tensorflow:From /content/graphsage/aggregators.py:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_accuracy', '_build', '_loss', 'accuracy', 'activations', 'adj_info', 'aggregate', 'aggregator_cls', 'aggregators', 'batch_size', 'build', 'concat', 'degrees', 'dims', 'embeds', 'features', 'grad', 'inputs', 'inputs1', 'layer_infos', 'layers', 'load', 'logging', 'loss', 'loss2', 'model_size', 'name', 'node_pred', 'node_preds', 'num_classes', 'opt_op', 'optimizer', 'outputs', 'outputs1', 'placeholders', 'predict', 'preds', 'sample', 'save', 'sigmoid_loss', 'vars']\n",
            "\n",
            "\n",
            "\n",
            "**************model created*****************\n",
            "Epoch: 0001\n",
            "Iter: 0000 train_loss= 0.00133 train_f1_mic= 1.00000 train_f1_mac= 1.00000 val_loss= 1.04417 val_f1_mic= 0.00000 val_f1_mac= 0.00000 time= 0.34286\n",
            "Epoch: 0002\n",
            "Epoch: 0003\n",
            "Epoch: 0004\n",
            "Epoch: 0005\n",
            "Epoch: 0006\n",
            "Iter: 0000 train_loss= 0.11844 train_f1_mic= 1.00000 train_f1_mac= 1.00000 val_loss= 0.10887 val_f1_mic= 0.00000 val_f1_mac= 0.00000 time= 0.09153\n",
            "Epoch: 0007\n",
            "Epoch: 0008\n",
            "Epoch: 0009\n",
            "Epoch: 0010\n",
            "Optimization Finished!\n",
            "Full validation stats: loss= 0.00368 f1_micro= 0.00000 f1_macro= 0.00000 time= 0.01303\n",
            "Writing test set stats to file (don't peak!)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DEcVTdzHTwe1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-t8zitZHRkne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a8b0029e-227d-466d-a221-73d392b94741"
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data   example_data  GraphSAGE\t  sup-example_data\n",
            "drive  graphsage     sample_data  sup-precomputed-graphSAGE-TF\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}