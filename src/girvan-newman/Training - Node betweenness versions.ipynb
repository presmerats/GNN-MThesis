{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how a GCN/GraphSAGE model is trained to compute Node betweenness centrality on different graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # output as multiclass target\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # output as regression target\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import importlib\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "\n",
    "def loadDataset(collection, name=None):\n",
    "    try:\n",
    "        # import datasets\n",
    "        themodule = importlib.import_module(\"torch_geometric.datasets\")\n",
    "        # get the function corresponding to collection\n",
    "        method_to_call = getattr(themodule, collection)\n",
    "        if name:\n",
    "            return method_to_call(root='./data/'+str(collection), name=name)\n",
    "        else:\n",
    "            return method_to_call(root='./data/'+str(collection)) \n",
    "    except:\n",
    "        # custom module\n",
    "        method_to_call = globals()[collection]\n",
    "       \n",
    "        if name:\n",
    "            return method_to_call(root='./data/'+str(collection), name=name)\n",
    "        else:\n",
    "            return method_to_call(root='./data/'+str(collection)) \n",
    "        \n",
    "\n",
    "\n",
    "def shuffleTrainTestMasks(data, trainpct = 0.7):\n",
    "    ysize = list(data.y.size())[0]\n",
    "    data.train_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "    data.train_mask[int(ysize*trainpct):] = 1\n",
    "    data.train_mask = data.train_mask[torch.randperm(ysize)]\n",
    "    data.test_mask = torch.ones(ysize,1, dtype=torch.long) - data.train_mask\n",
    "  \n",
    "\n",
    "def transformMask(mask):\n",
    "    train_mask = []\n",
    "    i = 0\n",
    "    for pick in mask:\n",
    "        if pick[0]==1:\n",
    "            train_mask.append(i)\n",
    "        i+=1\n",
    "    return train_mask\n",
    "    \n",
    "def shuffleTrainTestValMasks(data, trainpct = 0.7, valpct = 0.2):\n",
    "\n",
    "    ysize = list(data.y.size())[0]\n",
    "    #print(\"total \", ysize)\n",
    "    #print(\" train \",int(ysize*trainpct)-int(ysize*trainpct*valpct))\n",
    "    #print(\" val \",int(ysize*trainpct*valpct))\n",
    "    #print(\" test \",int(ysize*(1- trainpct) ))\n",
    "    data.train_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "    data.train_mask[:int(ysize*trainpct)] = 1\n",
    "    data.train_mask = data.train_mask[torch.randperm(ysize)]\n",
    "    #print(\" train sum \",data.train_mask.sum())\n",
    "    data.test_mask = torch.ones(ysize,1, dtype=torch.long) - data.train_mask\n",
    "    #print(\" test sum \",data.test_mask.sum())\n",
    "    \n",
    "    # quick and dirt\n",
    "    # set first ysize*trainpct*valpct to 0, for those that are 1\n",
    "    data.val_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "    #print(\" val sum \",data.val_mask.sum())\n",
    "    data.val_mask[:int(ysize*trainpct*valpct)] = 1\n",
    "    #print(\" val sum \",data.val_mask.sum())\n",
    "    data.val_mask = data.val_mask[torch.randperm(ysize)]\n",
    "    #print(\" val sum \",data.val_mask.sum())\n",
    "    data.val_mask = data.val_mask - data.test_mask\n",
    "    #print(\" val sum \",data.val_mask.sum())\n",
    "    data.val_mask[data.val_mask <= 0 ]= 0\n",
    "    #print(\" val sum \",data.val_mask.sum())\n",
    "\n",
    "    while data.val_mask.sum() < int(ysize*trainpct*valpct):\n",
    "        data.val_mask = torch.zeros(ysize,1, dtype=torch.long)\n",
    "        #print(\" val sum \",data.val_mask.sum())\n",
    "        data.val_mask[:int(ysize*trainpct*valpct)] = 1\n",
    "        #print(\" val sum \",data.val_mask.sum())\n",
    "        data.val_mask = data.val_mask[torch.randperm(ysize)]\n",
    "        #print(\" val sum \",data.val_mask.sum())\n",
    "        data.val_mask = data.val_mask - data.test_mask\n",
    "        #print(\" val sum \",data.val_mask.sum())\n",
    "        data.val_mask[data.val_mask <= 0 ]= 0\n",
    "        #print(\" val sum \",data.val_mask.sum())\n",
    "    \n",
    "        \n",
    "    #print(\"final val sum \",data.val_mask.sum())\n",
    "    data.train_mask = data.train_mask - data.val_mask\n",
    "    #print(\"final train sum \",data.train_mask.sum())\n",
    "    \n",
    "    \n",
    "    print(data.train_mask.sum())\n",
    "    #print(data.val_mask)\n",
    "    print(data.val_mask.sum())\n",
    "    #print(data.test_mask)  \n",
    "    print(data.test_mask.sum())\n",
    "    \n",
    "    \n",
    "    # transform to list of indexes\n",
    "    data.train_mask = transformMask(data.train_mask)\n",
    "    data.val_mask = transformMask(data.val_mask)\n",
    "    data.test_mask = transformMask(data.test_mask)\n",
    "    \n",
    "    print(data.train_mask)\n",
    "    print(data.val_mask)\n",
    "    print(data.test_mask)\n",
    "    \n",
    "    \n",
    "\n",
    "def trainTestEval(dataset, iterations=1, batch_size=32):\n",
    "    loader = DataLoader(dataset,  shuffle=False)\n",
    "    i = 0\n",
    "    print(loader)\n",
    "    print(dir(loader))\n",
    "    \n",
    "    G = dataset.data\n",
    "    print(G)\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # 1.  prepare model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print(\"using \",device)\n",
    "    model = Net().to(device)\n",
    "    data = G.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    model.train()\n",
    "\n",
    "    # 2.  create a train_mask, and a test_mask (val_mask for further experiments)\n",
    "    #shuffleTrainTestMasks(data)\n",
    "    shuffleTrainTestValMasks(data)\n",
    "\n",
    "    # 3. train some epochs\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 25 == 0 :\n",
    "            print(\"epoch-loss: \",epoch, loss)\n",
    "\n",
    "    # 4. Model evaluation\n",
    "    model.eval()\n",
    "    #  classification in a multiclass setting\n",
    "    #_, pred = model(data).max(dim=1)\n",
    "    #correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
    "    #acc = correct / data.test_mask.sum().item()\n",
    "    #print('Accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "\n",
    "    # regression \n",
    "    pred = model(data)\n",
    "    print(\"target: \",data.y[data.test_mask])\n",
    "    print(\"prediction: \",pred[data.test_mask])\n",
    "    #print(pred[data.test_mask].type())\n",
    "    #print(data.y[data.test_mask].type())\n",
    "    # prepare the normalized mean root squared error\n",
    "    t = data.y[data.test_mask]\n",
    "    y = pred[data.test_mask]\n",
    "    nrmse = torch.sum((t - y) ** 2)/len(data.test_mask)\n",
    "    nrmse = nrmse.sqrt()\n",
    "    print(\"RMSE: \",nrmse)\n",
    "\n",
    "    #m = torch.mean(t)\n",
    "    #print(\"mean\",m)\n",
    "    #tmax = torch.max(t)\n",
    "    #tmin = torch.min(t)\n",
    "    #sd = tmax-tmin\n",
    "    #print(\"sd\",sd)\n",
    "    #nrmse = (nrmse - m)/sd\n",
    "    #print(\"NRMSE:\",nrmse)\n",
    "\n",
    "\n",
    "    endtime = time.time()\n",
    "    print(\"Total train-test time: \"+str(endtime-start))\n",
    "\n",
    "    #i+=1\n",
    "    #if i==1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnDataset2():\n",
    "    def __init__(self,  root, name, transform=None, pre_transform=None):\n",
    "        f = open(name, 'rb')\n",
    "        self.data = pickle.load(f) \n",
    "        #print(self.data.num_features)\n",
    "        self.num_features = self.data.num_features\n",
    "        self.num_classes = 1\n",
    "        f.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0160, 0.0056, 0.0083, 0.0097, 0.0020, 0.0191, 0.0096, 0.0042, 0.0111,\n",
      "        0.0057, 0.0136, 0.0135, 0.0079, 0.0100, 0.0040, 0.0146, 0.0095, 0.0062,\n",
      "        0.0047, 0.0131, 0.0062, 0.0040, 0.0202, 0.0061, 0.0082, 0.0046, 0.0091,\n",
      "        0.0079, 0.0078, 0.0083, 0.0074, 0.0140, 0.0079, 0.0097, 0.0174, 0.0144,\n",
      "        0.0087, 0.0117, 0.0085, 0.0070, 0.0216, 0.0101, 0.0106, 0.0102, 0.0132,\n",
      "        0.0101, 0.0161, 0.0089, 0.0082, 0.0051, 0.0059, 0.0105, 0.0046, 0.0058,\n",
      "        0.0083, 0.0060, 0.0039, 0.0080, 0.0052, 0.0033, 0.0092, 0.0194, 0.0152,\n",
      "        0.0053, 0.0037, 0.0108, 0.0213, 0.0118, 0.0039, 0.0106, 0.0032, 0.0078,\n",
      "        0.0043, 0.0107, 0.0056, 0.0101, 0.0061, 0.0081, 0.0138, 0.0102, 0.0124,\n",
      "        0.0056, 0.0137, 0.0100, 0.0137, 0.0050, 0.0055, 0.0047, 0.0052, 0.0156,\n",
      "        0.0089, 0.0204, 0.0266, 0.0038, 0.0094, 0.0036, 0.0224, 0.0104, 0.0042,\n",
      "        0.0176])\n",
      "<torch_geometric.data.dataloader.DataLoader object at 0x7f96caf5d6a0>\n",
      "['_DataLoader__initialized', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'batch_sampler', 'batch_size', 'collate_fn', 'dataset', 'drop_last', 'num_workers', 'pin_memory', 'sampler', 'timeout', 'worker_init_fn']\n",
      "Data(edge_index=[2, 740], x=[100, 1], y=[100])\n",
      "tensor(56)\n",
      "tensor(14)\n",
      "tensor(30)\n",
      "[0, 3, 4, 5, 8, 12, 16, 19, 20, 23, 24, 25, 27, 28, 29, 30, 32, 33, 36, 38, 41, 44, 45, 46, 47, 48, 50, 51, 54, 56, 57, 58, 59, 60, 64, 68, 73, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 93, 94, 95, 98]\n",
      "[1, 6, 21, 26, 31, 39, 43, 55, 63, 67, 69, 72, 97, 99]\n",
      "[2, 7, 9, 10, 11, 13, 14, 15, 17, 18, 22, 34, 35, 37, 40, 42, 49, 52, 53, 61, 62, 65, 66, 70, 71, 74, 77, 87, 92, 96]\n",
      "epoch-loss:  0 tensor(0.5175, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  25 tensor(0.0321, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  50 tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  75 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  100 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  125 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  150 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "epoch-loss:  175 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "target:  tensor([0.0083, 0.0042, 0.0057, 0.0136, 0.0135, 0.0100, 0.0040, 0.0146, 0.0062,\n",
      "        0.0047, 0.0202, 0.0174, 0.0144, 0.0117, 0.0216, 0.0106, 0.0051, 0.0046,\n",
      "        0.0058, 0.0194, 0.0152, 0.0108, 0.0213, 0.0032, 0.0078, 0.0056, 0.0081,\n",
      "        0.0047, 0.0266, 0.0224])\n",
      "prediction:  tensor([[-0.0021],\n",
      "        [-0.0025],\n",
      "        [ 0.0004],\n",
      "        [-0.0151],\n",
      "        [-0.0003],\n",
      "        [-0.0028],\n",
      "        [ 0.0157],\n",
      "        [-0.0137],\n",
      "        [ 0.0027],\n",
      "        [ 0.0025],\n",
      "        [-0.0018],\n",
      "        [ 0.0018],\n",
      "        [ 0.0032],\n",
      "        [-0.0065],\n",
      "        [-0.0076],\n",
      "        [-0.0017],\n",
      "        [ 0.0087],\n",
      "        [ 0.0072],\n",
      "        [-0.0042],\n",
      "        [ 0.0092],\n",
      "        [ 0.0180],\n",
      "        [ 0.0267],\n",
      "        [-0.0052],\n",
      "        [ 0.0308],\n",
      "        [ 0.0188],\n",
      "        [ 0.0280],\n",
      "        [ 0.0302],\n",
      "        [ 0.0188],\n",
      "        [ 0.0072],\n",
      "        [ 0.0417]], grad_fn=<IndexBackward>)\n",
      "RMSE:  tensor(0.0877, grad_fn=<SqrtBackward>)\n",
      "Total train-test time: 0.2795095443725586\n"
     ]
    }
   ],
   "source": [
    "# load the dataset examples---------------------------------------\n",
    "\n",
    "#PPI\n",
    "#dataset = loadDataset('PPI')\n",
    "#QM7b\n",
    "#dataset = loadDataset('QM7b')\n",
    "#MUTAG\n",
    "#dataset = loadDataset(collection='Entities',name='MUTAG')\n",
    "#ENZYMES FROM TUDataset\n",
    "#dataset = loadDataset(collection='TUDataset',name='ENZYMES')\n",
    "# Cora\n",
    "#dataset = loadDataset(collection='Planetoid',name='Cora')\n",
    "\n",
    "dataset = loadDataset(collection='MyOwnDataset2', name='er_100_0_15_nb.pickle')\n",
    "print(dataset.data.y)\n",
    "\n",
    "trainTestEval(dataset, iterations=2, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pytorch",
   "language": "python",
   "name": "gnn-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
